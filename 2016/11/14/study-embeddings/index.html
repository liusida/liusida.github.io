<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    学习Tensorflow的Embeddings例子 &middot; 
    Liu Sida’s Homepage
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>

    <header class="masthead">
          <p>
            <a href='https://github.com/liusida/'>
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#333" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            </a>
          </p>
      <div class="masthead-inner">
        <h1><a href='/'>Liu Sida's Homepage</a></h1>
        <p class="lead">Machine Learning & Human Learning</p>

        <div class="colophon">
          <p>&copy; 2014. liusida All rights reserved.</p>
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="post">
  <h1>学习Tensorflow的Embeddings例子</h1>
  <span class="post-date">14 Nov 2016</span>
  <p>Udacity上有一个Google技术人员提供的基于Tensorflow的深度学习课程，今天学到Embeddings，有点难理解，所以写个笔记记录下，以备日后回忆。</p>

<p>链接：</p>

<p><a href="https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/63742734590923">Udacity课程视频</a></p>

<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb">例子Github地址</a></p>

<h2 id="section">理解课程：</h2>

<ul>
  <li>课程使用的实现无监督文本学习的根据：<strong>相似的词，会伴随相似的上下文</strong>。
（我记得有人说过，看一个人的朋友，就知道这个人大致是怎样，看来词也一样。）如下图：</li>
</ul>

<p><img src="/images/2016-11-14-study-embeddings/catandkitty.png" alt="word cat and kitty" /></p>

<ul>
  <li>Embeddings的目标，就是把词都放到一个向量空间里去，这样相近的词就聚集在一起。有了这个模型以后，就可以做很多应用，例如找近义词、某一类词聚类、甚至进行向量加减来寻找衍生词。如下图：</li>
</ul>

<p><img src="/images/2016-11-14-study-embeddings/embeddings.png" alt="embeddings" /></p>

<ul>
  <li>如何建立这个Embeddings呢？首先使用一个工具叫word2vec。word2vec算法描述是这样，载入句子，从句子中取出一个词，例如FOX，将他放入Embeddings向量空间（最初位置肯定是随机的），然后通过一次逻辑回归分类预测出他对应的词语，然后与文中实际的QUICK BROWN JUMPS OVER四个词比对，并修正他在Embeddings向量空间里的位置。反复上述步骤，便可得到Embeddings向量空间。（TODO:我这段还需要再理解下）如下图：</li>
</ul>

<p><img src="/images/2016-11-14-study-embeddings/word2vec.png" alt="word2vec" /></p>

<ul>
  <li>再来看一下word2vec的具体流程图：把词<code class="highlighter-rouge">cat</code>放进Embeddings向量空间，然后做一次线性计算，然后取softmax，得到一个一批0-1的数值，然后cross_entropy，产出预测词<code class="highlighter-rouge">purr</code>。跟目标比对，然后调整。这就是训练过程。如下图：</li>
</ul>

<p><img src="/images/2016-11-14-study-embeddings/word2vecm.png" alt="word2vec" /></p>

<ul>
  <li>因为Embeddings向量空间是高维的（需要几维可以自己定义，比如说128维），要想直观的看到他，可以使用t-SNE降维技术，这个技术据说比原始的PCA降维要先进，能保留更多信息。</li>
</ul>

<p><img src="/images/2016-11-14-study-embeddings/t-SNE.png" alt="t-sne" /></p>

<h2 id="section-1">读例子程序：</h2>

<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb">点此查看notebook</a></p>

<h3 id="section-2">1. 引入库文件</h3>

<p>先是引入一些库文件。第一行是如果要在Jupyter Notebook里运行的话，加这一行<code class="highlighter-rouge">%matplotlib inline</code>表示输出的图直接嵌入到Notebook里。</p>

<p><code class="highlighter-rouge">__future__</code> 和 <code class="highlighter-rouge">six</code> 都是保证python2 3兼容的做法。</p>

<p>我们可以看到，<code class="highlighter-rouge">TSNE</code>库，tensorflow并没有，所以使用的是<code class="highlighter-rouge">sklearn</code>的实现。
<code class="highlighter-rouge">collections</code>是python自带的一个工具库，据说很好，新手可以看这篇<a href="http://www.zlovezl.cn/articles/collections-in-python/">中文介绍博文</a>。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># These are all the modules we'll be using later. Make sure you can import them</span>
<span class="c"># before proceeding further.</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
</code></pre>
</div>

<h3 id="section-3">2. 下载压缩包</h3>

<p>下载数据集。如果网速不快，可以用下载工具下载，地址是 http://mattmahoney.net/dc/text8.zip 。保存到运行目录即可。</p>

<p>如果手工解压开查看具体内容，你会开到这个文件里没有标点，全部小写，词与词之间空格隔开：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>anarchism originated as a term of abuse first used against early working class ...
</code></pre>
</div>

<p>英语不太好，去了标题之后，我都看不太懂是啥意思。想起了我们的没有标点古文。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">url</span> <span class="o">=</span> <span class="s">'http://mattmahoney.net/dc/'</span>

<span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="s">"""Download a file if not present, and make sure it's the right size."""</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Found and verified </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span>
      <span class="s">'Failed to verify '</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'. Can you get to it with a browser?'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">filename</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">'text8.zip'</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="section-4">3. 读入文本</h3>

<p>然后是读入压缩包里第一个文件的所有内容，并以空格分割，形成一个很大的list。</p>

<p>这里tf.compat.as_str只是确保一下是string，应该没什么额外用途。</p>

<p>tf.compat包里面都是一些关于兼容性（compatibility)的小工具。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="s">"""Extract the first file enclosed in a zip file as a list of words"""</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">data</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Data size </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="section-5">4. 数据预处理</h3>

<p><code class="highlighter-rouge">collections.Counter</code>很厉害，可以很方便的数元素出现了几次。以后就不要自己造轮子了，用这个Counter可高效多了。</p>

<p><code class="highlighter-rouge">UNK</code>应该是Unknow的缩写，就是这边只统计50000-1个常用词，剩下的词统称<code class="highlighter-rouge">UNK</code>。</p>

<p><code class="highlighter-rouge">count</code>就是包括<code class="highlighter-rouge">UNK</code>在内的所有50000-1个常用词的词语和出现次数。</p>

<p>按照<code class="highlighter-rouge">count</code>里的词先后顺序，给词进行编号，<code class="highlighter-rouge">UNK</code>是0，出现最多的<code class="highlighter-rouge">the</code>是1，出现第二多的<code class="highlighter-rouge">of</code>是2。</p>

<p><code class="highlighter-rouge">dictionary</code>就是词到编号的对应关系，可以快速查找词的编号：<code class="highlighter-rouge">index = dictionary(word)</code>。</p>

<p><code class="highlighter-rouge">reverse_dictionary</code>则是编号到词的对应关系，可以快速查找某个编号是什么词：<code class="highlighter-rouge">word = reverse_dictionary(index)</code>。</p>

<p>最后<code class="highlighter-rouge">data</code>是把原文的词都转化成对应编码以后的串。</p>

<p>保存<code class="highlighter-rouge">dictionary</code>和<code class="highlighter-rouge">reverse_dictionary</code>这一点十分值的学习，对于频繁的查询，这样的缓存能大大增加速度。如果用函数的方式，每次查都要轮询，就土了。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
  <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'UNK'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
  <span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
    <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># dictionary['UNK']</span>
      <span class="n">unk_count</span> <span class="o">=</span> <span class="n">unk_count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
  <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
  <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
  <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span>

<span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Most common words (+UNK)'</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Sample data'</span><span class="p">,</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="k">del</span> <span class="n">words</span>  <span class="c"># Hint to reduce memory.</span>
</code></pre>
</div>

<h3 id="section-6">5. 生成训练数据集函数</h3>

<p>准备要生成可供训练的数据集了。</p>

<p>这里使用全局变量<code class="highlighter-rouge">data_index</code>来记录当前取到哪了，每次取一个batch后会向后移动，如果超出结尾，则又从头开始。<code class="highlighter-rouge">data_index = (data_index + 1) % len(data)</code>（怎么把这东西放在全局变量里，是不是有点不够漂亮？）</p>

<p><code class="highlighter-rouge">skip_window</code>是确定取一个词周边多远的词来训练，比如说<code class="highlighter-rouge">skip_window</code>是2，则取这个词的左右各两个词，来作为它的上下文词。后面正式使用的时候取值是1，也就是只看左右各一个词。</p>

<p>这里的<code class="highlighter-rouge">num_skips</code>我有点疑问，按下面注释是说，<code class="highlighter-rouge">How many times to reuse an input to generate a label.</code>，但是我觉得如果确定了<code class="highlighter-rouge">skip_window</code>之后，完全可以用</p>

<p><code class="highlighter-rouge">num_skips=2*skip_window</code>来确定需要reuse的次数呀，难道还会浪费数据源不成？</p>

<p>这边用了一个双向队列<code class="highlighter-rouge">collections.deque</code>，第一次遇见，看代码与<code class="highlighter-rouge">list</code>没啥区别，从网上简介来看，双向队列主要在左侧插入弹出的时候效率高，但这里并没有左侧的插入弹出呀，所以是不是应该老实用<code class="highlighter-rouge">list</code>会比较好呢？</p>

<p>然后要维护一个<code class="highlighter-rouge">targets_to_avoid</code>，如果不是左右<code class="highlighter-rouge">skip_window</code>个词都用起来的话，则通过随机函数来随机选取对应的词。还是上面那个问题，为啥不直接全用啊？不全用还得随机，想不明白。</p>

<p>这一段代码感觉写的怪怪的，疑点比较多，如果有能知道其中淫巧的同学，可以留言指点一下。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">data_index</span>
  <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
  <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># [ skip_window target skip_window ]</span>
  <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
    <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">num_skips</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">skip_window</span>  <span class="c"># target label at the center of the buffer</span>
    <span class="n">targets_to_avoid</span> <span class="o">=</span> <span class="p">[</span> <span class="n">skip_window</span> <span class="p">]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_skips</span><span class="p">):</span>
      <span class="k">while</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets_to_avoid</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">targets_to_avoid</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
      <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
      <span class="n">labels</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
    <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span>
</code></pre>
</div>

<p>这里是测试一下<code class="highlighter-rouge">generate_batch</code>函数，并无实际用途。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'data:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">8</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]:</span>
  <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_skips</span><span class="o">=</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="o">=</span><span class="n">skip_window</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">with num_skips = </span><span class="si">%</span><span class="s">d and skip_window = </span><span class="si">%</span><span class="s">d:'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">))</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'    batch:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'    labels:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</code></pre>
</div>

<h3 id="section-7">6. 超参数</h3>

<p>不管怎样，<code class="highlighter-rouge">generate_batch</code>函数算是准备好了，先备用，后面真实训练的时候会用到。</p>

<p>接下来来定义一些超参数吧，这些超参数你也可以根据你的需要修改，来看看是否训练出来的Embeddings更符合你的需要。</p>

<p>从前面图上，有人或许会以为Embeddings向量空间只是三维的，其实不是，它是高维的。这里定义<code class="highlighter-rouge">embedding_size</code>是128维。</p>

<p>然后定义用来供人直观检验validate的一些参数。</p>

<p><code class="highlighter-rouge">num_sampled</code>则是Sample Softmax时候用到的一个超参数，确定选几个词来对比优化。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">128</span> <span class="c"># Dimension of the embedding vector.</span>
<span class="n">skip_window</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># How many words to consider left and right.</span>
<span class="n">num_skips</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># How many times to reuse an input to generate a label.</span>
<span class="c"># We pick a random validation set to sample nearest neighbors. here we limit the</span>
<span class="c"># validation samples to the words that have a low numeric ID, which by</span>
<span class="c"># construction are also the most frequent.</span>
<span class="n">valid_size</span> <span class="o">=</span> <span class="mi">16</span> <span class="c"># Random set of words to evaluate similarity on.</span>
<span class="n">valid_window</span> <span class="o">=</span> <span class="mi">100</span> <span class="c"># Only pick dev samples in the head of the distribution.</span>
<span class="n">valid_examples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">valid_window</span><span class="p">),</span> <span class="n">valid_size</span><span class="p">))</span>
<span class="n">num_sampled</span> <span class="o">=</span> <span class="mi">64</span> <span class="c"># Number of negative examples to sample.</span>
</code></pre>
</div>

<h3 id="tensorflow">7. 定义Tensorflow模型</h3>

<p>终于要开始定义Tensorflow的模型了。先回顾一下视频截图：</p>

<p><img src="/images/2016-11-14-study-embeddings/word2vecm.png" alt="word2vec" /></p>

<p><code class="highlighter-rouge">train_dataset</code> 和 <code class="highlighter-rouge">train_labels</code> 两个<code class="highlighter-rouge">placeholder</code>用来训练时传入x和y。</p>

<p><code class="highlighter-rouge">valid_dataset</code> 则是用来人工验证的小数据集，是<code class="highlighter-rouge">constant</code>，直接赋值前面生成的<code class="highlighter-rouge">valid_examples</code>。</p>

<p><code class="highlighter-rouge">embeddings</code>是用来存储Embeddings向量空间的变量，初始化成-1到1之间的随机数，后面优化时调整。这里它是一个 <code class="highlighter-rouge">50000</code> * <code class="highlighter-rouge">128</code> 的二维变量。</p>

<p><code class="highlighter-rouge">softmax_weights</code> 和 <code class="highlighter-rouge">softmax_biases</code> 是用来做线性逻辑分类的参数。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>

  <span class="c"># Input data.</span>
  <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
  <span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">valid_examples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="c"># Variables.</span>
  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
  <span class="n">softmax_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span>
                         <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)))</span>
  <span class="n">softmax_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>
</code></pre>
</div>

<h3 id="section-8">8. 模型前半部分</h3>

<p>通过<code class="highlighter-rouge">tf.nn.embedding_lookup</code>可以直接根据<code class="highlighter-rouge">embeddings</code>表(50000,128)，取出一个与输入词对应的128个值的<code class="highlighter-rouge">embed</code>，也就是128维向量。其实是一batch同时处理，但说一个好理解一些。</p>

<p>通过<code class="highlighter-rouge">tf.nn.sampled_softmax_loss</code>可以用效率较高的Sample Softmax来得到优化所需要的偏差。这个方法视频里有带过，反正就是全部比对速度慢，这样速度快。这个方法顺带把 wX+b 这步也一起算了。可能是因为放在一起可以优化计算速度，记得还有那个很长名字的<code class="highlighter-rouge">softmax_cross_entropy_with_logits</code>同时搞定softmax和cross_entropy，也是为了优化计算速度。但，这样的代码读起来就不好看了！</p>

<p>通过<code class="highlighter-rouge">tf.reduce_mean</code>把<code class="highlighter-rouge">loss</code>偏差压到一个数值，用于优化。</p>

<p>然后就是优化，这里使用了<code class="highlighter-rouge">AdagradOptimizer</code>，当然也可以使用其他<code class="highlighter-rouge">SGD</code>、<code class="highlighter-rouge">Adam</code>等各种优化算法，Tensorflow都实现同样的接口，只需要换个函数名就可以。</p>

<p>这里注释提到，因为<code class="highlighter-rouge">embeddings</code>是定义成<code class="highlighter-rouge">tf.Variable</code>的，所以在优化的时候同时也会调整<code class="highlighter-rouge">embeddings</code>里的参数。这是因为<code class="highlighter-rouge">minimize</code>函数会将与传入的<code class="highlighter-rouge">loss</code>连接的所有源头变量进行调整优化。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Model.</span>
  <span class="c"># Look up embeddings for inputs.</span>
  <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">)</span>
  <span class="c"># Compute the softmax loss, using a sample of the negative labels each time.</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">softmax_weights</span><span class="p">,</span> <span class="n">softmax_biases</span><span class="p">,</span> <span class="n">embed</span><span class="p">,</span>
                               <span class="n">train_labels</span><span class="p">,</span> <span class="n">num_sampled</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>

  <span class="c"># Optimizer.</span>
  <span class="c"># Note: The optimizer will optimize the softmax_weights AND the embeddings.</span>
  <span class="c"># This is because the embeddings are defined as a variable quantity and the</span>
  <span class="c"># optimizer's `minimize` method will by default modify all variable quantities</span>
  <span class="c"># that contribute to the tensor it is passed.</span>
  <span class="c"># See docs on `tf.train.Optimizer.minimize()` for more details.</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

</code></pre>
</div>

<h3 id="section-9">9. 模型关键部位</h3>

<p><a name="normalization"></a></p>

<p>这里写的有点难看懂，必须再刷一次视频。</p>

<p><img src="/images/2016-11-14-study-embeddings/cosine.png" alt="cosine compare" /></p>

<p>原来这是要计算一个<code class="highlighter-rouge">valid_dataset</code>中单词的相似度。</p>

<p>首先，<code class="highlighter-rouge">norm</code>是模的意思，也就是二范数，就是这个向量和原点的距离，土话就是这个向量的长度。</p>

<p>计算<code class="highlighter-rouge">norm</code>的方法，就是向量各维度的平方和：</p>

<script type="math/tex; mode=display">norm = \|X\| = {\sqrt{\sum_{i=1}^N{X_i^2}}}</script>

<p>然后利用这个向量长度<code class="highlighter-rouge">norm</code>来给向量标准化：</p>

<script type="math/tex; mode=display">X_{normalized} = {X \over \|X\|}</script>

<p>这样得到的就是长度为1的向量。也就是抛弃了向量的长度信息，就剩下方向信息。（感谢潘程同学提醒，这东西就叫“<a href="https://zh.wikipedia.org/wiki/%E5%8D%95%E4%BD%8D%E5%90%91%E9%87%8F">单位向量</a>”，可以表示为 \(\hat{X}\)！）</p>

<p>接下来，</p>

<p>造一些简单的数据，用<code class="highlighter-rouge">numpy</code>模拟一下这一段代码，确认一下这段代码的意义：</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="p">[[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">2</span> <span class="mi">4</span> <span class="mi">6</span> <span class="mi">8</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">3</span> <span class="mi">3</span> <span class="mi">3</span> <span class="mi">3</span><span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="p">[[</span> <span class="mi">1</span>  <span class="mi">4</span>  <span class="mi">9</span> <span class="mi">16</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">4</span> <span class="mi">16</span> <span class="mi">36</span> <span class="mi">64</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">4</span>  <span class="mi">4</span>  <span class="mi">4</span>  <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">9</span>  <span class="mi">9</span>  <span class="mi">9</span>  <span class="mi">9</span><span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">[[</span> <span class="mi">30</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">16</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">36</span><span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
<span class="p">[[</span>  <span class="mf">5.47722558</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">10.95445115</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">4.</span>        <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">6.</span>        <span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">/</span> <span class="n">norm</span>
<span class="p">[[</span> <span class="mf">0.18257419</span>  <span class="mf">0.36514837</span>  <span class="mf">0.54772256</span>  <span class="mf">0.73029674</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.18257419</span>  <span class="mf">0.36514837</span>  <span class="mf">0.54772256</span>  <span class="mf">0.73029674</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>       <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>       <span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">normalized_embeddings</span><span class="p">,</span><span class="n">normalized_embeddings</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="p">[[</span> <span class="mf">1.</span>          <span class="mf">1.</span>          <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.</span>          <span class="mf">1.</span>          <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span>  <span class="mf">1.</span>          <span class="mf">1.</span>        <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span>  <span class="mf">1.</span>          <span class="mf">1.</span>        <span class="p">]]</span>
</code></pre>
</div>

<p>可以看到，向量(1,2,3,4)和向量(2,4,6,8)在经过标准化之后，变成了同样的值(0.18,0.36,0.54,0.73)。同样向量(2,2,2,2)和(3,3,3,3)也都变成了一样的(0.5,0.5,0.5,0.5)。</p>

<p>也就是说，这个操作是针对每一个向量，各自做 Normalization 标准化。</p>

<p>也就是说，这个标准化操作抛弃了向量的长度，只关注向量的方向。视频里说，比较两个embeddings向量的时候，用的是cosine距离。</p>

<p>然后从标准化后的向量空间里，找出用于检验的词语对应的向量值，<code class="highlighter-rouge">valid_embeddings</code> 这个变量名有点迷惑人，我建议将其改名为 <code class="highlighter-rouge">valid_embed</code>，以对应前文中的 <code class="highlighter-rouge">embed</code> 变量，他们俩是有相似意义的。</p>

<p>最后通过<a href="https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7">余弦相似性原理</a>，计算出<code class="highlighter-rouge">similarity</code>：</p>

<script type="math/tex; mode=display">similarity = cos(\theta) = { A \cdot B \over \|A\| \cdot \|B\| } = A_{normalized} \cdot B_{normalized}</script>

<p>当 \(\theta=0\) 时，两者重合， \(similarity=cos(0)=1\)。</p>

<p>用检索出来的结果矩阵（前面模拟里我用了全部矩阵） 乘以 <code class="highlighter-rouge">normalized_embeddings</code>的转置，得到的结果最大值是1也就是相同，值越大代表越相似。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Compute the similarity between minibatch examples and all embeddings.</span>
  <span class="c"># We use the cosine distance:</span>
  <span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
  <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">/</span> <span class="n">norm</span>
  <span class="n">valid_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
    <span class="n">normalized_embeddings</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">)</span>
  <span class="n">similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">valid_embeddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">normalized_embeddings</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="section-10">10. 迭代训练</h3>

<p>模型定义完，就来执行训练了。</p>

<p>这里有个小bug，新版本的Tensorflow不再支持<code class="highlighter-rouge">global_variables_initializer</code>方法，而改为使用<code class="highlighter-rouge">initialize_all_variables</code>方法，需要手工修改一下教程代码。（年轻而高速发展的Tensorflow里面有很多类似的接口变动，所以如果一个程序跑不通，去github上看看Tensorflow的源代码很有必要。）</p>

<p>定义迭代次数，100001次，每一次迭代，都从<code class="highlighter-rouge">generate_batch</code>里面获取一个batch的训练数据。制作成<code class="highlighter-rouge">feed_dict</code>，对应前面模型里定义的<code class="highlighter-rouge">placeholder</code>。</p>

<p>跑一次优化，记录下loss并加起来，等到每2000次的时候，输出一次平均loss。（又一个细节：这里的<code class="highlighter-rouge">average_loss</code>是否应该取名叫<code class="highlighter-rouge">subtotal_loss</code>？）</p>

<p>等到每10000次，获取一下<code class="highlighter-rouge">similarity</code>对应的值，然后把最相似的几个词输出到屏幕，用于人工检验。<code class="highlighter-rouge">argsort</code>是获取排序后的indices。然后通过<code class="highlighter-rouge">reverse_dictionary</code>快速定位到词是什么。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">100001</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Initialized'</span><span class="p">)</span>
  <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">batch_data</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span>
      <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">)</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">train_dataset</span> <span class="p">:</span> <span class="n">batch_data</span><span class="p">,</span> <span class="n">train_labels</span> <span class="p">:</span> <span class="n">batch_labels</span><span class="p">}</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
    <span class="n">average_loss</span> <span class="o">+=</span> <span class="n">l</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">average_loss</span> <span class="o">=</span> <span class="n">average_loss</span> <span class="o">/</span> <span class="mi">2000</span>
      <span class="c"># The average loss is an estimate of the loss over the last 2000 batches.</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Average loss at step </span><span class="si">%</span><span class="s">d: </span><span class="si">%</span><span class="s">f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">average_loss</span><span class="p">))</span>
      <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c"># note that this is expensive (~20% slowdown if computed every 500 steps)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">sim</span> <span class="o">=</span> <span class="n">similarity</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid_size</span><span class="p">):</span>
        <span class="n">valid_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">valid_examples</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="mi">8</span> <span class="c"># number of nearest neighbors</span>
        <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">sim</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">log</span> <span class="o">=</span> <span class="s">'Nearest to </span><span class="si">%</span><span class="s">s:'</span> <span class="o">%</span> <span class="n">valid_word</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
          <span class="n">close_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">nearest</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
          <span class="n">log</span> <span class="o">=</span> <span class="s">'</span><span class="si">%</span><span class="s">s </span><span class="si">%</span><span class="s">s,'</span> <span class="o">%</span> <span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">close_word</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>
</code></pre>
</div>

<p>训练完成，获取最后得到的<code class="highlighter-rouge">embeddings</code>向量空间！</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="n">final_embeddings</span> <span class="o">=</span> <span class="n">normalized_embeddings</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre>
</div>

<h3 id="section-11">11. 降维可视化</h3>

<p>下面把<code class="highlighter-rouge">embeddings</code>向量空间通过t-SNE（t-distributed Stochastic Neighbor Embedding）降维到2D，然后打出来看看。</p>

<p>其实根据t-SNE的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">文档</a>，如果向量空间维数过高，比如说我们把128维改成1024维，或者更高，那么这里建议先使用PCA降维方法，降到50维左右以后，再使用t-SNE来继续降到二维或者三维。因为直接用t-SNE给高维向量空间做降维的话，效率会比较低。</p>

<p>画图部分这里用<code class="highlighter-rouge">pylab</code>，就不再赘述，如果喜欢用<code class="highlighter-rouge">matplotlib</code>，也是类似的。（pylab是对标matlab的库，matplotlib也包含在pylab里。）</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_points</span> <span class="o">=</span> <span class="mi">400</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">two_d_embeddings</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">final_embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_points</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">'More labels than embeddings'</span>
  <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c"># in inches</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s">'offset points'</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'bottom'</span><span class="p">)</span>
  <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_points</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre>
</div>

<p>嗯，我们会看到这样的结果：</p>

<p><img src="/images/2016-11-14-study-embeddings/t-sne-result.png" alt="t-sne result" /></p>

<h2 id="debug">一个小debug处理</h2>

<p>这个例子里用到了sklearn的tSNE来降维作显示，但每次运行到这里Jupyter都提醒Kernel死掉了，Kernel重启。于是把源码复制到单独python文件里执行，看到如下错误：</p>
<div class="highlighter-rouge"><pre class="highlight"><code>Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.
</code></pre>
</div>
<p>这个问题似乎是Intel的MKL有问题，网上查到这篇<a href="https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimizations">为Anaconda2.5提供MKL优化</a>，里面也提供了去掉MKL优化的方法：</p>
<div class="highlighter-rouge"><pre class="highlight"><code>conda install nomkl numpy scipy scikit-learn numexpr
conda remove mkl mkl-service
</code></pre>
</div>
<p>执行过之后，就去除了mkl库，import mkl就找不到了，同时例子程序里的问题也没有再出现。
如果还是想使用Intel MKL优化的话，或许可以参考Anaconda官方文档<a href="https://docs.continuum.io/mkl-optimizations/">MKL OPTIMIZATIONS</a>来重新安装过。不过，我先不装了。</p>

<h2 id="tensor---rank--vector-space---vector---dimension">张量 Tensor 的 阶数 Rank ，向量空间 Vector Space 和 向量 Vector 的 维数 Dimension</h2>

<p>前面我在写到Embeddings是128维的时候，脑子有点晕，原来是混淆了Tensor的维数和Vector的维数。</p>

<p>为了搞清楚概念，我查了下资料，应该是这样的：</p>

<p>我们平时说的128维向量，是指一个向量，值是有128个数组成，代表128个方向上的量，例子里的 <code class="highlighter-rouge">embed</code> 变量就是一个batch的128维向量。</p>

<p>向量空间是一群向量的集合，我们这里 <code class="highlighter-rouge">embeddings</code> 变量就是一个50000个向量集合起来的向量空间，所以他的<code class="highlighter-rouge">shape</code>是<code class="highlighter-rouge">(50000,128)</code>。</p>

<p>张量 Tensor 也有一个维数，为了和向量的维数区分，一般成其为阶数 Rank。比如说变量 <code class="highlighter-rouge">embeddings</code> 是一个128维的向量空间，同时他也是一个二阶张量，也就是一张二维表格，行是单词，列是这个单词的128个值。假设其他地方我们看到<code class="highlighter-rouge">128维张量</code>的话，那么它的<code class="highlighter-rouge">shape</code>应该是(xx,xx,xx,xx,….,xx)一共128个xx数，一般来说现在我们学习中还没有碰到这么高阶的张量，可能4阶张量已经很多了，在Convolutional Neural Network里用到的输入，它的<code class="highlighter-rouge">shape</code>是<code class="highlighter-rouge">(batch, image_width, image_height, image_channel)</code>，就是一个4阶张量（其实应该算一个batch的3阶张量的集合）。注意这里的<code class="highlighter-rouge">image_width</code>不是说输入的是图片的宽度值，而是输入的是宽度为<code class="highlighter-rouge">image_width</code>的图片本身。</p>

<p>这就是阶和维的区别。</p>


</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//liu-sidas-homepage.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/11/24/deriving-lstm/">
            推荐《写下记忆：理解、推导、扩展LSTM》
            <small>24 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/16/study-lstm/">
            学习Tensorflow的LSTM的RNN例子
            <small>16 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/12/how-to-ask-a-good-question/">
            如何问一个好问题？
            <small>12 Nov 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
