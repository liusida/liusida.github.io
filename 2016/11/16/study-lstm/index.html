<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    学习Tensorflow的LSTM的RNN例子 &middot; 
    Liu Sida’s Homepage
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>

    <header class="masthead">
          <p>
            <a href='https://github.com/liusida/'>
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#333" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            </a>
          </p>
      <div class="masthead-inner">
        <h1><a href='/'>Liu Sida's Homepage</a></h1>
        <p class="lead">Machine Learning & Human Learning</p>

        <div class="colophon">
          <p>&copy; 2014. liusida All rights reserved.</p>
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="post">
  <h1>学习Tensorflow的LSTM的RNN例子</h1>
  <span class="post-date">16 Nov 2016</span>
  <p>前几天写了<a href="https://liusida.github.io/2016/11/14/study-embeddings/">学习Embeddings的例子</a>，因为琢磨了各个细节，自己也觉得受益匪浅。于是，开始写下一个LSTM的教程吧。</p>

<p>还是<a href="https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/63770919610923">Udacity上那个课程</a>。</p>

<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb">源码也在Github上</a>。</p>

<p>RNN是一个<strong>非常棒</strong>的技术，可能它已经向我们揭示了“活”的意义。RNN我已经尝试学习了几次，包括前面我<a href="https://liusida.github.io/2016/11/04/rnn-implementation/">这篇笔记</a>，所以就直接进入代码阅读吧。</p>

<h2 id="section">读例子程序：</h2>

<h3 id="section-1">1. 引入库文件</h3>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># These are all the modules we'll be using later. Make sure you can import them</span>
<span class="c"># before proceeding further.</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
</code></pre>
</div>

<h3 id="section-2">2. 下载数据</h3>

<p>然后下载数据，如果<a href="https://liusida.github.io/2016/11/04/rnn-implementation/">前面</a>已经下载过，那直接把text8.zip拷过来就可以用。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">url</span> <span class="o">=</span> <span class="s">'http://mattmahoney.net/dc/'</span>

<span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="s">"""Download a file if not present, and make sure it's the right size."""</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Found and verified </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span>
      <span class="s">'Failed to verify '</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'. Can you get to it with a browser?'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">filename</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">'text8.zip'</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="section-3">3. 读入文本</h3>

<p>读文件稍微有些不一样，不是处理成list，而是直接读成一个字符串，因为后面用到的就是串数据。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
  <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Data size </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="section-4">4. 生成训练数据集函数</h3>

<p>切割一下，留1000个字符做检验，其他99999000个字符拿来训练。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">valid_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">valid_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[:</span><span class="n">valid_size</span><span class="p">]</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">valid_size</span><span class="p">:]</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">train_text</span><span class="p">[:</span><span class="mi">64</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">valid_size</span><span class="p">,</span> <span class="n">valid_text</span><span class="p">[:</span><span class="mi">64</span><span class="p">])</span>
</code></pre>
</div>

<h3 id="section-5">5. 两个工具函数</h3>

<p>建立两个函数<code class="highlighter-rouge">char2id</code>和<code class="highlighter-rouge">id2char</code>，用来把字符对应成数字。</p>

<p>本程序只考虑26个字母外加1个空格字符，其他字符都当做空格来对待。所以可以用两个函数，通过ascii码加减，直接算出对应的数值或字符。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># [a-z] + ' '</span>
<span class="n">first_letter</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">char2id</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="o">-</span> <span class="n">first_letter</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="s">' '</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">0</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Unexpected character: </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">id2char</span><span class="p">(</span><span class="n">dictid</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">dictid</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">dictid</span> <span class="o">+</span> <span class="n">first_letter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="s">' '</span>

<span class="k">print</span><span class="p">(</span><span class="n">char2id</span><span class="p">(</span><span class="s">'a'</span><span class="p">),</span> <span class="n">char2id</span><span class="p">(</span><span class="s">'z'</span><span class="p">),</span> <span class="n">char2id</span><span class="p">(</span><span class="s">' '</span><span class="p">),</span> <span class="n">char2id</span><span class="p">(</span><span class="s">'ï'</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">id2char</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">id2char</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">id2char</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="section-6">6. 生成训练数据集函数</h3>

<p>这次 <code class="highlighter-rouge">BatchGenerator</code> 做的比前两天的那个要认真了，用了成员变量来记录位置，而不是用全局变量。</p>

<p>用 <code class="highlighter-rouge">BatchGenerator.next()</code> 方法，可以获取一批子字符串用于训练。</p>

<p><code class="highlighter-rouge">batch_size</code> 是每批几串字符串，<code class="highlighter-rouge">num_unrollings</code> 是每串子字符串的长度（实际上字符串开头还加了上一次获取的最后一个字符，所以实际上字符串长度要比 <code class="highlighter-rouge">num_unrollings</code> 多一个）。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span>
<span class="n">num_unrollings</span><span class="o">=</span><span class="mi">10</span>

<span class="k">class</span> <span class="nc">BatchGenerator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_unrollings</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_text</span> <span class="o">=</span> <span class="n">text</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_text_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_unrollings</span> <span class="o">=</span> <span class="n">num_unrollings</span>
    <span class="n">segment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_text_size</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cursor</span> <span class="o">=</span> <span class="p">[</span> <span class="n">offset</span> <span class="o">*</span> <span class="n">segment</span> <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_batch</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">"""Generate a single batch from the current cursor position in the data."""</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">):</span>
      <span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">char2id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_text</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cursor</span><span class="p">[</span><span class="n">b</span><span class="p">]])]</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cursor</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cursor</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_text_size</span>
    <span class="k">return</span> <span class="n">batch</span>

  <span class="k">def</span> <span class="nf">next</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">"""Generate the next array of batches from the data. The array consists of
    the last batch of the previous array, followed by num_unrollings new ones.
    """</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_batch</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_unrollings</span><span class="p">):</span>
      <span class="n">batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_next_batch</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_batch</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">batches</span>
</code></pre>
</div>

<p>真不愧是优秀程序员写的代码，这个函数写的又让我学习了！</p>

<p>它在初始化的时候先根据 <code class="highlighter-rouge">batch_size</code> 把段分好，然后设立一组游标 <code class="highlighter-rouge">_cursor</code> ，是一组哦，不是一个哦！然后定义好 <code class="highlighter-rouge">_last_batch</code> 看或许到哪了。</p>

<p>然后获取需要的字符串的时候，是一批一批的获取各个字符。</p>

<p>这样做，就可以针对整段字符串均匀的取样，从而避免某些地方学的太细，某些地方又没有学到。</p>

<p>值得注意的是，在RNN准备数据的时候，所喂数据的结构是很容易搞错的。在前面博客中，也有很多同学对于他使用 <code class="highlighter-rouge">transpose</code> 的意义没法理解。这里需要详细记录一下。</p>

<p><code class="highlighter-rouge">BatchGenerator.next()</code> 返回的数据格式，是一个list，list的长度是 <code class="highlighter-rouge">num_unrollings+1</code>，每一个元素，都是一个(<code class="highlighter-rouge">batch_size</code>,27)的array，27是 <code class="highlighter-rouge">vocabulary_size</code>，一个27维向量代表一个字符，是one-hot encoding的格式。</p>

<p>所以，<strong>喂这一批数据进神经网络的时候，理论上是先进去一批的首字符，然后再进去同一批的第二个字符，然后再进去同一批的第三个字符…</strong></p>

<p>也就是说，下图才是真正的RNN的结构，我们要做的，是按照顺序一个一个的按顺序把东西喂进去。这个图，我看到名字叫 <code class="highlighter-rouge">RNN-rolled</code>：</p>

<p><img src="/images/2016-11-16-study-lstm/RNN-rolled.png" alt="RNN-rolled" /></p>

<p>我们平时看到的向右一路展开的RNN其实向右方向（我用了虚线）是代表先后顺序（同时也带记忆数据流），跟上下方向意义是不一样的。有没有同学误解那么一排东西是可以同时喂进去的？这个图，我看到名字叫 <code class="highlighter-rouge">RNN-unrolled</code>。</p>

<p><img src="/images/2016-11-16-study-lstm/RNN-unrolled.png" alt="RNN-unrolled" /></p>

<h3 id="section-7">7. 另外两个工具函数</h3>

<p>再定义两个用来把训练数据转换成可展现字符串的函数。</p>

<p><code class="highlighter-rouge">characters</code> 先从one-hot encoding变回数字，再用id2char变成字符。</p>

<p><code class="highlighter-rouge">batches2string</code> 则将训练数据变成可以展现的字符串。高手这么一批一批的处理数据逻辑还这么绕，而不是按凡人逻辑一个一个的处理让我觉得有点窒息的感觉，自感智商捉急了。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">characters</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
  <span class="s">"""Turn a 1-hot encoding or a probability distribution over the possible
  characters back into its (most likely) character representation."""</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">id2char</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">batches2string</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
  <span class="s">"""Convert a sequence of batches back into their (most likely) string
  representation."""</span>
  <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="s">''</span><span class="p">]</span> <span class="o">*</span> <span class="n">batches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">characters</span><span class="p">(</span><span class="n">b</span><span class="p">))]</span>
  <span class="k">return</span> <span class="n">s</span>

<span class="n">train_batches</span> <span class="o">=</span> <span class="n">BatchGenerator</span><span class="p">(</span><span class="n">train_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_unrollings</span><span class="p">)</span>
<span class="n">valid_batches</span> <span class="o">=</span> <span class="n">BatchGenerator</span><span class="p">(</span><span class="n">valid_text</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">train_batches</span><span class="o">.</span><span class="nb">next</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">train_batches</span><span class="o">.</span><span class="nb">next</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">valid_batches</span><span class="o">.</span><span class="nb">next</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">valid_batches</span><span class="o">.</span><span class="nb">next</span><span class="p">()))</span>
</code></pre>
</div>

<h3 id="section-8">8. 另外四个工具函数</h3>

<p>四个函数，给训练中输出摘要时使用。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
  <span class="s">"""Log-probability of the true labels in a predicted batch."""</span>
  <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-10</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)))</span> <span class="o">/</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sample_distribution</span><span class="p">(</span><span class="n">distribution</span><span class="p">):</span>
  <span class="s">"""Sample one element from a distribution assumed to be an array of normalized
  probabilities.
  """</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">distribution</span><span class="p">)):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">distribution</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">i</span>
  <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">distribution</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">prediction</span><span class="p">):</span>
  <span class="s">"""Turn a (column) prediction into 1-hot encoded samples."""</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
  <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_distribution</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="k">return</span> <span class="n">p</span>

<span class="k">def</span> <span class="nf">random_distribution</span><span class="p">():</span>
  <span class="s">"""Generate a random column of probabilities."""</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">b</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[:,</span><span class="bp">None</span><span class="p">]</span>
</code></pre>
</div>

<p><code class="highlighter-rouge">logprob</code>： 用来测量预测工作完成的如何。</p>

<p>先回忆一下 <code class="highlighter-rouge">cross_entropy</code>：</p>

<script type="math/tex; mode=display">Cross Entropy = - \sum_{i}^N({predictions \cdot \log(labels)})</script>

<p>那么，</p>

<script type="math/tex; mode=display">logprob = { Cross Entropy \over N }</script>

<p>后面三个函数 <code class="highlighter-rouge">sample_distribution</code> <code class="highlighter-rouge">sample</code> <code class="highlighter-rouge">random_distribution</code> 是一起使用的。</p>

<p><code class="highlighter-rouge">random_distribution</code> 就是生成一个平均分布的，加总和为 1 的 array。但是我不知道为何写的这么花哨，我试了半天，似乎 <code class="highlighter-rouge">b/np.sum(b, 1)[:,None]</code> 和 <code class="highlighter-rouge">b/np.sum(b)</code> 的意思是一样的。</p>

<p><code class="highlighter-rouge">sample</code> 则是靠 <code class="highlighter-rouge">sample_distribution</code> 以传入的 <code class="highlighter-rouge">prediction</code> 的概率，随机取一个维设成 1 ，其他都设成 0 ，也就是按照 <code class="highlighter-rouge">prediction</code> 的概率获得一个随机字母。（为啥不直接取概率最大的那个字母呢？搞这么复杂真的好吗？）</p>

<h3 id="tensorflow">9. 定义Tensorflow模型</h3>

<p>分为几个部分：定义变量，定义LSTM Cell，定义输入接口，循环执行LSTM Cell，定义loss，定义优化，定义预测。</p>

<p>num_nodes 是代表这个神经网络中LSTM Cell层的Cell个数。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_nodes</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
</code></pre>
</div>

<h4 id="section-9">1) 定义变量</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>
  <span class="c"># Parameters:</span>
  <span class="c"># Input gate: input, previous output, and bias.</span>
  <span class="n">ix</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">im</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">ib</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c"># Forget gate: input, previous output, and bias.</span>
  <span class="n">fx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">fm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">fb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c"># Memory cell: input, state and bias.                             </span>
  <span class="n">cx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">cm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">cb</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c"># Output gate: input, previous output, and bias.</span>
  <span class="n">ox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">om</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">ob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c"># Variables saving state across unrollings.</span>
  <span class="n">saved_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]),</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">saved_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]),</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="c"># Classifier weights and biases.</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>
</code></pre>
</div>

<p>LSTM Cell 首先有三个门，input output forget三门。</p>

<p>Memory cell 暂时不知道是个什么。</p>

<p>saved_output 是向上的产出，saved_state 是自己的状态记忆。</p>

<p>w 和 b 是最后用来做一个 full connection 的标准神经网络层，把结果变为 vocabulary_size 个之一。</p>

<h4 id="lstm-cell">2) 定义LSTM Cell</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Definition of the cell computation.</span>
  <span class="k">def</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="s">"""Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
    Note that in this formulation, we omit the various connections between the
    previous state and the gates."""</span>
    <span class="n">input_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span> <span class="o">+</span> <span class="n">ib</span><span class="p">)</span>
    <span class="n">forget_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">fm</span><span class="p">)</span> <span class="o">+</span> <span class="n">fb</span><span class="p">)</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span> <span class="o">+</span> <span class="n">cb</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">forget_gate</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="n">input_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
    <span class="n">output_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ox</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">om</span><span class="p">)</span> <span class="o">+</span> <span class="n">ob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="n">state</span>
</code></pre>
</div>

<p>这里定义的 LSTM Cell 似乎并不是我们平时熟悉的那种，而是如下图（http://arxiv.org/pdf/1402.1128v1.pdf）：</p>

<p><img src="/images/2016-11-16-study-lstm/lstm-for-chars.png" alt="lstm for chars" /></p>

<p>初看这个图可能不是很能理解，于是我重新画了一下：</p>

<p><img src="/images/2016-11-16-study-lstm/lstm-model.png" alt="lstm model" /></p>

<p>我手画的图例解释：</p>

<p>(1) \(\otimes\)代表两个数据源乘上参数后相加。\(\oplus\)代表两个数据源相加。</p>

<p>(2) \(\otimes\)外面再加花边的，代表两个数据源相乘后再取 <code class="highlighter-rouge">sigmoid</code> 。</p>

<p>(3) 圆圈里是 \(g\) 的，代表取 <code class="highlighter-rouge">tanh</code> 。</p>

<p>(4) \(State_{-1}\) 下标-1代表这是上一次迭代时的结果。</p>

<blockquote>
  <p>回想一下，<code class="highlighter-rouge">sigmoid</code> 函数产生一个(0,1)的数，<code class="highlighter-rouge">tanh</code> 函数产生一个(-1,1)的数。</p>
</blockquote>

<p>作为对比，我再引用一个我认为画的最完美的标准 LSTM Cell 图，来自 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah 的博客</a>：</p>

<p><img src="/images/2016-11-16-study-lstm/LSTM3-chain.png" alt="LSTM3-chain" /></p>

<p>Colah 图例解释：</p>

<p>(1) 方形中带 \(\sigma\) ，代表两个数据源连接在一起后乘参数，再取 <code class="highlighter-rouge">sigmoid</code> 。（<strong>嗯，这里有不同</strong>：Colah 博客中标准的 LSTM Cell 中，这里的操作是先接在一起，再乘参数，而我们这里是先各自乘参数，再相加。）</p>

<p>(2) 方形中带 \(tanh\) ，代表两个数据源连接在一起后乘参数，再取 <code class="highlighter-rouge">tanh</code> 。（<strong>这里也是</strong>）</p>

<p>(3) 椭圆形中带 \(tanh\)， 代表直接取 <code class="highlighter-rouge">tanh</code> 。</p>

<p>(4) \(\otimes\)代表两个数据源相乘。\(\oplus\)代表两个数据源相加。</p>

<p>(5) 两条从过去\(-1\)到当前 Cell 再到未来\(+1\)的横向黑色线条箭头，上方代表 <code class="highlighter-rouge">state</code>，下方代表 <code class="highlighter-rouge">output</code>。</p>

<p>所以像论文里指出的，这里实现的 LSTM Cell 含有更多参数，效果更好？这种比较目前超出我的认知范围，以后再细看。</p>

<h4 id="section-10">3) 定义输入接口</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Input data.</span>
  <span class="n">train_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unrollings</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span><span class="n">vocabulary_size</span><span class="p">]))</span>
  <span class="n">train_inputs</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[:</span><span class="n">num_unrollings</span><span class="p">]</span>
  <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c"># labels are inputs shifted by one time step.</span>
</code></pre>
</div>

<p>这里也是一个 batch 同时处理的。但为了容易理解，我先假设 <code class="highlighter-rouge">batch_size=1</code> ，然后假设我们要训练一个字符串 abcdefg。</p>

<p>那么 <code class="highlighter-rouge">train_inputs</code> 是 abcdef，<code class="highlighter-rouge">train_labels</code> 是 bcdefg 。</p>

<h4 id="lstm-cell-1">4) 循环执行LSTM Cell</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Unrolled LSTM loop.</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">saved_output</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">saved_state</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">train_inputs</span><span class="p">:</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre>
</div>

<p>根据前面定义变量的时候规定，初始 <code class="highlighter-rouge">saved_output</code> 和 <code class="highlighter-rouge">saved_state</code> 都是全零。</p>

<p>依次输入 a b c d e f ，把每一次的输出放在一起形成一个 list 就是 <code class="highlighter-rouge">outputs</code>。</p>

<h4 id="loss">5) 定义loss</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># State saving across unrollings.</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">saved_output</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
                                <span class="n">saved_state</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">state</span><span class="p">)]):</span>
    <span class="c"># Classifier.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">outputs</span><span class="p">),</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)))</span>
</code></pre>
</div>

<p>因为不是顺序执行语言，一般模型如果不是相关的语句，其执行是没有先后顺序的，<code class="highlighter-rouge">control_dependencies</code> 的作用就是建立先后顺序，保证前面两句被执行后，才执行后面的内容。</p>

<p>这里也就是先把 <code class="highlighter-rouge">saved_output</code> 和 <code class="highlighter-rouge">saved_state</code> 保存之后，再计算 <code class="highlighter-rouge">logits</code> 和 <code class="highlighter-rouge">loss</code>。否则因为下面计算时没有关联到 <code class="highlighter-rouge">saved_output</code> 和 <code class="highlighter-rouge">saved_state</code>，如果不用 <code class="highlighter-rouge">control_dependencies</code> 那上面两句保存就不会被优化语句触发。</p>

<p><a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#concat"><code class="highlighter-rouge">tf.concat(0, values)</code></a> 是指在 0 维上把 values 连接起来。本来 outputs 是一个 list，每一个元素都是一个27维向量表示一个字母（还是假设 <code class="highlighter-rouge">batch_size=1</code>）。</p>

<p>通过 <code class="highlighter-rouge">tf.concat</code> 把结果连接起来，成为一个向量，可以拿来乘以 w 加上 b 这样进入一个 full connection，从而得到 logits 。</p>

<p>然后再通过 <code class="highlighter-rouge">softmax_cross_entropy_with_logits</code> 比较连接并 full connection 的 <code class="highlighter-rouge">outputs</code> 和 连接起来的 <code class="highlighter-rouge">train_labels</code> ，得到 <code class="highlighter-rouge">loss</code> 。</p>

<h4 id="section-11">6) 定义优化</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Optimizer.</span>
  <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">(</span>
    <span class="mf">10.0</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">staircase</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
  <span class="n">gradients</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
  <span class="n">gradients</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
</code></pre>
</div>

<p><a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#exponential_decay"><code class="highlighter-rouge">tf.train.exponential_decay</code></a> 可以用来实现 <code class="highlighter-rouge">learning_rate</code> 的指数型衰减，越到后面 <code class="highlighter-rouge">learning_rate</code> 越小。（依赖后面修改 <code class="highlighter-rouge">global_step</code> 值来实现）</p>

<p><code class="highlighter-rouge">optimizer</code> 定义成使用标准 Gradient Descent 。每一种 <code class="highlighter-rouge">optimizer</code> 都有几个标准接口，我们前面常用的是 <code class="highlighter-rouge">minimize</code> 接口，他自动的调整整个 <code class="highlighter-rouge">Graph</code> 中可调节的 <code class="highlighter-rouge">Variables</code> 尝试最小化 <code class="highlighter-rouge">loss</code>。其实 <code class="highlighter-rouge">minimize</code> 函数就是这两步并起来： <code class="highlighter-rouge">compute_gradients</code> 和 <code class="highlighter-rouge">apply_gradients</code>。先计算梯度值，然后再把那些参数减去梯度值。这里把两步分开了，为了在 apply 之前先处理一下梯度值，Tensorflow 给了详细解释，我们来看看[手册][manual-compute-gradients]。</p>

<p><code class="highlighter-rouge">compute_gradients</code> 函数返回一个list，里面是一对一对的 <code class="highlighter-rouge">gradient</code> 和 <code class="highlighter-rouge">variable</code>，说明针对某个可调整的变量，他的梯度是多少。</p>

<p><code class="highlighter-rouge">clip_by_global_norm</code> 避免梯度值过大产生 Exploding Gradients 梯度爆炸问题，视频里有这么一个图：</p>

<p><img src="/images/2016-11-16-study-lstm/clip_gradient.png" alt="clip gradients" /></p>

<p><code class="highlighter-rouge">clip_by_global_norm</code> 的具体计算是，先计算 <code class="highlighter-rouge">global_norm</code> ，也就是整个 <code class="highlighter-rouge">tensor</code> 的模（二范数）。看这个模是否大于文中的<code class="highlighter-rouge">1.25</code>，如果大于，则结果等于 <code class="highlighter-rouge">gradients * 1.25 / global_norm</code>，如果不大于，就不变。</p>

<p>最后，<code class="highlighter-rouge">apply_gradients</code>。这里传入的 <code class="highlighter-rouge">global_step</code> 是会被修改的，每次加一，这样下次计算 <code class="highlighter-rouge">learning_rate</code> 的时候就会使用新的 <code class="highlighter-rouge">global_step</code> 值。</p>

<h4 id="section-12">7) 定义预测</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="c"># Predictions.</span>
  <span class="n">train_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

  <span class="c"># Sampling and validation eval: batch 1, no unrolling.</span>
  <span class="n">sample_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">])</span>
  <span class="n">saved_sample_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="n">saved_sample_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="n">reset_sample_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
    <span class="n">saved_sample_output</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">])),</span>
    <span class="n">saved_sample_state</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">])))</span>
  <span class="n">sample_output</span><span class="p">,</span> <span class="n">sample_state</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="p">(</span>
    <span class="n">sample_input</span><span class="p">,</span> <span class="n">saved_sample_output</span><span class="p">,</span> <span class="n">saved_sample_state</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">saved_sample_output</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">sample_output</span><span class="p">),</span>
                                <span class="n">saved_sample_state</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">sample_state</span><span class="p">)]):</span>
    <span class="n">sample_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">sample_output</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</code></pre>
</div>

<p><code class="highlighter-rouge">sample_input</code> 是一个1-hot编码过的字符。</p>

<p>建立初始 state 和 output，经过同样的 LSTM Cell，得到下一个预测的字符 <code class="highlighter-rouge">sample_prediction</code>。</p>

<h3 id="section-13">10. 开始训练</h3>

<h4 id="section-14">1) 训练</h4>

<p>注意到这里喂进去的字符串长度正好是 <code class="highlighter-rouge">num_unrollings + 1</code>，恰好对应前面 <code class="highlighter-rouge">BatchGenerator.next()</code> 获取的时候得到的字符串长度，也恰好对应了模型定义里 <code class="highlighter-rouge">train_inputs</code> 和 <code class="highlighter-rouge">train_labels</code> 错开1个字符。</p>

<p><code class="highlighter-rouge">mean_loss</code> 用来加总各步的 <code class="highlighter-rouge">loss</code> 值，用来后面输出。（还是建议叫 <code class="highlighter-rouge">subtotal_loss</code>）</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">7001</span>
<span class="n">summary_frequency</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Initialized'</span><span class="p">)</span>
  <span class="n">mean_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">train_batches</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unrollings</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">feed_dict</span><span class="p">[</span><span class="n">train_data</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
      <span class="p">[</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">train_prediction</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
    <span class="n">mean_loss</span> <span class="o">+=</span> <span class="n">l</span>
</code></pre>
</div>

<h4 id="section-15">2) 定期输出摘要</h4>

<p>他怎么不用 tensorflow 来计算呀，反而用 numpy 来计算，很奇怪。来仔细看看。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">summary_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">mean_loss</span> <span class="o">/</span> <span class="n">summary_frequency</span>
      <span class="c"># The mean loss is an estimate of the loss over the last few batches.</span>
      <span class="k">print</span><span class="p">(</span>
        <span class="s">'Average loss at step </span><span class="si">%</span><span class="s">d: </span><span class="si">%</span><span class="s">f learning rate: </span><span class="si">%</span><span class="s">f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">lr</span><span class="p">))</span>
      <span class="n">mean_loss</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">batches</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Minibatch perplexity: </span><span class="si">%.2</span><span class="s">f'</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">))))</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="p">(</span><span class="n">summary_frequency</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c"># Generate some samples.</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'='</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
          <span class="n">feed</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">random_distribution</span><span class="p">())</span>
          <span class="n">sentence</span> <span class="o">=</span> <span class="n">characters</span><span class="p">(</span><span class="n">feed</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
          <span class="n">reset_sample_state</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">79</span><span class="p">):</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">sample_prediction</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">sample_input</span><span class="p">:</span> <span class="n">feed</span><span class="p">})</span>
            <span class="n">feed</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
            <span class="n">sentence</span> <span class="o">+=</span> <span class="n">characters</span><span class="p">(</span><span class="n">feed</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
          <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'='</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
      <span class="c"># Measure validation set perplexity.</span>
      <span class="n">reset_sample_state</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">valid_logprob</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid_size</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">valid_batches</span><span class="o">.</span><span class="nb">next</span><span class="p">()</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">sample_prediction</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">sample_input</span><span class="p">:</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]})</span>
        <span class="n">valid_logprob</span> <span class="o">=</span> <span class="n">valid_logprob</span> <span class="o">+</span> <span class="n">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Validation set perplexity: </span><span class="si">%.2</span><span class="s">f'</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">valid_logprob</span> <span class="o">/</span> <span class="n">valid_size</span><span class="p">)))</span>
</code></pre>
</div>

<p>每当 <code class="highlighter-rouge">summary_frequency</code> 整数倍步的时候，输出平均 <code class="highlighter-rouge">loss</code> 值和 <code class="highlighter-rouge">learning_rate</code> ，看看是否有 clip 掉，如果没有 clip 掉，那么都是 10.0 。然后再计算这一部分 train set perplexity。</p>

<p>每当 <code class="highlighter-rouge">summary_frequency * 10</code> 整数倍步的时候，尝试输出一些文字结果。</p>

<p>这里尝试得到 5 句，每局 80 个字符的文字结果。</p>

<p>首先以平均分布随机得到一个字符，并作为 <code class="highlighter-rouge">sentence</code> 的第一个字符。</p>

<p>然后 <code class="highlighter-rouge">reset_sample_state</code> 一下，保证初始化的 <code class="highlighter-rouge">state</code> 和 <code class="highlighter-rouge">output</code> 都设成 0 。</p>

<p>然后传入第一个字符作为输入，得到第一个预测字符的预测概率 <code class="highlighter-rouge">prediction</code>，通过 <code class="highlighter-rouge">sample</code> 将其蜕化成一个确定的字符 <code class="highlighter-rouge">feed</code>，然后接到 <code class="highlighter-rouge">sentence</code> 上，并下一次传给模型作为输入。</p>

<p>这样就得到了一句80字符的句子。重复这个过程 5 次，得到 5 句。</p>

<p>继而，又是每当 <code class="highlighter-rouge">summary_frequency</code> 整数倍步的时候，（写的不好啊，明明应当把相近的写在一起。）用 valid_text 来计算平均的 validation set perplexity。</p>

<p>根据信息论，perplexity <a href="https://en.wikipedia.org/wiki/Perplexity">wikipedia定义</a> 和 cross_entropy 的关系如下：</p>

<script type="math/tex; mode=display">perplexity = e^{cross\_entropy}</script>

<h2 id="section-16">结束</h2>

<p>谢谢阅读，敬请留言。</p>


</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//liu-sidas-homepage.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/11/25/cross-entropy/">
            Cross Entropy 的通俗意义
            <small>25 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/24/deriving-lstm/">
            推荐《写下记忆：理解、推导、扩展LSTM》
            <small>24 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/14/study-embeddings/">
            学习Tensorflow的Embeddings例子
            <small>14 Nov 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
