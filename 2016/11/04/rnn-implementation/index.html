<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    Recurrent Neural Network(RNN) Implementation &middot; 
    Liu Sida’s Homepage
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>

    <header class="masthead">
          <p>
            <a href='https://github.com/liusida/'>
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#333" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            </a>
          </p>
      <div class="masthead-inner">
        <h1><a href='/'>Liu Sida's Homepage</a></h1>
        <p class="lead">Machine Learning & Human Learning</p>

        <div class="colophon">
          <p>&copy; 2014. liusida All rights reserved.</p>
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="post">
  <h1>Recurrent Neural Network(RNN) Implementation</h1>
  <span class="post-date">04 Nov 2016</span>
  <p>I heard about RNN for a long time, and have learned the concept several times, but until yesterday, I can’t implement any useful code to solve my own problem.</p>

<p>So I checked some tutorial. The most basic one is applying RNN to the MNIST dataset. The sample code is from <a href="https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/">sentdex’s video tutorial</a>:<br />
```python<br />
import tensorflow as tf<br />
from tensorflow.examples.tutorials.mnist import input_data<br />
from tensorflow.python.ops import rnn, rnn_cell<br />
mnist = input_data.read_data_sets(“/tmp/data/”, one_hot = True)</p>

<p>hm_epochs = 3<br />
n_classes = 10<br />
batch_size = 128<br />
chunk_size = 28<br />
n_chunks = 28<br />
rnn_size = 128</p>

<p>x = tf.placeholder(‘float’, [None, n_chunks,chunk_size])<br />
y = tf.placeholder(‘float’)</p>

<p>def recurrent_neural_network(x):<br />
    layer = {‘weights’:tf.Variable(tf.random_normal([rnn_size,n_classes])),<br />
             ‘biases’:tf.Variable(tf.random_normal([n_classes]))}</p>

<div class="highlighter-rouge"><pre class="highlight"><code>x = tf.transpose(x, [1,0,2])
x = tf.reshape(x, [-1, chunk_size])
x = tf.split(0, n_chunks, x)

lstm_cell = rnn_cell.BasicLSTMCell(rnn_size,state_is_tuple=True)
outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)
output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']

return output
</code></pre>
</div>

<p>def train_neural_network(x):<br />
    prediction = recurrent_neural_network(x)<br />
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )<br />
    optimizer = tf.train.AdamOptimizer().minimize(cost)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    for epoch in range(hm_epochs):
        epoch_loss = 0
        for _ in range(int(mnist.train.num_examples/batch_size)):
            epoch_x, epoch_y = mnist.train.next_batch(batch_size)
            epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))

            _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
            epoch_loss += c

        print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
    print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels}))
</code></pre>
</div>

<p>train_neural_network(x)<br />
```</p>

<p>I suggest you to watch <a href="https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/">sentdex’s video tutorial</a> first, and if you are not confident with what’s going on in the function <strong>recurrent_neural_network</strong>, you can go on read this article.</p>

<p>Let’s take a close look at this function:<br />
```python<br />
def recurrent_neural_network(x):<br />
    layer = {‘weights’:tf.Variable(tf.random_normal([rnn_size,n_classes])),<br />
             ‘biases’:tf.Variable(tf.random_normal([n_classes]))}</p>

<div class="highlighter-rouge"><pre class="highlight"><code>x = tf.transpose(x, [1,0,2])
x = tf.reshape(x, [-1, chunk_size])
x = tf.split(0, n_chunks, x)

lstm_cell = rnn_cell.BasicLSTMCell(rnn_size,state_is_tuple=True)
outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)
output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']

return output ```
</code></pre>
</div>

<p>In the 1st line, we define variables for weights and biases. That’s common in any other neural network. If you don’t understand this line, you should go back and learn what is a neural network.</p>

<p>Next, we make some tricks to the input X. Tensorflow cannot output the value right away, as it define the whole model first and run later, so we can use numpy to mimik this tricks to see what happened.<br />
<code class="highlighter-rouge">python
import numpy as np
#First we define a small tensor to observe
x = np.arange(24).reshape(2,4,3)
print(x)
#Then do the transepose
x = x.transpose([1,0,2])
print(x)
#And do the reshape
x = x.reshape(-1,3)
print(x)
#And do the split
x = np.split(x, 4)
print(x)
</code><br />
the outcome should be like this (I pretty the output a little):<br />
```code<br />
[[[ 0  1  2]<br />
  [ 3  4  5]<br />
  [ 6  7  8]<br />
  [ 9 10 11]]<br />
 [[12 13 14]<br />
  [15 16 17]<br />
  [18 19 20]<br />
  [21 22 23]]]</p>

<p>[[[ 0  1  2]<br />
  [12 13 14]]<br />
 [[ 3  4  5]<br />
  [15 16 17]]<br />
 [[ 6  7  8]<br />
  [18 19 20]]<br />
 [[ 9 10 11]<br />
  [21 22 23]]]</p>

<p>[[ 0  1  2]<br />
 [12 13 14]<br />
 [ 3  4  5]<br />
 [15 16 17]<br />
 [ 6  7  8]<br />
 [18 19 20]<br />
 [ 9 10 11]<br />
 [21 22 23]]</p>

<p>[array([[ 0,  1,  2],<br />
       [12, 13, 14]]), array([[ 3,  4,  5],<br />
       [15, 16, 17]]), array([[ 6,  7,  8],<br />
       [18, 19, 20]]), array([[ 9, 10, 11],<br />
       [21, 22, 23]])]<br />
```<br />
OK. We can see that, we finally have a list, which contains 4 elemnts. The 1st elements contains the 1st lines of the origin images. The 2nd contains the 2nd lines of origin images.</p>

<p>Now the final list is the input of the RNN.</p>

<p>After define a BasicLSTMCell cell, the next line is the key RNN implementation.<br />
<code class="highlighter-rouge">python
outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)
</code><br />
Let take a look at <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py">the source code of rnn.rnn</a> on github, the programmer said that, the simplest form of RNN network generated is:<br />
<code class="highlighter-rouge">python
  state = cell.zero_state(...)
  outputs = []
  for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
  return (outputs, state)
</code></p>

<p>That’s good! The output is also a list, just like the input list.</p>

<p><img src="/images/rnn_flow.png" alt="RNN-flow" /></p>

<p>Every single LSTM cell has a layer, which contain 128(rnn_size) neurals.<br />
At first, we feed the LSTM cell with the first slide of our input list, which is happen to be the whole first origin image. The first line of first origin image goes to the first LSTM first. Because the rnn has 128(rnn_size) neurals, so it will output 128 numbers this time. And then the 2nd line of first origin image goes to the 2nd LSTM, until meets the final line, and get another 128 numbers output…<br />
Finally, we will input all images in this batch, and get a batch of result.</p>

<p>And it comes to the final line:<br />
<code class="highlighter-rouge">python
output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']
</code></p>

<p>We see that only the last element of output is used. Sometimes we use all of the outputs list, and sometimes we just use the last one. That is because the information is contained in the last output. Why? Because this cell is called Long-short Term Memory(LSTM). It is designed to remember the whole sequence!</p>

<p>Here is a very good tutoral of <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">What is LSTM</a>.</p>

<p>Thank you for reading. Comments are welcomed.</p>


</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//liu-sidas-homepage.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2017/08/24/deep-scratch/">
            Implement a Deep Neural Network using Python and Numpy
            <small>24 Aug 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/25/cross-entropy/">
            Cross Entropy 的通俗意义
            <small>25 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/24/deriving-lstm/">
            推荐《写下记忆：理解、推导、扩展LSTM》
            <small>24 Nov 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
