<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    Recurrent Neural Network(RNN) Implementation &middot; 
    Liu Sida | Star Liu | liusida
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <header class="masthead">
          <p>
            <a href='https://github.com/liusida/'>
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#333" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            </a>
          </p>
      <div class="masthead-inner">
        <h1><a href='/'>Liu Sida's Homepage</a></h1>
        <p class="lead">Machine Learning & Human Learning</p>

        <div class="colophon">
          <p>&copy; 2014. liusida All rights reserved.</p>
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="post">
  <h1>Recurrent Neural Network(RNN) Implementation</h1>
  <span class="post-date">04 Nov 2016</span>
  <p>I heard about RNN for a long time, and have learned the concept several times, but until yesterday, I can’t implement any useful code to solve my own problem.</p>

<p>So I checked some tutorial. The most basic one is applying RNN to the MNIST dataset. The sample code is from <a href="https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/">sentdex’s video tutorial</a>:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">rnn</span><span class="p">,</span> <span class="n">rnn_cell</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">"/tmp/data/"</span><span class="p">,</span> <span class="n">one_hot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">hm_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">n_chunks</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">rnn_size</span> <span class="o">=</span> <span class="mi">128</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">'float'</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span><span class="n">chunk_size</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">recurrent_neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="p">{</span><span class="s">'weights'</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">rnn_size</span><span class="p">,</span><span class="n">n_classes</span><span class="p">])),</span>
             <span class="s">'biases'</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_classes</span><span class="p">]))}</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">,</span><span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">lstm_cell</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">layer</span><span class="p">[</span><span class="s">'weights'</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer</span><span class="p">[</span><span class="s">'biases'</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span> <span class="nf">train_neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">recurrent_neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hm_epochs</span><span class="p">):</span>
            <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">num_examples</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)):</span>
                <span class="n">epoch_x</span><span class="p">,</span> <span class="n">epoch_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="n">epoch_x</span> <span class="o">=</span> <span class="n">epoch_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span><span class="n">n_chunks</span><span class="p">,</span><span class="n">chunk_size</span><span class="p">))</span>

                <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">cost</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">epoch_x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">epoch_y</span><span class="p">})</span>
                <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">c</span>

            <span class="k">print</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="s">'completed out of'</span><span class="p">,</span><span class="n">hm_epochs</span><span class="p">,</span><span class="s">'loss:'</span><span class="p">,</span><span class="n">epoch_loss</span><span class="p">)</span>

        <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct</span><span class="p">,</span> <span class="s">'float'</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Accuracy:'</span><span class="p">,</span><span class="n">accuracy</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">x</span><span class="p">:</span><span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)),</span> <span class="n">y</span><span class="p">:</span><span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">}))</span>

<span class="n">train_neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre>
</div>

<p>I suggest you to watch <a href="https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/">sentdex’s video tutorial</a> first, and if you are not confident with what’s going on in the function <strong>recurrent_neural_network</strong>, you can go on read this article.</p>

<p>Let’s take a close look at this function:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">recurrent_neural_network</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="p">{</span><span class="s">'weights'</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">rnn_size</span><span class="p">,</span><span class="n">n_classes</span><span class="p">])),</span>
             <span class="s">'biases'</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_classes</span><span class="p">]))}</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">,</span><span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">lstm_cell</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">layer</span><span class="p">[</span><span class="s">'weights'</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer</span><span class="p">[</span><span class="s">'biases'</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre>
</div>

<p>In the 1st line, we define variables for weights and biases. That’s common in any other neural network. If you don’t understand this line, you should go back and learn what is a neural network.</p>

<p>Next, we make some tricks to the input X. Tensorflow cannot output the value right away, as it define the whole model first and run later, so we can use numpy to mimik this tricks to see what happened.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c">#First we define a small tensor to observe</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c">#Then do the transepose</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c">#And do the reshape</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c">#And do the split</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre>
</div>
<p>the outcome should be like this (I pretty the output a little):</p>
<pre><code class="language-code">[[[ 0  1  2]
  [ 3  4  5]
  [ 6  7  8]
  [ 9 10 11]]
 [[12 13 14]
  [15 16 17]
  [18 19 20]
  [21 22 23]]]

[[[ 0  1  2]
  [12 13 14]]
 [[ 3  4  5]
  [15 16 17]]
 [[ 6  7  8]
  [18 19 20]]
 [[ 9 10 11]
  [21 22 23]]]

[[ 0  1  2]
 [12 13 14]
 [ 3  4  5]
 [15 16 17]
 [ 6  7  8]
 [18 19 20]
 [ 9 10 11]
 [21 22 23]]

[array([[ 0,  1,  2],
       [12, 13, 14]]), array([[ 3,  4,  5],
       [15, 16, 17]]), array([[ 6,  7,  8],
       [18, 19, 20]]), array([[ 9, 10, 11],
       [21, 22, 23]])]
</code></pre>
<p>OK. We can see that, we finally have a list, which contains 4 elemnts. The 1st elements contains the 1st lines of the origin images. The 2nd contains the 2nd lines of origin images.</p>

<p>Now the final list is the input of the RNN.</p>

<p>After define a BasicLSTMCell cell, the next line is the key RNN implementation.</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">outputs</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">lstm_cell</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre>
</div>
<p>Let take a look at <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py">the source code of rnn.rnn</a> on github, the programmer said that, the simplest form of RNN network generated is:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="n">state</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">input_</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
</code></pre>
</div>

<p>That’s good! The output is also a list, just like the input list.</p>

<p><img src="/images/rnn_flow.png" alt="RNN-flow" /></p>

<p>Every single LSTM cell has a layer, which contain 128(rnn_size) neurals.
At first, we feed the LSTM cell with the first slide of our input list, which is happen to be the whole first origin image. The first line of first origin image goes to the first LSTM first. Because the rnn has 128(rnn_size) neurals, so it will output 128 numbers this time. And then the 2nd line of first origin image goes to the 2nd LSTM, until meets the final line, and get another 128 numbers output…
Finally, we will input all images in this batch, and get a batch of result.</p>

<p>And it comes to the final line:</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">layer</span><span class="p">[</span><span class="s">'weights'</span><span class="p">])</span> <span class="o">+</span> <span class="n">layer</span><span class="p">[</span><span class="s">'biases'</span><span class="p">]</span>
</code></pre>
</div>

<p>We see that only the last element of output is used. Sometimes we use all of the outputs list, and sometimes we just use the last one. That is because the information is contained in the last output. Why? Because this cell is called Long-short Term Memory(LSTM). It is designed to remember the whole sequence!</p>

<p>Here is a very good tutoral of <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">What is LSTM</a>.</p>

<p>Thank you for reading. Comments are welcomed.</p>


</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//liu-sidas-homepage.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/11/14/study-embeddings/">
            学习Tensorflow的Embeddings例子
            <small>14 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/11/12/how-to-ask-a-good-question/">
            如何问一个好问题？
            <small>12 Nov 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/10/31/translate-from-tf-2-keras/">
            from Tensorflow to Keras
            <small>31 Oct 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
