<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    About sparse_column_with_hash_bucket &middot; 
    Liu Sida | Star Liu | liusida
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Liu Sida's Homepage</h1>
        <p class="lead">Machine Learning & Human Learning</p>

        <div class="colophon">
          <p>&copy; 2014. liusida All rights reserved.</p>
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="post">
  <h1>About sparse_column_with_hash_bucket</h1>
  <span class="post-date">30 Oct 2016</span>
  <p>Yesterday, I saw tf.contrib.layers.sparse_column_with_hash_bucket in a <a href="https://www.tensorflow.org/versions/r0.11/tutorials/wide/index.html">tutorial</a>. That's a very useful function! I thought. I never met such a function in Keras or TFLearn.</p>

<p>Basically, the function do something like this:</p>

<pre><code class="python">hash(category_string) % dim
</code></pre>

<p>Let's say the text "the quick brown fox". If we want to put them into 5 buckets, we can get result like this:</p>

<pre><code class="python">hash(the) % 5 = 0
hash(quick) % 5 = 1
hash(brown) % 5 = 1
hash(fox) % 5 = 3
</code></pre>

<p>This example is metioned by <a href="https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way">Luis Argerich</a></p>

<p>That's really easy for preprocessing, but there are disadvantages of that, metioned by <a href="https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way">Artem Onuchin</a> also in that page.</p>

<p>So, the common way to do this <strong>feature engineering</strong> thing is metioned by <a href="https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering">Rahul Agarwal</a>:</p>

<ul>
<li>Scaling by Max-Min</li>
<li>Normalization using Standard Deviation</li>
<li>Log based feature/Target: use log based features or log based target function.</li>
<li>One Hot Encoding</li>
</ul>


<p>Anyway, if we want to do hash_bucket without tensorflow, we can do it in Pandas which is metioned <a href="http://stackoverflow.com/questions/8673035/what-is-feature-hashing-hashing-trick/33581487">here</a>:</p>

<pre><code class="python">import pandas as pd
import numpy as np

data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

data = pd.DataFrame(data)

def hash_col(df, col, N):
    cols = [col + "_" + str(i) for i in range(N)]
    print(cols)
    def xform(x): tmp = [0 for i in range(N)]; tmp[hash(x) % N] = 1; return pd.Series(tmp,index=cols)
    df[cols] = df[col].apply(xform)
    return df.drop(col,axis=1)

print(hash_col(data, 'state',4))
</code></pre>

<p>result:
<code>code
   pop  year  state_0  state_1  state_2  state_3
0  1.5  2000        1        0        0        0
1  1.7  2001        1        0        0        0
2  3.6  2002        1        0        0        0
3  2.4  2001        1        0        0        0
4  2.9  2002        1        0        0        0
</code></p>

<p><strong>[Edited]</strong> Actually, we can use <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html">pandas.get_dummies</a> to do this directly:</p>

<pre><code class="python">import pandas as pd

data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

data = pd.DataFrame(data)
print(pd.get_dummies(data, columns=['state','year']))
</code></pre>

<p>result:
<code>code
   pop  state_Nevada  state_Ohio  year_2000  year_2001  year_2002
0  1.5           0.0         1.0        1.0        0.0        0.0
1  1.7           0.0         1.0        0.0        1.0        0.0
2  3.6           0.0         1.0        0.0        0.0        1.0
3  2.4           1.0         0.0        0.0        1.0        0.0
4  2.9           1.0         0.0        0.0        0.0        1.0
</code></p>

<p>After all,</p>

<p>I think I should learn more about one-hot-encoding and word2vec embedding.</p>

<blockquote><p>Coming up with features is difficult, time-consuming, requires expert knowledge. "Applied machine learning" is basically feature engineering.</p></blockquote>

<p>Said <a href="http://www.andrewng.org/">Andrew Ng</a>.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/10/30/thank-you-github-n-jekyll/">
            Thank you, GitHub and Jekyll
            <small>30 Oct 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
