I"À<p>cross_entropy å…¬å¼å¦‚ä¸‹ï¼š</p>

\[CrossEntropy = - \sum_i( L_i \cdot \log( S_i ) )\]

<p>å®ƒæè¿°çš„æ˜¯<strong>å¯èƒ½æ€§ S åˆ° L çš„è·ç¦»</strong>ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯æè¿°<strong>ç”¨ S æ¥æè¿° L è¿˜éœ€è¦å¤šå°‘ä¿¡æ¯</strong>ï¼ˆå¦‚æœæ˜¯ä»¥2ä¸ºåº•çš„logï¼Œåˆ™ä»£è¡¨è¿˜éœ€è¦å¤šå°‘bitçš„ä¿¡æ¯ï¼›å¦‚æœæ˜¯ä»¥10ä¸ºåº•çš„logï¼Œåˆ™ä»£è¡¨è¿˜éœ€è¦å¤šå°‘ä½åè¿›åˆ¶æ•°çš„ä¿¡æ¯ï¼‰ã€‚</p>

<p>å½“å¹´ é¦™å†œ Shannon åˆ›ç«‹ä¿¡æ¯è®ºçš„æ—¶å€™ï¼Œè€ƒè™‘çš„æ˜¯æ¯ä¸€æ¬¡éƒ½æ˜¯æ‰”ç¡¬å¸ï¼Œç»“æœåªæœ‰2ä¸ªå¯èƒ½ï¼Œæ‰€ä»¥ç”¨çš„æ˜¯ä»¥2ä¸ºåº•ï¼Œå‘æ˜äº†bitè®¡é‡å•ä½ã€‚</p>

<p>è€Œè½¯ä»¶å®ç°ï¼Œä¾‹å¦‚ Tensorflow é‡Œçš„å®ç°ï¼Œåˆ™æ˜¯ä½¿ç”¨ä»¥ e ä¸ºåº•çš„logã€‚</p>

<p>Tensorflow ä¸­æœ‰ä¸ªç»å¸¸ç”¨åˆ°çš„å‡½æ•°å« <code class="highlighter-rouge">tf.nn.softmax_cross_entropy_with_logits</code> ã€‚è¿™ä¸ªå‡½æ•°çš„å®ç°å¹¶ä¸åœ¨ Python ä¸­ï¼Œæ‰€ä»¥æˆ‘ç”¨ Numpy å®ç°ä¸€ä¸ªåŒæ ·åŠŸèƒ½çš„å‡½æ•°è¿›è¡Œæ¯”å¯¹ï¼Œç¡®è®¤å®ƒä½¿ç”¨çš„æ˜¯ä»¥ e ä¸ºåº•çš„logã€‚ç†ç”±å¾ˆç®€å•ï¼Œå› ä¸º Softmax å‡½æ•°é‡Œä½¿ç”¨äº† e çš„æŒ‡æ•°ï¼Œæ‰€ä»¥å½“ Cross Entropy ä¹Ÿä½¿ç”¨ä»¥ e çš„logï¼Œç„¶åè¿™ä¸¤ä¸ªå‡½æ•°æ”¾åˆ°ä¸€èµ·å®ç°ï¼Œå¯ä»¥è¿›è¡Œå¾ˆå¥½çš„æ€§èƒ½ä¼˜åŒ–ã€‚</p>

<p>å…¶ä¸­å¯¹äº logits è¿™ä¸ªç§°å‘¼ï¼Œæˆ‘ä»ç„¶æ²¡æœ‰æ˜ç™½æ˜¯ä¸ºä»€ä¹ˆã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="c1"># Make up some testing data, need to be rank 2
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
		<span class="p">[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span>
		<span class="p">[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">2.</span><span class="p">]</span>
		<span class="p">])</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
		<span class="p">[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span>
		<span class="p">[</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]</span>
		<span class="p">])</span>


<span class="c1"># Numpy part #
</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">sf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">sf</span> <span class="o">=</span> <span class="n">sf</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sf</span>

<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">softmax</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
	<span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span> <span class="n">labels</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">softmax</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span> <span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span> <span class="n">cross_entropy</span> <span class="p">)</span>

<span class="n">numpy_result</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span> <span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">numpy_result</span><span class="p">)</span>

<span class="c1"># Tensorflow part #
</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">g</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>
	<span class="n">tf_x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">tf_label</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
	<span class="n">tf_ret</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">tf_x</span><span class="p">,</span><span class="n">tf_label</span><span class="p">)</span> <span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="k">as</span> <span class="n">ss</span><span class="p">:</span>
	<span class="n">tensorflow_result</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">tf_ret</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">tensorflow_result</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="é™„å„å…¬å¼">é™„å„å…¬å¼</h2>
<h3 id="1-softmax">1. Softmax</h3>
<p>\(S_i = { e^{X_i} \over \sum_j( e^{X_j})}\)
è¿™é‡Œçš„ X å°±æ˜¯ logitsï¼ŒS è¡¨ç¤ºä¸€æ¬¡åˆ¤æ–­ï¼ŒSi è¡¨ç¤ºä¸€æ¬¡åˆ¤æ–­ä¸­çš„ç¬¬iä¸ªé€‰é¡¹ã€‚</p>
<h3 id="2-cross-entropy">2. Cross Entropy</h3>
<p>\(D = - \sum_i( L_i \cdot \log( S_i ) )\)
è¿™é‡Œ D è¡¨ç¤ºä¸€æ¬¡åˆ¤æ–­ï¼ŒLi æ˜¯ä¸€æ¬¡åˆ¤æ–­ä¸­ä¸€ä¸ª label çš„ç¬¬ i ä¸ªé€‰é¡¹ã€‚log æ˜¯ä»¥ e ä¸ºåº•ã€‚</p>
<h3 id="3-loss">3. loss</h3>
<p>\(loss = {1\over N} \sum_k(Dk)\)
è¿™é‡Œçš„ Dk è¡¨ç¤ºç¬¬ k æ¬¡åˆ¤æ–­ï¼ŒN è¡¨ç¤ºæ€»æ¬¡æ•°ï¼Œä¹Ÿå°±æ˜¯å–å¹³å‡å€¼ã€‚</p>
:ET