<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Liu Sida’s Homepage
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>

    <header class="masthead">
          <p>
            <a href='https://github.com/liusida/'>
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#333" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            </a>
          </p>
      <div class="masthead-inner">
        <h1><a href='/'>Liu Sida's Homepage</a></h1>
        <p class="lead">Machine Learning & Human Learning</p>

        <div class="colophon">
          <p>&copy; 2014. liusida All rights reserved.</p>
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1>
      <a href="/2016/11/24/deriving-lstm/">
        推荐《写下记忆：理解、推导、扩展LSTM》
      </a>
    </h1>

    <span class="post-date">24 Nov 2016</span>

    <p>今天学习一篇挺有意思的文章《<a href="http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">写下记忆：理解、推导、扩展LSTM</a>》（英文），从 RNN 的原理开始讲起，并讲述 RNN 为什么理论上很美好，实践起来要出问题，以及各种问题是怎么被解决的，从而诞生了 LSTM 。</p>

<p>不过似乎作者隐藏了他的私人信息，不知道是谁，本来还想看看是哪国人，因为感觉不是英语母语。:P</p>

<p>这回应该不会忘记为什么 Basic LSTM 的模型是那样的了，因为都是有原因的，虽然并不能全懂。</p>


  </div>
  
  <div class="post">
    <h1>
      <a href="/2016/11/16/study-lstm/">
        学习Tensorflow的LSTM的RNN例子
      </a>
    </h1>

    <span class="post-date">16 Nov 2016</span>

    <p>前几天写了<a href="https://liusida.github.io/2016/11/14/study-embeddings/">学习Embeddings的例子</a>，因为琢磨了各个细节，自己也觉得受益匪浅。于是，开始写下一个LSTM的教程吧。</p> <p>还是<a href="https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/63770919610923">Udacity上那个课程</a>。</p> <p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb">源码也在Github上</a>。</p> <p>RNN是一个<strong>非常棒</strong>的技术，可能它已经向我们揭示了“活”的意义。RNN我已经尝试学习了几次，包括前面我<a href="https://liusida.github.io/2016/11/04/rnn-implementation/">这篇笔记</a>，所以就直接进入代码阅读吧。</p> <h2 id="section">读例子程序：</h2> <h3 id="section-1">1. 引入库文件</h3> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># These are all the modules we'll be using later. Make sure you can import them</span> <span class="c"># before proceeding further.</span> <span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span> <span class="kn">import</span> <span class="nn">os</span>...
  </div>
  
  <div class="post">
    <h1>
      <a href="/2016/11/14/study-embeddings/">
        学习Tensorflow的Embeddings例子
      </a>
    </h1>

    <span class="post-date">14 Nov 2016</span>

    <p>Udacity上有一个Google技术人员提供的基于Tensorflow的深度学习课程，今天学到Embeddings，有点难理解，所以写个笔记记录下，以备日后回忆。</p> <p>链接：</p> <p><a href="https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/63742734590923">Udacity课程视频</a></p> <p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb">例子Github地址</a></p> <h2 id="section">理解课程：</h2> <ul> <li>课程使用的实现无监督文本学习的根据：<strong>相似的词，会伴随相似的上下文</strong>。 （我记得有人说过，看一个人的朋友，就知道这个人大致是怎样，看来词也一样。）如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/catandkitty.png" alt="word cat and kitty" /></p> <ul> <li>Embeddings的目标，就是把词都放到一个向量空间里去，这样相近的词就聚集在一起。有了这个模型以后，就可以做很多应用，例如找近义词、某一类词聚类、甚至进行向量加减来寻找衍生词。如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/embeddings.png" alt="embeddings" /></p> <ul> <li>如何建立这个Embeddings呢？首先使用一个工具叫word2vec。word2vec算法描述是这样，载入句子，从句子中取出一个词，例如FOX，将他放入Embeddings向量空间（最初位置肯定是随机的），然后通过一次逻辑回归分类预测出他对应的词语，然后与文中实际的QUICK BROWN JUMPS OVER四个词比对，并修正他在Embeddings向量空间里的位置。反复上述步骤，便可得到Embeddings向量空间。（TODO:我这段还需要再理解下）如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/word2vec.png" alt="word2vec" /></p> <ul> <li>再来看一下word2vec的具体流程图：把词<code class="highlighter-rouge">cat</code>放进Embeddings向量空间，然后做一次线性计算，然后取softmax，得到一个一批0-1的数值，然后cross_entropy，产出预测词<code class="highlighter-rouge">purr</code>。跟目标比对，然后调整。这就是训练过程。如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/word2vecm.png" alt="word2vec" /></p> <ul> <li>因为Embeddings向量空间是高维的（需要几维可以自己定义，比如说128维），要想直观的看到他，可以使用t-SNE降维技术，这个技术据说比原始的PCA降维要先进，能保留更多信息。</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/t-SNE.png"...
  </div>
  
  <div class="post">
    <h1>
      <a href="/2016/11/12/how-to-ask-a-good-question/">
        如何问一个好问题？
      </a>
    </h1>

    <span class="post-date">12 Nov 2016</span>

    <p><a href="http://stackoverflow.com/help/how-to-ask">本文翻译自StackOverflow.com，原文链接（英文）</a></p> <p>首先，我们很愿意帮助你。为了提高你获得回答的机会，这里有几个小提示：</p> <h2 id="section">先搜索，再搜索。</h2> <p>把你找到的东西记下来。就算你在搜索的时候找不到其他有用的答案，把你搜到的跟你问题有关的链接放进来并说明为什么没有回答你的问题，这样能够让来回答的人更明白你问题的意思。</p> <h2 id="section-1">写一个能够准确概括你问题的<strong>标题</strong>。</h2> <p>标题是潜在回答者首先看到的东西，如果你的标题没有引起他们的兴趣，他们是不会点进去读到你其他内容的。你可以这样：</p> <ul> <li>假装你是要问一个很忙的同事，你需要把你整个问题用一句话总结：什么细节可以让人马上分辨出你想要问的？错误提示、关键API、不寻常的情况，这些细节可以让你的问题更具识别度，如果跟其他问题回答过的没什么区别，别人可能就不会再受累回答了。</li> <li>拼写、语法、标点，这些都很重要！请记住，标题是你给人家的第一印象。如果你不习惯使用英语，你可以找个朋友帮你过一下标题。</li> <li>如果你觉得总结问题关键点有点难，你可以写完正文以后再写标题。有时候先把问题写出来后，就很自然能总结这个问题关键了。</li> </ul> <p>举一些例子：</p> <ul> <li><strong>坏</strong> ：C# Math Confusion</li> <li><strong>好</strong> ：Why does using float instead of int give me different results when all of my inputs are integers?</li> <li><strong>坏</strong> ：[php] session doubt</li> <li><strong>好</strong> ：How can I redirect users to...
  </div>
  
  <div class="post">
    <h1>
      <a href="/2016/11/04/rnn-implementation/">
        Recurrent Neural Network(RNN) Implementation
      </a>
    </h1>

    <span class="post-date">04 Nov 2016</span>

    <p>I heard about RNN for a long time, and have learned the concept several times, but until yesterday, I can’t implement any useful code to solve my own problem.</p> <p>So I checked some tutorial. The most basic one is applying RNN to the MNIST dataset. The sample code is from...
  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page2" class="older">Older</a>
  
  
    <span class="previous">Newer</span>
  
</div>
    </div>

  </body>
</html>
