<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://172.25.168.122:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://172.25.168.122:4000/" rel="alternate" type="text/html" /><updated>2023-05-15T22:13:38+08:00</updated><id>http://172.25.168.122:4000/feed.xml</id><title type="html">Sida Liu</title><subtitle>The world is so complex that we cannot stop learning.</subtitle><author><name>Sida Liu</name></author><entry><title type="html">Understanding ‘A Thousand Brain’, A Personal Review</title><link href="http://172.25.168.122:4000/reading/2023/05/15/Hawkins-Thousand-Brains-book/" rel="alternate" type="text/html" title="Understanding 'A Thousand Brain', A Personal Review" /><published>2023-05-15T00:00:00+08:00</published><updated>2023-05-15T00:00:00+08:00</updated><id>http://172.25.168.122:4000/reading/2023/05/15/Hawkins-Thousand-Brains-book</id><content type="html" xml:base="http://172.25.168.122:4000/reading/2023/05/15/Hawkins-Thousand-Brains-book/">&lt;p&gt;I recently finished reading Jeff Hawkins’s book, &lt;em&gt;A Thousand Brains&lt;/em&gt;. I found this book to be incredibly enlightening, introducing a new framework for understanding the brain. This innovative perspective deepened my comprehension of the subject, transforming my previously vague notions about the brain into tangible, concrete concepts.&lt;/p&gt;

&lt;p&gt;In this article, I will summarize the concepts presented in the first part of the book, starting from the level of the whole brain and gradually delving deeper until we reach the level of an individual neuron.&lt;/p&gt;

&lt;p&gt;Our brain can be divided into two parts: the neocortex, which Hawkins refers to as “the new brain”, and everything else, which was referred to as “the old brain”.&lt;/p&gt;

&lt;p&gt;The old brain, having evolved earlier in animals, is found in fish, birds, and reptiles. It is responsible for controlling emotions, desires, body movement, and the like.&lt;/p&gt;

&lt;p&gt;On the other hand, the new brain evolved later and is exclusive to mammals. It enables the formation of ideas, languages, concepts, and more.&lt;/p&gt;

&lt;p&gt;Structurally, the new brain envelops the old brain and is connected to it via numerous neural links, facilitating communication between the two.&lt;/p&gt;

&lt;p&gt;If we were to flatten the neocortex, it would be, as Hawkins describes, “approximately the size of a large dinner napkin and twice as thick (about 2.5 mm)”. Imagine numerous circles on this napkin. Each circle corresponds to a structure called a cortical column, which measures about 1mm wide and 2.5mm high.&lt;/p&gt;

&lt;p&gt;The main premise of the book is that this &lt;strong&gt;cortical column&lt;/strong&gt; functions as &lt;strong&gt;the fundamental computational unit of the brain&lt;/strong&gt;. The brain has approximately 150,000 cortical columns, each of them functioning like a mini-brain, capable of storing hundreds of different models and making predictions based on its input. This is why the book is titled “A Thousand Brains”.&lt;/p&gt;

&lt;p&gt;A cortical column consists of multiple layers. Two important layers are discussed in the book. The upper layer receives sensory information while the lower layer receives movement information. Hawkins explains, “The basic flow of information goes as follows: A sensory input arrives and is represented by the neurons in the upper layer. This invokes the location in the lower layer that is associated with the input. When movement occurs, such as moving a finger, then the lower layer changes to the expected new location, which causes a prediction of the next input in the upper layer.”&lt;/p&gt;

&lt;p&gt;This is analogous to the place cells and grid cells in the old brain, which provide animals with a sense of their location in the environment. However, whereas we have only one set of place cells and grid cells in the old brain, in the neocortex, we have a set of cortical place cells and cortical grid cells in &lt;em&gt;every&lt;/em&gt; cortical column. When we interact with a coffee cup, for instance, the relevant cortical columns become active. Thanks to the cortical place cells and cortical grid cells, these columns can determine that the current sensory input corresponds to a certain point in the model, and when movement occurs, they can predict what sensory input will come next.&lt;/p&gt;

&lt;p&gt;Cortical columns complement each other. Each object is stored “in many, but not too many, columns” simultaneously. This distributed storage system is both efficient and robust. Each perception or thought results from a consensus reached by the relevant columns through a process akin to voting. The ‘voting’ neurons act as representatives of a cortical column, broadcasting the column’s prediction through long-distance axons connected to other cortical columns.&lt;/p&gt;

&lt;p&gt;It’s easy to see how this would work with everyday objects, like a coffee cup. However, this process also applies to objects we can’t see, such as a DNA molecule. We tend to visualize these invisible objects, allowing the cortical columns to form reference frames for these mental models.&lt;/p&gt;

&lt;p&gt;Furthermore, the same method can handle abstract concepts. Instead of using a 2D or 3D physical space as a reference frame, a cortical column can create novel reference frames for abstract concepts. For example, when we think about political opinions, we might use the political spectrum to form a 1D space, with each opinion placed at a specific point on that spectrum. If we treat one opinion as an object, then the political spectrum represents one of the dimensions of the space in which the model lives. As our thoughts drift from one abstract concept to another, it’s as though we’re moving through one dimension of this abstract space, encountering new concepts along the way.&lt;/p&gt;

&lt;p&gt;As Hawkins explains, “This is one reason that learning conceptual knowledge can be difficult. … Part of the learning is discovering what constitutes a good reference frame, including the number of dimensions. … Becoming an expert in a field of study requires discovering an effective framework to represent the associated data and facts.”&lt;/p&gt;

&lt;p&gt;Cortical columns are composed of tens of thousands of neurons. According to the book, the model of the current artificial neural network doesn’t accurately reflect the structure and function of these neurons, and it proposes that we need a better model.&lt;/p&gt;

&lt;p&gt;A neuron has many dendrites, with the vast majority of synapses—about 90%—located on these dendrites. When these synapses are activated, they trigger dendritic spikes that travel from the synapses to the cell body. Although these spikes don’t cause the neuron to fire, they make it easier and quicker for the neuron to fire. This process helps explain the phenomenon of priming in cognitive psychology. The remaining 10% of synapses, known as proximal synapses, are located near the cell body and are modeled as the input in the artificial neural network. In essence, the dendritic synapses prime the neuron, and the proximal synapses trigger the action potential. Without the dendritic synapses, the input would be too ambiguous for the neuron to generate a prediction.&lt;/p&gt;

&lt;p&gt;The concepts outlined above comprise only the first part of the book, focusing on how the brain works. The latter part of the book, although equally fascinating and convincing, takes a more subjective approach, so I’ve chosen to exclude it from this summary.&lt;/p&gt;

&lt;p&gt;To me, this framework is incredibly compelling. One of the most fascinating aspects of neuroscience is the sheer abundance of research papers containing a wealth of experimental data. Researchers measure various aspects of the brain and document their findings. This means that anyone can study neuroscience by reading these papers and forming their own theories about the brain. I’m excited to learn more about the brain and see how I can incorporate additional knowledge and information into this framework.&lt;/p&gt;

&lt;p&gt;P.S. An interesting side note: The author of &lt;a href=&quot;2023-05-07-tegmark-mathematical-universe-book.md&quot;&gt;the book I read a week earlier&lt;/a&gt;, Max Tegmark, expresses significant concern about the existential risks introduced by artificial intelligence. Conversely, Jeff Hawkins sits on the other end of the spectrum, exuding optimism about the development of artificial intelligence. Both authors are very thoughtful and reasonable, yet it’s fascinating to see how they’ve arrived at vastly different opinions about the existential risk posed by AI.&lt;/p&gt;

&lt;p&gt;Any feedback? We can discuss it under &lt;a href=&quot;https://twitter.com/liusida2007/status/1655240922521800704&quot;&gt;this Tweet. &lt;i class=&quot;fab fa-twitter&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Sida Liu (with the help of ChatGPT)</name></author><category term="Reading" /><summary type="html">I recently finished reading Jeff Hawkins’s book, A Thousand Brains. I found this book to be incredibly enlightening, introducing a new framework for understanding the brain. This innovative perspective deepened my comprehension of the subject, transforming my previously vague notions about the brain into tangible, concrete concepts.</summary></entry><entry><title type="html">A Personal Journey Through Tegmark’s Mathematical Universe</title><link href="http://172.25.168.122:4000/reading/2023/05/07/tegmark-mathematical-universe-book/" rel="alternate" type="text/html" title="A Personal Journey Through Tegmark's Mathematical Universe" /><published>2023-05-07T00:00:00+08:00</published><updated>2023-05-07T00:00:00+08:00</updated><id>http://172.25.168.122:4000/reading/2023/05/07/tegmark-mathematical-universe-book</id><content type="html" xml:base="http://172.25.168.122:4000/reading/2023/05/07/tegmark-mathematical-universe-book/">&lt;p&gt;I recently finished reading Max Tegmark’s book &lt;em&gt;Our Mathematical Universe: My Quest for the Ultimate Nature of Reality&lt;/em&gt;, and I found it to be a truly amazing and transformative experience. This book, along with &lt;em&gt;Complexity: A Guided Tour&lt;/em&gt; by Melanie Mitchell, which I read several years ago, has had a significant impact on my understanding of the world and my intellectual journey.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Our Mathematical Universe&lt;/em&gt; has provided clarity for several of the big questions that have been troubling me since grad school. I had expressed to my advisor, Josh, that my interests felt too broad—I wanted to understand intelligence, life, and reality, as well as build artificial versions of each to learn from the process. This book has helped to clear up much of my confusion and guide me in my pursuit of these answers.&lt;/p&gt;

&lt;p&gt;Tegmark’s book starts by exploring reality from various angles.&lt;/p&gt;

&lt;p&gt;First, the book takes a cosmological perspective, delving into the history of our universe. It explains how physicists have been able to extrapolate the 14-billion-year history of our observable universe, leading to the conclusion that the universe began with &lt;em&gt;the Big Bang&lt;/em&gt;. Years ago, I was skeptical about the Big Bang theory because it seemed like an extreme extrapolation that went too far. However, the book provides a wealth of evidence to support this theory, specifically, our observable universe seems to be quite flat in spacetime, and I now find the extrapolation quite convincing.&lt;/p&gt;

&lt;p&gt;The book also introduces &lt;em&gt;the inflation theory&lt;/em&gt;, developed in 1979, as a potential explanation for the cause of the Big Bang. According to this theory, the universe underwent rapid expansion due to the doubling of matter in every dimension. This idea is compatible with general relativity and has gained acceptance among many physicists. I personally find this theory satisfying and see it as a potential starting point for further exploration if I want to delve into cosmology.&lt;/p&gt;

&lt;p&gt;Next, Tegmark delves into the microscopic world, discussing the debate between Einstein and Bohr over the nature of quantum mechanics. When I first learned this debate years ago, I had initially sided with Einstein, believing that there must be some mechanisms behind the apparent randomness of quantum mechanics–“God does not play dice with the universe”. The book introduces Everett’s &lt;em&gt;Many Worlds interpretation&lt;/em&gt;, first proposed in 1957, which posits that the wavefunction never collapses and that parallel universes exist. I find this idea intriguing and think that Einstein might have appreciated it as well. In comparison, the widely accepted &lt;em&gt;Copenhagen interpretation&lt;/em&gt;, dating back to the 1920s, seems more like a higher-level model of reality—useful, but not the ultimate.&lt;/p&gt;

&lt;p&gt;Additionally, the book covers the concept of &lt;em&gt;decoherence&lt;/em&gt;, which explains why we don’t observe quantum superposition on a macroscopic scale. This idea was first introduced in 1970 by H. Dieter Zeh, but Tegmark also discovered this independently. Decoherence is now well-accepted in the field and could serve as a starting point for further investigation if I want to delve into the microscopic realm.&lt;/p&gt;

&lt;p&gt;Tegmark takes a step back and discusses three different types of reality: internal, external, and consensus reality. &lt;em&gt;Internal reality&lt;/em&gt; is what we perceive and the world model created in our brain, akin to Anil Seth’s “controlled hallucination”, while &lt;em&gt;external reality&lt;/em&gt; is the ultimate, objective, independent reality. &lt;em&gt;Consensus reality&lt;/em&gt; lies somewhere in between, as it is the shared understanding of reality among people. The idea of consensus reality particularly resonates with me, as it seems to be the most important reality for ordinary people. For example, a wall is a wall not because it exists independently from human observers, but because most people agree that it is a wall. However, at a more fundamental level, we know that it is just a collection of protons, neutrons, and electrons.&lt;/p&gt;

&lt;p&gt;The book proposes that the ultimate external reality is a mathematical object, which is an intriguing but speculative idea. While I am open to the possibility, I would like to see more evidence before fully accepting it.&lt;/p&gt;

&lt;p&gt;In discussing the mathematical nature of reality, Tegmark emphasizes the distinction between &lt;em&gt;mathematical structures&lt;/em&gt; and their &lt;em&gt;descriptions&lt;/em&gt;. For example, the ideas of addition and multiplication are the structures, and we can describe them using either natural language, like ancient Greeks did, or using equations, like modern students do. The description can vary from culture to culture, but the mathematical structure doesn’t change. He also explores the fundamental properties of mathematical structures, such as &lt;em&gt;symmetry&lt;/em&gt; and &lt;em&gt;relation&lt;/em&gt;. Regardless of whether mathematics is the ultimate reality or merely describes the reality, his elaboration deepens my understanding of mathematics.&lt;/p&gt;

&lt;p&gt;Finally, the book touches on fascinating topics inside the reality, like the nature of life and consciousness. Tegmark suggests that &lt;em&gt;life&lt;/em&gt; is not a binary category but a spectrum defined by complexity. Similarly, he proposes that &lt;em&gt;consciousness&lt;/em&gt; arises from the brain’s computational processes as a byproduct of understanding the self and the world.&lt;/p&gt;

&lt;p&gt;While I have chosen to disregard some of the more speculative ideas in the book, the concepts that resonated with me have significantly influenced my understanding of the world. I feel incredibly fortunate to have encountered so many valuable ideas in one book. As I continue my intellectual journey, I eagerly anticipate reading more works by Max Tegmark and others who delve into these fascinating subjects.&lt;/p&gt;

&lt;p&gt;Any feedback? We can discuss it under &lt;a href=&quot;https://twitter.com/liusida2007/status/1655240922521800704&quot;&gt;this Tweet. &lt;i class=&quot;fab fa-twitter&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>Sida Liu (with the help of ChatGPT)</name></author><category term="Reading" /><summary type="html">I recently finished reading Max Tegmark’s book Our Mathematical Universe: My Quest for the Ultimate Nature of Reality, and I found it to be a truly amazing and transformative experience. This book, along with Complexity: A Guided Tour by Melanie Mitchell, which I read several years ago, has had a significant impact on my understanding of the world and my intellectual journey.</summary></entry><entry><title type="html">Validate HLSL grammar in command line</title><link href="http://172.25.168.122:4000/gpu/2021/10/11/validate-hlsl-grammar-in-command-line/" rel="alternate" type="text/html" title="Validate HLSL grammar in command line" /><published>2021-10-11T00:00:00+08:00</published><updated>2021-10-11T00:00:00+08:00</updated><id>http://172.25.168.122:4000/gpu/2021/10/11/validate-hlsl-grammar-in-command-line</id><content type="html" xml:base="http://172.25.168.122:4000/gpu/2021/10/11/validate-hlsl-grammar-in-command-line/">&lt;p&gt;HLSL, a.k.a. High-level shader language, is used in Unity.&lt;/p&gt;

&lt;p&gt;One can write the shader in Unity (with VS or VSCode) and compile it automatically in Unity. If there’s any error in the shader code, the error message will show up in the Unity console.&lt;/p&gt;

&lt;p&gt;However, since I am using a Linux, I can’t debug the shader in Unity.
So I turn to a light-weight tool called SHADERed.
One can install the SHADERed plug-in in VSCode along with SHADERed, and voilà, we can debug the shader code!&lt;/p&gt;

&lt;p&gt;So we can write HLSL in VSCode and SHADERed first, and then copy it to Unity shader and modify it to be compatible.&lt;/p&gt;

&lt;p&gt;Here is a cool video shows how to do the modification, it even works with GLSL.
&lt;a href=&quot;https://www.youtube.com/watch?v=CzORVWFvZ28&quot;&gt;Youtube: How to convert a shader from ShaderToy to Unity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now my problem was that SHADERed won’t show the detailed compilation error if there’s any error in the shader code. It just says, “Can not display preview - there are some errors you should fix.”&lt;/p&gt;

&lt;p&gt;So I digged in a bit, and realized that both Unity and SHADERed use &lt;a href=&quot;https://github.com/KhronosGroup/glslang&quot;&gt;glslang&lt;/a&gt; to compile the shader code.&lt;/p&gt;

&lt;p&gt;Luckily, glslang provide a standalone executable that can compile the shader code as a showcase of how to use the glslang API. It is called &lt;code class=&quot;highlighter-rouge&quot;&gt;glslangValidator&lt;/code&gt;. I am on Ubuntu, so I can get this tool by simply &lt;code class=&quot;highlighter-rouge&quot;&gt;apt install glslang-tools&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So for example we want to validate this piece of fragment shader named &lt;code class=&quot;highlighter-rouge&quot;&gt;2d_SimplePS.hlsl&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cbuffer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uResolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uTime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fragCoord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SV_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SV_TARGET&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fragCoord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uResolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uResolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uResolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intentional&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And we use the command:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# look it up using `man glslangValidator`&lt;/span&gt;
glslangValidator &lt;span class=&quot;nt&quot;&gt;-S&lt;/span&gt; frag &lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--client&lt;/span&gt; vulkan100 &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; frag 2d_SimplePS.hlsl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the compiler will show the error message:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2d_SimplePS.hlsl
ERROR: 2d_SimplePS.hlsl:12: &lt;span class=&quot;s1&quot;&gt;'here'&lt;/span&gt; : unknown variable 
ERROR: 2d_SimplePS.hlsl:12: &lt;span class=&quot;s1&quot;&gt;';'&lt;/span&gt; : Expected 
2d_SimplePS.hlsl&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;12&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: error at column 11, HLSL parsing failed.
ERROR: 3 compilation errors.  No code generated.

SPIR-V is not generated &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;failed compile or &lt;span class=&quot;nb&quot;&gt;link&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have the file name, line number, and the detailed error messages. Great!&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="GPU" /><summary type="html">HLSL, a.k.a. High-level shader language, is used in Unity.</summary></entry><entry><title type="html">Setup Unity ML-Agents</title><link href="http://172.25.168.122:4000/simulation/2021/09/23/setup-unity-ml-agents/" rel="alternate" type="text/html" title="Setup Unity ML-Agents" /><published>2021-09-23T00:00:00+08:00</published><updated>2021-09-23T00:00:00+08:00</updated><id>http://172.25.168.122:4000/simulation/2021/09/23/setup-unity-ml-agents</id><content type="html" xml:base="http://172.25.168.122:4000/simulation/2021/09/23/setup-unity-ml-agents/">&lt;p&gt;Unity has published ML-Agents 2.x which can do reinforcement learning on Unity environment. Here is the &lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents&quot;&gt;GitHub Repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To set everything up, we first need to install Unity. I have installed 2020.3.18f1 LTS version to my Ubuntu 20.04 from Unity Hub.&lt;/p&gt;

&lt;p&gt;Then, we need to clone the &lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents&quot;&gt;ML-Agents 2.x GitHub Repo&lt;/a&gt;, for example, to &lt;code class=&quot;highlighter-rouge&quot;&gt;~/code/ml-agents/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Create a new Unity project, drag the folder &lt;code class=&quot;highlighter-rouge&quot;&gt;~/code/ml-agents/Project/Assets/ML-Agents/Examples&lt;/code&gt; to the Unity Assets, right beside the &lt;code class=&quot;highlighter-rouge&quot;&gt;Scenes&lt;/code&gt; folder.&lt;/p&gt;

&lt;p&gt;Then, we need to install some packages to Unity. Click on the menu &lt;code class=&quot;highlighter-rouge&quot;&gt;Window&lt;/code&gt;-&amp;gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Package Manager&lt;/code&gt;, select &lt;code class=&quot;highlighter-rouge&quot;&gt;Packages: In Project&lt;/code&gt;, click the &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; on its left, &lt;code class=&quot;highlighter-rouge&quot;&gt;Add package from disk...&lt;/code&gt;, and choose the &lt;code class=&quot;highlighter-rouge&quot;&gt;package.json&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/code/ml-agents/com.unity.ml-agents/&lt;/code&gt; folder, install it. Click the &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Add package from disk...&lt;/code&gt; again, and choose the &lt;code class=&quot;highlighter-rouge&quot;&gt;package.json&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/code/ml-agents/com.unity.ml-agents.extensions/&lt;/code&gt; folder, install it as well.
&lt;img src=&quot;/images/2021-09-23/package-manager.png&quot; alt=&quot;packagemanager&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also need another package from &lt;code class=&quot;highlighter-rouge&quot;&gt;Unity Registry&lt;/code&gt;. Search for &lt;code class=&quot;highlighter-rouge&quot;&gt;Input System&lt;/code&gt;, and install that.
&lt;img src=&quot;/images/2021-09-23/input-system.png&quot; alt=&quot;input-system&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We would notice that there are still errors in the console. It seems that there are some incompatible code in the example &lt;code class=&quot;highlighter-rouge&quot;&gt;PushBlockWithInput&lt;/code&gt;. I simply delete that example in &lt;code class=&quot;highlighter-rouge&quot;&gt;Assets&lt;/code&gt;. (There is one folder called &lt;code class=&quot;highlighter-rouge&quot;&gt;Assets/Examples/SharedAssets&lt;/code&gt;, it must be included.)&lt;/p&gt;

&lt;p&gt;Now, if we open the scene &lt;code class=&quot;highlighter-rouge&quot;&gt;Assets/Examples/3DBall/Scenes/3DBall.unity&lt;/code&gt;, we can click play to see the pre-trained model controling the robots. (Although there are still many error messages. ;)
&lt;img src=&quot;/images/2021-09-23/3dball.png&quot; alt=&quot;3dball&quot; /&gt;&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Simulation" /><summary type="html">Unity has published ML-Agents 2.x which can do reinforcement learning on Unity environment. Here is the GitHub Repo.</summary></entry><entry><title type="html">A simple Android virtual environment</title><link href="http://172.25.168.122:4000/simulation/2021/07/03/simple-android-env-copy/" rel="alternate" type="text/html" title="A simple Android virtual environment" /><published>2021-07-03T00:00:00+08:00</published><updated>2021-07-03T00:00:00+08:00</updated><id>http://172.25.168.122:4000/simulation/2021/07/03/simple-android-env%20copy</id><content type="html" xml:base="http://172.25.168.122:4000/simulation/2021/07/03/simple-android-env-copy/">&lt;p&gt;Creating an intelligent agent that can browse the internet like a human user does is interesting.
Many websites don’t want to provide services to artificial agents, but my opinion is that, making AI that can acqure information like human will facilitate the communication between AI and human, providing a common ground for both parties.&lt;/p&gt;

&lt;p&gt;Recently, DeepMind has open-sourced a &lt;a href=&quot;https://github.com/deepmind/android_env/&quot;&gt;virtual Android environment for RL agents&lt;/a&gt;.
Comparing to a virtual PC, a Android emulator tends to be faster.
Moreover, some applications only provide interface on mobile devices.
So I found this project interesting.&lt;/p&gt;

&lt;p&gt;However, I am not familiar with Android, so the whole project is a little too complicated to me.
I decide to make a toy/simplified version of this project, just to learn more about the details.&lt;/p&gt;

&lt;p&gt;First of all, this project doesn’t include the emulator of Android, rather, we need to &lt;a href=&quot;https://github.com/deepmind/android_env/blob/main/docs/emulator_guide.md&quot;&gt;install the emulator from Android Studio&lt;/a&gt;.
Once we created an AVD (Android Virtual Device), we should be able to start the device by this command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/Android/Sdk/emulator/emulator &lt;span class=&quot;nt&quot;&gt;-avd&lt;/span&gt; my_device
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;suppose the emulator was installed into &lt;code class=&quot;highlighter-rouge&quot;&gt;~/Android/Sdk/emulator&lt;/code&gt; folder, and &lt;code class=&quot;highlighter-rouge&quot;&gt;my_device&lt;/code&gt; is the name of the AVD I created.&lt;/p&gt;

&lt;p&gt;However, the guide suggests that we should start the AVD from Android Studio. I am not sure why we should not start the emulator without Android Studio.
If you happen to know why, please let me know.&lt;/p&gt;

&lt;p&gt;You can probably see a GUI like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-07-03-simple-android-env/avd.png&quot; alt=&quot;avd&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And you can use your mouse and keyboard to play with this virtual device.&lt;/p&gt;

&lt;p&gt;Then, we will want to send mouse and keyboard events to the virtual device programmably.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.android.com/studio/run/emulator-console&quot;&gt;Here&lt;/a&gt; is a documentation for how to send commands to the emulator. 
However, this is a very simple explanation of the commands, and it is not clear how to send mouse events (with the command &lt;code class=&quot;highlighter-rouge&quot;&gt;event&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;After reading this &lt;a href=&quot;https://github.com/deepmind/android_env/blob/main/android_env/components/emulator_console.py#L244&quot;&gt;source file&lt;/a&gt; from the AndroidEnv, I discovered that we can use the command &lt;code class=&quot;highlighter-rouge&quot;&gt;event mouse&lt;/code&gt; to send mouse events to the virtual device.
This is not mentioned in the documentation, and I don’t know why.&lt;/p&gt;

&lt;p&gt;Once we are able to send events (actions) to the virtual device, we also would like to get the feedback from the device.
According to the documentation, the way we can achieve that is using the &lt;code class=&quot;highlighter-rouge&quot;&gt;screenrecord screenshot&lt;/code&gt; command.
However, this command only takes a path to a folder as its parameter (not a path to a file) and doesn’t return the filename it has created, so we need to clean the folder beforehand and probably use &lt;code class=&quot;highlighter-rouge&quot;&gt;glob&lt;/code&gt; to get whatever created in that folder after the command returns.
Also, the command returns before the file has been created, so one need to make sure the file has been created before using &lt;code class=&quot;highlighter-rouge&quot;&gt;glob&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The file created by &lt;code class=&quot;highlighter-rouge&quot;&gt;screenrecord&lt;/code&gt; is a PNG file, and we can use, for example, Python package &lt;code class=&quot;highlighter-rouge&quot;&gt;imageio&lt;/code&gt; to read that file as a Numpy array, and feed that to the neural network.&lt;/p&gt;

&lt;p&gt;Now we have actions and observations, and this is basically the simplest version of an Android envrionment.
Instead of using &lt;a href=&quot;https://github.com/deepmind/android_env/&quot;&gt;DeepMind/AndroidEnv&lt;/a&gt;, we can manipulate the virtual device directly.&lt;/p&gt;

&lt;p&gt;I’ll be happy to see some intelligent agents that can operate the virtual mobile phone and discover the Internet on their own in a more human-like way.&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Simulation" /><summary type="html">Creating an intelligent agent that can browse the internet like a human user does is interesting. Many websites don’t want to provide services to artificial agents, but my opinion is that, making AI that can acqure information like human will facilitate the communication between AI and human, providing a common ground for both parties.</summary></entry><entry><title type="html">How to use PyCUDA to bring significant speedup</title><link href="http://172.25.168.122:4000/cuda/2020/08/02/pycuda/" rel="alternate" type="text/html" title="How to use PyCUDA to bring significant speedup" /><published>2020-08-02T00:00:00+08:00</published><updated>2020-08-02T00:00:00+08:00</updated><id>http://172.25.168.122:4000/cuda/2020/08/02/pycuda</id><content type="html" xml:base="http://172.25.168.122:4000/cuda/2020/08/02/pycuda/">&lt;p&gt;Imagine that we have designed an computational experiment in Python, and we waited 3 days for the results, and after that, unfortunately we discovered there was a typo or a small bug in the source code. What do you think we would say when we restart the experiment? I would hope that the experiment could be run in half a hour.&lt;/p&gt;

&lt;p&gt;It is possible, by making the code parallized.&lt;/p&gt;

&lt;p&gt;CUDA is a C++-like program language for parallel programs which can run on Nvidia GPU. &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit&quot;&gt;CUDA website&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PyCUDA is an open source Python interface to compile CUDA source code on the fly and execute it. &lt;a href=&quot;https://documen.tician.de/pycuda/&quot;&gt;PyCUDA documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here we show an example of using CUDA and PyCUDA to rewrite a Python program.&lt;/p&gt;

&lt;p&gt;Source code: &lt;a href=&quot;https://github.com/liusida/JacksCarRental-via-PyCUDA&quot;&gt;GitHub repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The file &lt;code class=&quot;highlighter-rouge&quot;&gt;car_rental.py&lt;/code&gt; is a Python program. It is slow because there are huge nested loops. We can exam this by searching for the keywords &lt;code class=&quot;highlighter-rouge&quot;&gt;while True&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The file &lt;code class=&quot;highlighter-rouge&quot;&gt;car_rental_cuda.py&lt;/code&gt; is the CUDA-optimized version of the original program. The &lt;code class=&quot;highlighter-rouge&quot;&gt;gpu_policy_evaluation&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;gpu_policy_improvement&lt;/code&gt; are two kernels (CUDA interfaces) that can run 21*21 (num_state=21) threads in parallel. In this code, it prepares the pre-defined constant vairables and read in the CUDA source file &lt;code class=&quot;highlighter-rouge&quot;&gt;car_rental_cuda.py.cu&lt;/code&gt;, compiles them on the fly, and expose the interfaces as Python functions.&lt;/p&gt;

&lt;p&gt;By running them, we can get the results in the &lt;code class=&quot;highlighter-rouge&quot;&gt;images/&lt;/code&gt; folder. And we can see the CUDA version only takes 6 seconds while the original version would take more than a hour.&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="CUDA" /><summary type="html">Imagine that we have designed an computational experiment in Python, and we waited 3 days for the results, and after that, unfortunately we discovered there was a typo or a small bug in the source code. What do you think we would say when we restart the experiment? I would hope that the experiment could be run in half a hour.</summary></entry><entry><title type="html">Nuance in Monty Hall Paradox</title><link href="http://172.25.168.122:4000/old/2018/07/02/nuance-in-monty-hall-paradox/" rel="alternate" type="text/html" title="Nuance in Monty Hall Paradox" /><published>2018-07-02T00:00:00+08:00</published><updated>2018-07-02T00:00:00+08:00</updated><id>http://172.25.168.122:4000/old/2018/07/02/nuance-in-monty-hall-paradox</id><content type="html" xml:base="http://172.25.168.122:4000/old/2018/07/02/nuance-in-monty-hall-paradox/">&lt;p&gt;Marilyn vos Savant has made a mistake. She knew the game show Let’s Make a Deal too well, that she assumed the rules of the game show also applied to the question she was asked.&lt;/p&gt;

&lt;p&gt;On the website of Marilyn vos Savant, the original question could be found here http://marilynvossavant.com/game-show-problem/ :&lt;/p&gt;

&lt;p&gt;“Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say #3, which has a goat. He says to you, “Do you want to pick door #2?” Is it to your advantage to switch your choice of doors?”&lt;/p&gt;

&lt;p&gt;Interestingly, the original question never indicate any established rules of Let’s Make a Deal, which is critical to this question. Examination of the original question reveals that the host is not mandatory to open another door, i.e., the host could just open the door that is choosen. In this case, which the host is free to choose opening the door directly or opening another door for you then asking you to switch, his choice of opening another door is probably in order to lead you away from winning the car. (Unforturenately, the rule of winning what ever showed behind the door is not metioned in the question either.)&lt;/p&gt;

&lt;p&gt;For this reason, the “Marilyn’s question” is different from “Monty Hall Paradox.”&lt;/p&gt;

&lt;p&gt;So how to solve “Marilyn’s question”?&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Old" /><summary type="html">Marilyn vos Savant has made a mistake. She knew the game show Let’s Make a Deal too well, that she assumed the rules of the game show also applied to the question she was asked.</summary></entry><entry><title type="html">What is Mathematics According to Keith Devlin</title><link href="http://172.25.168.122:4000/old/2018/06/27/what-is-mathematics/" rel="alternate" type="text/html" title="What is Mathematics According to Keith Devlin" /><published>2018-06-27T00:00:00+08:00</published><updated>2018-06-27T00:00:00+08:00</updated><id>http://172.25.168.122:4000/old/2018/06/27/what-is-mathematics</id><content type="html" xml:base="http://172.25.168.122:4000/old/2018/06/27/what-is-mathematics/">&lt;p&gt;上午读了Keith Devlin教授的课程背景材料，有点感触，摘抄了几段对我很有启发的文字，以备日后参考。&lt;/p&gt;

&lt;p&gt;In Keith Devlin’s book ‘Introduction to Mathematical Thinking’, he writes,&lt;/p&gt;

&lt;p&gt;“Virtually nothing (with just two further advances, both from the 17th century: calculus and probability theory) from the last three hundred years has found its way into the classroom. Yet most of the mathematics used in today’s world was developed in the last two hundred years! As a result, anyone whose view of mathematics is confined to what is typically taught in schools is unlikely to appreciate that research in mathematics is a thriving, worldwide activity, or to accept that mathematics permeates, often to a considerable extent, most walks of present-day life and society.”&lt;/p&gt;

&lt;p&gt;小学到高中课堂里所涉及到的数学，基本都是三百年前的东西了，说三百年还是给面子了，因为三百年前的也就是微积分和概率，其他的就更古老了。但是呢，今天我们在社会上用到的数学大多都是近两百年内发展出来的。所以如果认为数学就是我们中学课本里教的那些的话，就无法欣赏最新的数学研究发展咯。&lt;/p&gt;

&lt;p&gt;“…mathematical notation no more is mathematics than musical notation is music. … In 1623, Galileo wrote, ‘The great book of nature can be read only by those who know the language in which it was written. And this language is mathematics.’…”&lt;/p&gt;

&lt;p&gt;说数学符号就是数学，就跟说音乐符号就是音乐一样，其实他只是数学的语言，并不是数学本身。但是呢，伽利略说，这个自然界是数学写成的。所以学习数学语言挺重要啊。&lt;/p&gt;

&lt;p&gt;“As one of the greatest creations of human civilization, mathematics should be taught alongside science, literature, history, and art in order to pass along the jewels of our culture from one generation to the next. We humans are far more than the jobs we do and the careers we pursue.”&lt;/p&gt;

&lt;p&gt;数学作为人类创造的最伟大的东西之一，跟科学、文学、历史和艺术一并，都应该教给下一代，这是人类最珍贵的东西了。我们人类嘛，不仅仅是个干活的工人，我们值更多。&lt;/p&gt;

&lt;p&gt;“…those skills (use mathematics as a tool) fall into two categories. … The second category comprises people who can take a new problem, say in manufacturing, identify and describe key features of the problem mathematically, and use that mathematical description to analyze the problem in a precise fashion. … I propose to give them one (name): innovative mathematical thinkers.”&lt;/p&gt;

&lt;p&gt;使用数学可以有两类，一类是拿到已经定义好的数学问题想办法计算结果，第二类是在实际生活中把遇到的问题数学化，并可以用精确地风格分析这些问题。我称这些人：有创意的数学思考者。&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Old" /><summary type="html">上午读了Keith Devlin教授的课程背景材料，有点感触，摘抄了几段对我很有启发的文字，以备日后参考。</summary></entry><entry><title type="html">Dynamic NN Allowing Additional Evidence?</title><link href="http://172.25.168.122:4000/2018/05/29/dynamic-nn/" rel="alternate" type="text/html" title="Dynamic NN Allowing Additional Evidence?" /><published>2018-05-29T00:00:00+08:00</published><updated>2018-05-29T00:00:00+08:00</updated><id>http://172.25.168.122:4000/2018/05/29/dynamic-nn</id><content type="html" xml:base="http://172.25.168.122:4000/2018/05/29/dynamic-nn/">&lt;p&gt;We have traditional Neural Network (NN), with static structure like this:&lt;/p&gt;

&lt;p&gt;signal -&amp;gt; input -&amp;gt; hidden layer -&amp;gt; prediction =?= truth&lt;/p&gt;

&lt;p&gt;-&amp;gt;  means to propagate forward
=?= means to minimize difference&lt;/p&gt;

&lt;p&gt;What if we already trained one model like that and there is another evidence (signal) coming in front of us? Can we add the signal into the model dynamically without abandon what has been trained already?&lt;/p&gt;

&lt;p&gt;I think we should use Bayesian Theory, but I havn’t figure out how yet.&lt;/p&gt;</content><author><name>Sida Liu</name></author><summary type="html">We have traditional Neural Network (NN), with static structure like this:</summary></entry><entry><title type="html">Why does the person with highest IQ not become the most successful one?</title><link href="http://172.25.168.122:4000/2017/09/27/highest-IQ/" rel="alternate" type="text/html" title="Why does the person with highest IQ not become the most successful one?" /><published>2017-09-27T00:00:00+08:00</published><updated>2017-09-27T00:00:00+08:00</updated><id>http://172.25.168.122:4000/2017/09/27/highest-IQ</id><content type="html" xml:base="http://172.25.168.122:4000/2017/09/27/highest-IQ/">&lt;p&gt;I heard about the “&lt;a href=&quot;https://euler.epfl.ch/files/content/sites/euler/files/users/144617/public/LubinskiPersson.pdf&quot;&gt;Study of Mathematically Precocious Youth After 35 Years&lt;/a&gt;” years ago, but after studying machine learning, especially the generalization problem, I guess I have glanced some possible reasons.&lt;/p&gt;

&lt;p&gt;While a human is learning, the process is more or less like the machine learning. The talent, the IQ testing result, can somehow prove the human has a more complex brain, just like the neural networks have more complex architectures. Unfortunately, overfitting often occurs when a model with more complex architecture learning. When a model stuck at overfitting, the training error will go down steadily while the validation error will become larger. This problem is called generalization problem. In the context of machine learning, we have several tricks to partially solve it. Here is a list of methods and their analogies of human learning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;More data. This is the best method both for machine learning and human learning. If we are smart youths, just keep learning new stuff, and we can avoid overfitting to the knowledge we learn, and generalize better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropout. This is my favorite method. Don’t study all the time. Do something else, or just do nothing, maybe sleep. And then we will find our ability of generalization improved. Sounds nice!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding noise to input. This is also a practical method. If we need study something several times to master a perticular idea, maybe after we feel confident enough, we can add some noise, e.g. maybe use an alternative material, or maybe focusing on different details.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;L2 regularization. This is pushing all the unrelated weights to be near zero. When we study, maybe after several rounds of learning, we ask ourselves to not doubt what we learned. If we are not 100% sure, then don’t trust it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Sida Liu</name></author><summary type="html">I heard about the “Study of Mathematically Precocious Youth After 35 Years” years ago, but after studying machine learning, especially the generalization problem, I guess I have glanced some possible reasons.</summary></entry></feed>