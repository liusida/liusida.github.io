<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-03T11:51:12+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sida Liu</title><subtitle>The world is so complex that we cannot stop learning.</subtitle><author><name>Sida Liu</name></author><entry><title type="html">A simple Android virtual environment</title><link href="http://localhost:4000/simulation/2021/07/03/simple-android-env/" rel="alternate" type="text/html" title="A simple Android virtual environment" /><published>2021-07-03T00:00:00+08:00</published><updated>2021-07-03T00:00:00+08:00</updated><id>http://localhost:4000/simulation/2021/07/03/simple-android-env</id><content type="html" xml:base="http://localhost:4000/simulation/2021/07/03/simple-android-env/">&lt;p&gt;Creating an intelligent agent that can browse the internet like a human user does is interesting.
Many websites don’t want to provide services to artificial agents, but my opinion is that, making AI that can acqure information like human will facilitate the communication between AI and human, providing a common ground for both parties.&lt;/p&gt;

&lt;p&gt;Recently, DeepMind has open-sourced a &lt;a href=&quot;https://github.com/deepmind/android_env/&quot;&gt;virtual Android environment for RL agents&lt;/a&gt;.
Comparing to a virtual PC, a Android emulator tends to be faster.
Moreover, some applications only provide interface on mobile devices.
So I found this project interesting.&lt;/p&gt;

&lt;p&gt;However, I am not familiar with Android, so the whole project is a little too complicated to me.
I decide to make a toy/simplified version of this project, just to learn more about the details.&lt;/p&gt;

&lt;p&gt;First of all, this project doesn’t include the emulator of Android, rather, we need to &lt;a href=&quot;https://github.com/deepmind/android_env/blob/main/docs/emulator_guide.md&quot;&gt;install the emulator from Android Studio&lt;/a&gt;.
Once we created an AVD (Android Virtual Device), we should be able to start the device by this command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/Android/Sdk/emulator/emulator &lt;span class=&quot;nt&quot;&gt;-avd&lt;/span&gt; my_device
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;suppose the emulator was installed into &lt;code class=&quot;highlighter-rouge&quot;&gt;~/Android/Sdk/emulator&lt;/code&gt; folder, and &lt;code class=&quot;highlighter-rouge&quot;&gt;my_device&lt;/code&gt; is the name of the AVD I created.&lt;/p&gt;

&lt;p&gt;However, the guide suggests that we should start the AVD from Android Studio. I am not sure why we should not start the emulator without Android Studio.
If you happen to know why, please let me know.&lt;/p&gt;

&lt;p&gt;You can probably see a GUI like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-07-03-simple-android-env/avd.png&quot; alt=&quot;avd&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And you can use your mouse and keyboard to play with this virtual device.&lt;/p&gt;

&lt;p&gt;Then, we will want to send mouse and keyboard events to the virtual device programmably.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.android.com/studio/run/emulator-console&quot;&gt;Here&lt;/a&gt; is a documentation for how to send commands to the emulator. 
However, this is a very simple explanation of the commands, and it is not clear how to send mouse events (with the command &lt;code class=&quot;highlighter-rouge&quot;&gt;event&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;After reading this &lt;a href=&quot;https://github.com/deepmind/android_env/blob/main/android_env/components/emulator_console.py#L244&quot;&gt;source file&lt;/a&gt; from the AndroidEnv, I discovered that we can use the command &lt;code class=&quot;highlighter-rouge&quot;&gt;event mouse&lt;/code&gt; to send mouse events to the virtual device.
This is not mentioned in the documentation, and I don’t know why.&lt;/p&gt;

&lt;p&gt;Once we are able to send events (actions) to the virtual device, we also would like to get the feedback from the device.
According to the documentation, the way we can achieve that is using the &lt;code class=&quot;highlighter-rouge&quot;&gt;screenrecord screenshot&lt;/code&gt; command.
However, this command only takes a path to a folder as its parameter (not a path to a file) and doesn’t return the filename it has created, so we need to clean the folder beforehand and probably use &lt;code class=&quot;highlighter-rouge&quot;&gt;glob&lt;/code&gt; to get whatever created in that folder after the command returns.
Also, the command returns before the file has been created, so one need to make sure the file has been created before using &lt;code class=&quot;highlighter-rouge&quot;&gt;glob&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The file created by &lt;code class=&quot;highlighter-rouge&quot;&gt;screenrecord&lt;/code&gt; is a PNG file, and we can use, for example, Python package &lt;code class=&quot;highlighter-rouge&quot;&gt;imageio&lt;/code&gt; to read that file as a Numpy array, and feed that to the neural network.&lt;/p&gt;

&lt;p&gt;Now we have actions and observations, and this is basically the simplest version of an Android envrionment.
Instead of using &lt;a href=&quot;https://github.com/deepmind/android_env/&quot;&gt;DeepMind/AndroidEnv&lt;/a&gt;, we can manipulate the virtual device directly.&lt;/p&gt;

&lt;p&gt;I’ll be happy to see some intelligent agents that can operate the virtual mobile phone and discover the Internet on their own in a more human-like way.&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Simulation" /><summary type="html">Creating an intelligent agent that can browse the internet like a human user does is interesting. Many websites don’t want to provide services to artificial agents, but my opinion is that, making AI that can acqure information like human will facilitate the communication between AI and human, providing a common ground for both parties.</summary></entry><entry><title type="html">How to use PyCUDA to bring significant speedup</title><link href="http://localhost:4000/cuda/2020/08/02/pycuda/" rel="alternate" type="text/html" title="How to use PyCUDA to bring significant speedup" /><published>2020-08-02T00:00:00+08:00</published><updated>2020-08-02T00:00:00+08:00</updated><id>http://localhost:4000/cuda/2020/08/02/pycuda</id><content type="html" xml:base="http://localhost:4000/cuda/2020/08/02/pycuda/">&lt;p&gt;Imagine that we have designed an computational experiment in Python, and we waited 3 days for the results, and after that, unfortunately we discovered there was a typo or a small bug in the source code. What do you think we would say when we restart the experiment? I would hope that the experiment could be run in half a hour.&lt;/p&gt;

&lt;p&gt;It is possible, by making the code parallized.&lt;/p&gt;

&lt;p&gt;CUDA is a C++-like program language for parallel programs which can run on Nvidia GPU. &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit&quot;&gt;CUDA website&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PyCUDA is an open source Python interface to compile CUDA source code on the fly and execute it. &lt;a href=&quot;https://documen.tician.de/pycuda/&quot;&gt;PyCUDA documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here we show an example of using CUDA and PyCUDA to rewrite a Python program.&lt;/p&gt;

&lt;p&gt;Source code: &lt;a href=&quot;https://github.com/liusida/JacksCarRental-via-PyCUDA&quot;&gt;GitHub repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The file &lt;code class=&quot;highlighter-rouge&quot;&gt;car_rental.py&lt;/code&gt; is a Python program. It is slow because there are huge nested loops. We can exam this by searching for the keywords &lt;code class=&quot;highlighter-rouge&quot;&gt;while True&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The file &lt;code class=&quot;highlighter-rouge&quot;&gt;car_rental_cuda.py&lt;/code&gt; is the CUDA-optimized version of the original program. The &lt;code class=&quot;highlighter-rouge&quot;&gt;gpu_policy_evaluation&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;gpu_policy_improvement&lt;/code&gt; are two kernels (CUDA interfaces) that can run 21*21 (num_state=21) threads in parallel. In this code, it prepares the pre-defined constant vairables and read in the CUDA source file &lt;code class=&quot;highlighter-rouge&quot;&gt;car_rental_cuda.py.cu&lt;/code&gt;, compiles them on the fly, and expose the interfaces as Python functions.&lt;/p&gt;

&lt;p&gt;By running them, we can get the results in the &lt;code class=&quot;highlighter-rouge&quot;&gt;images/&lt;/code&gt; folder. And we can see the CUDA version only takes 6 seconds while the original version would take more than a hour.&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="CUDA" /><summary type="html">Imagine that we have designed an computational experiment in Python, and we waited 3 days for the results, and after that, unfortunately we discovered there was a typo or a small bug in the source code. What do you think we would say when we restart the experiment? I would hope that the experiment could be run in half a hour.</summary></entry><entry><title type="html">Nuance in Monty Hall Paradox</title><link href="http://localhost:4000/old/2018/07/02/nuance-in-monty-hall-paradox/" rel="alternate" type="text/html" title="Nuance in Monty Hall Paradox" /><published>2018-07-02T00:00:00+08:00</published><updated>2018-07-02T00:00:00+08:00</updated><id>http://localhost:4000/old/2018/07/02/nuance-in-monty-hall-paradox</id><content type="html" xml:base="http://localhost:4000/old/2018/07/02/nuance-in-monty-hall-paradox/">&lt;p&gt;Marilyn vos Savant has made a mistake. She knew the game show Let’s Make a Deal too well, that she assumed the rules of the game show also applied to the question she was asked.&lt;/p&gt;

&lt;p&gt;On the website of Marilyn vos Savant, the original question could be found here http://marilynvossavant.com/game-show-problem/ :&lt;/p&gt;

&lt;p&gt;“Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say #1, and the host, who knows what’s behind the doors, opens another door, say #3, which has a goat. He says to you, “Do you want to pick door #2?” Is it to your advantage to switch your choice of doors?”&lt;/p&gt;

&lt;p&gt;Interestingly, the original question never indicate any established rules of Let’s Make a Deal, which is critical to this question. Examination of the original question reveals that the host is not mandatory to open another door, i.e., the host could just open the door that is choosen. In this case, which the host is free to choose opening the door directly or opening another door for you then asking you to switch, his choice of opening another door is probably in order to lead you away from winning the car. (Unforturenately, the rule of winning what ever showed behind the door is not metioned in the question either.)&lt;/p&gt;

&lt;p&gt;For this reason, the “Marilyn’s question” is different from “Monty Hall Paradox.”&lt;/p&gt;

&lt;p&gt;So how to solve “Marilyn’s question”?&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Old" /><summary type="html">Marilyn vos Savant has made a mistake. She knew the game show Let’s Make a Deal too well, that she assumed the rules of the game show also applied to the question she was asked.</summary></entry><entry><title type="html">What is Mathematics According to Keith Devlin</title><link href="http://localhost:4000/old/2018/06/27/what-is-mathematics/" rel="alternate" type="text/html" title="What is Mathematics According to Keith Devlin" /><published>2018-06-27T00:00:00+08:00</published><updated>2018-06-27T00:00:00+08:00</updated><id>http://localhost:4000/old/2018/06/27/what-is-mathematics</id><content type="html" xml:base="http://localhost:4000/old/2018/06/27/what-is-mathematics/">&lt;p&gt;上午读了Keith Devlin教授的课程背景材料，有点感触，摘抄了几段对我很有启发的文字，以备日后参考。&lt;/p&gt;

&lt;p&gt;In Keith Devlin’s book ‘Introduction to Mathematical Thinking’, he writes,&lt;/p&gt;

&lt;p&gt;“Virtually nothing (with just two further advances, both from the 17th century: calculus and probability theory) from the last three hundred years has found its way into the classroom. Yet most of the mathematics used in today’s world was developed in the last two hundred years! As a result, anyone whose view of mathematics is confined to what is typically taught in schools is unlikely to appreciate that research in mathematics is a thriving, worldwide activity, or to accept that mathematics permeates, often to a considerable extent, most walks of present-day life and society.”&lt;/p&gt;

&lt;p&gt;小学到高中课堂里所涉及到的数学，基本都是三百年前的东西了，说三百年还是给面子了，因为三百年前的也就是微积分和概率，其他的就更古老了。但是呢，今天我们在社会上用到的数学大多都是近两百年内发展出来的。所以如果认为数学就是我们中学课本里教的那些的话，就无法欣赏最新的数学研究发展咯。&lt;/p&gt;

&lt;p&gt;“…mathematical notation no more is mathematics than musical notation is music. … In 1623, Galileo wrote, ‘The great book of nature can be read only by those who know the language in which it was written. And this language is mathematics.’…”&lt;/p&gt;

&lt;p&gt;说数学符号就是数学，就跟说音乐符号就是音乐一样，其实他只是数学的语言，并不是数学本身。但是呢，伽利略说，这个自然界是数学写成的。所以学习数学语言挺重要啊。&lt;/p&gt;

&lt;p&gt;“As one of the greatest creations of human civilization, mathematics should be taught alongside science, literature, history, and art in order to pass along the jewels of our culture from one generation to the next. We humans are far more than the jobs we do and the careers we pursue.”&lt;/p&gt;

&lt;p&gt;数学作为人类创造的最伟大的东西之一，跟科学、文学、历史和艺术一并，都应该教给下一代，这是人类最珍贵的东西了。我们人类嘛，不仅仅是个干活的工人，我们值更多。&lt;/p&gt;

&lt;p&gt;“…those skills (use mathematics as a tool) fall into two categories. … The second category comprises people who can take a new problem, say in manufacturing, identify and describe key features of the problem mathematically, and use that mathematical description to analyze the problem in a precise fashion. … I propose to give them one (name): innovative mathematical thinkers.”&lt;/p&gt;

&lt;p&gt;使用数学可以有两类，一类是拿到已经定义好的数学问题想办法计算结果，第二类是在实际生活中把遇到的问题数学化，并可以用精确地风格分析这些问题。我称这些人：有创意的数学思考者。&lt;/p&gt;</content><author><name>Sida Liu</name></author><category term="Old" /><summary type="html">上午读了Keith Devlin教授的课程背景材料，有点感触，摘抄了几段对我很有启发的文字，以备日后参考。</summary></entry><entry><title type="html">Dynamic NN Allowing Additional Evidence?</title><link href="http://localhost:4000/2018/05/29/dynamic-nn/" rel="alternate" type="text/html" title="Dynamic NN Allowing Additional Evidence?" /><published>2018-05-29T00:00:00+08:00</published><updated>2018-05-29T00:00:00+08:00</updated><id>http://localhost:4000/2018/05/29/dynamic-nn</id><content type="html" xml:base="http://localhost:4000/2018/05/29/dynamic-nn/">&lt;p&gt;We have traditional Neural Network (NN), with static structure like this:&lt;/p&gt;

&lt;p&gt;signal -&amp;gt; input -&amp;gt; hidden layer -&amp;gt; prediction =?= truth&lt;/p&gt;

&lt;p&gt;-&amp;gt;  means to propagate forward
=?= means to minimize difference&lt;/p&gt;

&lt;p&gt;What if we already trained one model like that and there is another evidence (signal) coming in front of us? Can we add the signal into the model dynamically without abandon what has been trained already?&lt;/p&gt;

&lt;p&gt;I think we should use Bayesian Theory, but I havn’t figure out how yet.&lt;/p&gt;</content><author><name>Sida Liu</name></author><summary type="html">We have traditional Neural Network (NN), with static structure like this:</summary></entry><entry><title type="html">Why does the person with highest IQ not become the most successful one?</title><link href="http://localhost:4000/2017/09/27/highest-IQ/" rel="alternate" type="text/html" title="Why does the person with highest IQ not become the most successful one?" /><published>2017-09-27T00:00:00+08:00</published><updated>2017-09-27T00:00:00+08:00</updated><id>http://localhost:4000/2017/09/27/highest-IQ</id><content type="html" xml:base="http://localhost:4000/2017/09/27/highest-IQ/">&lt;p&gt;I heard about the “&lt;a href=&quot;https://euler.epfl.ch/files/content/sites/euler/files/users/144617/public/LubinskiPersson.pdf&quot;&gt;Study of Mathematically Precocious Youth After 35 Years&lt;/a&gt;” years ago, but after studying machine learning, especially the generalization problem, I guess I have glanced some possible reasons.&lt;/p&gt;

&lt;p&gt;While a human is learning, the process is more or less like the machine learning. The talent, the IQ testing result, can somehow prove the human has a more complex brain, just like the neural networks have more complex architectures. Unfortunately, overfitting often occurs when a model with more complex architecture learning. When a model stuck at overfitting, the training error will go down steadily while the validation error will become larger. This problem is called generalization problem. In the context of machine learning, we have several tricks to partially solve it. Here is a list of methods and their analogies of human learning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;More data. This is the best method both for machine learning and human learning. If we are smart youths, just keep learning new stuff, and we can avoid overfitting to the knowledge we learn, and generalize better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropout. This is my favorite method. Don’t study all the time. Do something else, or just do nothing, maybe sleep. And then we will find our ability of generalization improved. Sounds nice!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding noise to input. This is also a practical method. If we need study something several times to master a perticular idea, maybe after we feel confident enough, we can add some noise, e.g. maybe use an alternative material, or maybe focusing on different details.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;L2 regularization. This is pushing all the unrelated weights to be near zero. When we study, maybe after several rounds of learning, we ask ourselves to not doubt what we learned. If we are not 100% sure, then don’t trust it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Sida Liu</name></author><summary type="html">I heard about the “Study of Mathematically Precocious Youth After 35 Years” years ago, but after studying machine learning, especially the generalization problem, I guess I have glanced some possible reasons.</summary></entry><entry><title type="html">Use Tensorflow to Compute Gradient</title><link href="http://localhost:4000/2017/09/24/use-tensorflow-to-compute-gradient/" rel="alternate" type="text/html" title="Use Tensorflow to Compute Gradient" /><published>2017-09-24T00:00:00+08:00</published><updated>2017-09-24T00:00:00+08:00</updated><id>http://localhost:4000/2017/09/24/use-tensorflow-to-compute-gradient</id><content type="html" xml:base="http://localhost:4000/2017/09/24/use-tensorflow-to-compute-gradient/">&lt;p&gt;In most of Tensorflow tutorials, we use minimize(loss) to automatically update parameters of the model.&lt;/p&gt;

&lt;p&gt;In fact, minimize() is an integration of two steps: computing gradients, and applying the gradients to update parameters.&lt;/p&gt;

&lt;p&gt;Let’s take a look at an example:&lt;/p&gt;

\[Y = (100 - 3W - B)^2\]

&lt;p&gt;What is the gradient of W and B &lt;strong&gt;when W=1.0, B=1.0&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;We can calculate them by hand:&lt;/p&gt;

&lt;p&gt;let \(N = 100 - 3W - B\), so that \(Y = N^2\)&lt;/p&gt;

\[\frac{\partial{Y}}{\partial{W}} = 
\frac{\partial{Y}}{\partial{N}} * \frac{\partial{N}}{\partial{W}} = 
2N * 3 = 600 - 18W - 6B = 576\]

\[\frac{\partial{Y}}{\partial{B}} = 
\frac{\partial{Y}}{\partial{N}} * \frac{\partial{N}}{\partial{B}} = 
2N * 1 = 200 - 3W - B = 196\]

&lt;p&gt;ok, now let use tensorflow to compute that:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# make an example:
# Y = (100 - W X - B)^2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#the lr here is not about gradient computing. it only effect when appling
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ops&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grads_and_vars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ops&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compute_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# we can modify the gradient here and then:
# Op_update = Ops.apply_gradients(grads_and_vars)
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads_and_vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;run it, and we get:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(-576.0, 1.0), (-192.0, 1.0)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So next time your professor ask you to implement a back-propagation for some complex networks by your self, maybe this trick can help you double-check your implementation. Hooray!&lt;/p&gt;</content><author><name>Sida Liu</name></author><summary type="html">In most of Tensorflow tutorials, we use minimize(loss) to automatically update parameters of the model.</summary></entry><entry><title type="html">Scree of PCA(Principal Component Analysis)</title><link href="http://localhost:4000/2017/09/23/scree-of-pca/" rel="alternate" type="text/html" title="Scree of PCA(Principal Component Analysis)" /><published>2017-09-23T00:00:00+08:00</published><updated>2017-09-23T00:00:00+08:00</updated><id>http://localhost:4000/2017/09/23/scree-of-pca</id><content type="html" xml:base="http://localhost:4000/2017/09/23/scree-of-pca/">&lt;p&gt;I learned the concept of PCA today, and found out this method of reducing dimension is quite terse.&lt;/p&gt;

&lt;p&gt;If we do PCA to a 40-d dataset, reduce it into a 2-d dataset, it simply choose the 2 most “Principal Components”, i.e. the 2 most “important” dimensions, and drop others.&lt;/p&gt;

&lt;p&gt;So, before we do PCA, we’d better do a scree of PCA, to plot the proportion of variance of each dimension.&lt;/p&gt;

&lt;p&gt;take a look at &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/&quot;&gt;this implementation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-09-23-scree-of-pca/proportion-of-variance.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, I think we are quite safe to simply drop dimensions after PC30, i.e. we can use PCA to reduce the dataset to 30-d quite safely. (and then we may use t-sne, a more time-consuming method.)&lt;/p&gt;</content><author><name>Sida Liu</name></author><summary type="html">I learned the concept of PCA today, and found out this method of reducing dimension is quite terse.</summary></entry><entry><title type="html">Learning Rate is Too Large</title><link href="http://localhost:4000/2017/09/09/learning-rate-too-large/" rel="alternate" type="text/html" title="Learning Rate is Too Large" /><published>2017-09-09T00:00:00+08:00</published><updated>2017-09-09T00:00:00+08:00</updated><id>http://localhost:4000/2017/09/09/learning-rate-too-large</id><content type="html" xml:base="http://localhost:4000/2017/09/09/learning-rate-too-large/">&lt;p&gt;What if I see a training accuracy scalar graphic like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-09-09-learning-rate-too-large/accuracy-1.png&quot; alt=&quot;Accuracy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The accuracy curve of training mini-batch is going down a little bit over time after reached a relative high point. That might tell me the learning rate is too large.&lt;/p&gt;

&lt;p&gt;When the learning rate is too large, the optimizer function can not converge the loss by adding derivative to variables–every step is too large, and the loss will become biger and biger.&lt;/p&gt;</content><author><name>Sida Liu</name></author><summary type="html">What if I see a training accuracy scalar graphic like this:</summary></entry><entry><title type="html">Manipulating Tensorboard</title><link href="http://localhost:4000/2017/09/08/manipulating-tensorboard/" rel="alternate" type="text/html" title="Manipulating Tensorboard" /><published>2017-09-08T00:00:00+08:00</published><updated>2017-09-08T00:00:00+08:00</updated><id>http://localhost:4000/2017/09/08/manipulating-tensorboard</id><content type="html" xml:base="http://localhost:4000/2017/09/08/manipulating-tensorboard/">&lt;p&gt;Tensorboard is a very useful tool for visualizing the logs of Tensorflow. It is now an independent project on GitHub, here’s the &lt;a href=&quot;https://github.com/tensorflow/tensorboard&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the past, if we were doing small projects, we usually printed some log information on the screen or wrote them into log files. The disadvantage is that if there are so many outputs, we can easily get lost in them.&lt;/p&gt;

&lt;p&gt;So I think Tensorboard is a very helpful tool since it can re-organize log information, and present it in a Web form.&lt;/p&gt;

&lt;p&gt;There are several features which I think is worth to talk about:&lt;/p&gt;

&lt;h3 id=&quot;1-organize-logs-into-sub-directories&quot;&gt;1 Organize logs into sub-directories.&lt;/h3&gt;

&lt;p&gt;I selected a certain directory for all tensorboard logs, say it’s ~/tensorboard_logdir . And then, say, I had a project called Project_One, and I can just make a sub-directory in it. And every time I run the program, I can write log files into a sub-sub-directory which has a name of current time.&lt;/p&gt;

&lt;p&gt;When I run &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorboard --logdir=~/tensorboard_logdir&lt;/code&gt;, I get this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-09-08-tensorboard/tensorboard_project_one.png&quot; alt=&quot;Project One&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-the-histogram&quot;&gt;2 The Histogram.&lt;/h3&gt;

&lt;p&gt;We usually use histograms in bar style. The Tensorboard doesn’t.&lt;/p&gt;

&lt;p&gt;According to dandelionmane, the developer of tensorboard, &lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/5381&quot;&gt;the y-axis of the histogram is Density&lt;/a&gt;, but I think it is Frequency. For example, I created an array with 7 items–[1,2,3,4,5,6,7], and I wrote them to a histogram, I’d got this interesting result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-09-08-tensorboard/seven_items.png&quot; alt=&quot;Seven Items&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I added all values of those points, and I’d got 7. There is an interesting phenomena on the right, the item [7] was reperesented by three points which are all roughly 0.3, add up to 1. So, I think if there are not so many examples, the histogram will have problems of reperesenting. But if there are enough examples, the histogram will look smooth and nice:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-09-08-tensorboard/thousand_items.png&quot; alt=&quot;Thousand Items&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-flush-if-using-ipython-jupyter-notebook&quot;&gt;3 Flush if using Ipython (Jupyter) Notebook.&lt;/h3&gt;

&lt;p&gt;Suppose we wrote some log files in Notebook. Because the program was not ended, the files might have not been writen. I have found only part of information when I run some program in Notebook because of this. So every time, please call tf.summary.FileWriter.flush or close to make sure the information is fully outputed.&lt;/p&gt;

&lt;h3 id=&quot;4-in-histogram-z-axis-denotes-iterations&quot;&gt;4 In histogram, z-axis denotes iterations&lt;/h3&gt;

&lt;p&gt;Through z-axis, we can observe the change over time. I wrote a small piece of code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# version: r1.3
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stddev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;w1_hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;from_uniform_distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w2_hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;from_normal_distribution&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;summary_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/tmp/tensorboard/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see those two distributions merged together during training:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2017-09-08-tensorboard/merged_distributions.png&quot; alt=&quot;Merged Distributions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In fact, after the last iteration, the data W1 and W2 are almost same, but why in histogram, those two seems different? Because the scale of y-axis is different. We can see from_normal_distribution, the initial max value is higher than the from_uniform_distribution graphic.&lt;/p&gt;</content><author><name>Sida Liu</name></author><summary type="html">Tensorboard is a very useful tool for visualizing the logs of Tensorflow. It is now an independent project on GitHub, here’s the link.</summary></entry></feed>