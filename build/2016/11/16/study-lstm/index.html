<!DOCTYPE html> <html lang=" en "><head> <meta charset="utf-8"> <title>学习Tensorflow的LSTM的RNN例子</title> <meta http-equip="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="学习Tensorflow的LSTM的RNN例子" /> <meta name="keywords" content="学习Tensorflow的LSTM的RNN例子, Sida Liu, " /> <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"> <meta content="" property="fb:app_id"> <meta content="Sida Liu" property="og:site_name"> <meta content="学习Tensorflow的LSTM的RNN例子" property="og:title"> <meta content="article" property="og:type"> <meta content="The world is so complex that we cannot stop learning." property="og:description"> <meta content="http://172.25.168.122:4000/2016/11/16/study-lstm/" property="og:url"> <meta content="2016-11-16T00:00:00+08:00" property="article:published_time"> <meta content="http://172.25.168.122:4000/about/" property="article:author"> <meta name="twitter:card" content="summary"> <meta name="twitter:site" content="@"> <meta name="twitter:creator" content="@"> <meta name="twitter:title" content="学习Tensorflow的LSTM的RNN例子"> <meta content="Sida Liu" property="og:site_name"> <meta name="twitter:url" content="http://172.25.168.122:4000/2016/11/16/study-lstm/"> <meta name="twitter:description" content="The world is so complex that we cannot stop learning."> <link rel="stylesheet" href="/assets/css/main.css" /> <!-- <link rel="stylesheet" href="/assets/css/custom-style.css" /> --> <link rel="stylesheet" href="/assets/bower_components/lightgallery/dist/css/lightgallery.min.css"/> <!-- <link rel="stylesheet" href="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.css" /> --> <!-- <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css"> --> <!-- <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css"> --> <link rel="stylesheet" href="/assets/bower_components/bootstrap/dist/css/bootstrap.min.css" /> <link rel="stylesheet" href="/assets/bower_components/font-awesome/web-fonts-with-css/css/fontawesome-all.min.css" /> <link rel="stylesheet" href="/assets/bower_components/icono/dist/icono.min.css"/> <!-- Fonts--> <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet"> <!-- Favicon --> <link rel="icon" href="http://172.25.168.122:4000/assets/img/favicon.ico" type="image/gif" sizes="16x16"> <!-- Jquery --> <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha384-vk5WoKIaW/vJyUAd9n/wmopsmNhiy+L2Z+SBxGYnUkunIxVxAv/UtMOhba/xskxh" crossorigin="anonymous"></script> <!-- <script src="/assets/bower_components/jquery/dist/jquery.min.js"></script> --> <script src="/assets/bower_components/jquery.easing/jquery.easing.min.js"></script> <script src="/assets/bower_components/bootstrap/dist/js/bootstrap.bundle.min.js"></script> <script src="/assets/bower_components/jquery-mousewheel/jquery.mousewheel.min.js"></script> <script src="/assets/bower_components/lightgallery/dist/js/lightgallery-all.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/picturefill/3.0.2/picturefill.min.js"></script> <script src="/assets/bower_components/imagesloaded/imagesloaded.pkgd.min.js"></script> <script src="/assets/bower_components/nanobar/nanobar.min.js"></script> <script src="/assets/bower_components/typewrite/dist/typewrite.min.js"></script> <!-- <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script> --> <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script> <!-- Github Button --> <script async defer src="https://buttons.github.io/buttons.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script> </head><body> <nav class="navbar navbar-expand-lg fixed-top navbar-dark" id="topNav"> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation" > <span class="icono-hamburger" id="navbar-hamburger"> </span> </button> <a class="navbar-brand" href="/">Sida Liu</a> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav"> <li class="nav-item"> <a class="nav-link" href="http://172.25.168.122:4000/cover" >Cover</a > </li> <li class="nav-item"> <a class="nav-link" href="http://172.25.168.122:4000/blog" >Blog</a > </li> <li class="nav-item"> <a class="nav-link" href="http://172.25.168.122:4000/contact" >Contact Me</a > </li> </ul> </div> <ul class="nav justify-content-end"> <!-- <li class="nav-item"> <a class="nav-link" id="search-icon" href="http://172.25.168.122:4000/search#/" ><i class="fa fa-search" aria-hidden="true"></i ></a> </li> <li class="nav-item"> <input class="nav-link switch" id="theme-toggle" onclick="modeSwitcher() " type="checkbox" name="checkbox" /> </li> --> </ul> </nav> <div class="col-lg-12"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <div class="row" id="blog-post-container"> <div class="col-lg-8 offset-md-2"><article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-header"> <h1 class="post-title" itemprop="name headline">学习Tensorflow的LSTM的RNN例子</h1> <h4 class="post-meta"></h4> <p class="post-summary"> Posted by : (<time datetime="2016-11-16 00:00:00 +0800" itemprop="datePublished" >Nov 16, 2016</time >) </p> <span class="disqus-comment-count" data-disqus-identifier="/2016/11/16/study-lstm/" ></span> <div class="post-categories"> Category : </div> </div> <div class="card-body" itemprop="articleBody"> <!-- <img class="card-img-top" src="http://172.25.168.122:4000/assets/img/posts/" alt="" /> --> <p>前几天写了<a href="https://liusida.github.io/2016/11/14/study-embeddings/">学习Embeddings的例子</a>，因为琢磨了各个细节，自己也觉得受益匪浅。于是，开始写下一个LSTM的教程吧。</p> <p>还是<a href="https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/63770919610923">Udacity上那个课程</a>。</p> <p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb">源码也在Github上</a>。</p> <p>RNN是一个<strong>非常棒</strong>的技术，可能它已经向我们揭示了“活”的意义。RNN我已经尝试学习了几次，包括前面我<a href="https://liusida.github.io/2016/11/04/rnn-implementation/">这篇笔记</a>，所以就直接进入代码阅读吧。</p> <h2 id="读例子程序">读例子程序：</h2> <h3 id="1-引入库文件">1. 引入库文件</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># These are all the modules we'll be using later. Make sure you can import them
# before proceeding further.
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
</code></pre></div></div> <h3 id="2-下载数据">2. 下载数据</h3> <p>然后下载数据，如果<a href="https://liusida.github.io/2016/11/04/rnn-implementation/">前面</a>已经下载过，那直接把text8.zip拷过来就可以用。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">url</span> <span class="o">=</span> <span class="s">'http://mattmahoney.net/dc/'</span>

<span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="s">"""Download a file if not present, and make sure it's the right size."""</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="p">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Found and verified %s'</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="p">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span>
      <span class="s">'Failed to verify '</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'. Can you get to it with a browser?'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">filename</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">'text8.zip'</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>
</code></pre></div></div> <h3 id="3-读入文本">3. 读入文本</h3> <p>读文件稍微有些不一样，不是处理成list，而是直接读成一个字符串，因为后面用到的就是串数据。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">zipfile</span><span class="p">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="n">namelist</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
  <span class="n">f</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Data size %d'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</code></pre></div></div> <h3 id="4-生成训练数据集函数">4. 生成训练数据集函数</h3> <p>切割一下，留1000个字符做检验，其他99999000个字符拿来训练。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">valid_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">valid_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[:</span><span class="n">valid_size</span><span class="p">]</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">valid_size</span><span class="p">:]</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">train_text</span><span class="p">[:</span><span class="mi">64</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">valid_size</span><span class="p">,</span> <span class="n">valid_text</span><span class="p">[:</span><span class="mi">64</span><span class="p">])</span>
</code></pre></div></div> <h3 id="5-两个工具函数">5. 两个工具函数</h3> <p>建立两个函数<code class="highlighter-rouge">char2id</code>和<code class="highlighter-rouge">id2char</code>，用来把字符对应成数字。</p> <p>本程序只考虑26个字母外加1个空格字符，其他字符都当做空格来对待。所以可以用两个函数，通过ascii码加减，直接算出对应的数值或字符。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">string</span><span class="p">.</span><span class="n">ascii_lowercase</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># [a-z] + ' '
</span><span class="n">first_letter</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">string</span><span class="p">.</span><span class="n">ascii_lowercase</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">char2id</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">string</span><span class="p">.</span><span class="n">ascii_lowercase</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="o">-</span> <span class="n">first_letter</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="s">' '</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">0</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Unexpected character: %s'</span> <span class="o">%</span> <span class="n">char</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">id2char</span><span class="p">(</span><span class="n">dictid</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">dictid</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">dictid</span> <span class="o">+</span> <span class="n">first_letter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="s">' '</span>

<span class="k">print</span><span class="p">(</span><span class="n">char2id</span><span class="p">(</span><span class="s">'a'</span><span class="p">),</span> <span class="n">char2id</span><span class="p">(</span><span class="s">'z'</span><span class="p">),</span> <span class="n">char2id</span><span class="p">(</span><span class="s">' '</span><span class="p">),</span> <span class="n">char2id</span><span class="p">(</span><span class="s">'ï'</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">id2char</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">id2char</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">id2char</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div> <h3 id="6-生成训练数据集函数">6. 生成训练数据集函数</h3> <p>这次 <code class="highlighter-rouge">BatchGenerator</code> 做的比前两天的那个要认真了，用了成员变量来记录位置，而不是用全局变量。</p> <p>用 <code class="highlighter-rouge">BatchGenerator.next()</code> 方法，可以获取一批子字符串用于训练。</p> <p><code class="highlighter-rouge">batch_size</code> 是每批几串字符串，<code class="highlighter-rouge">num_unrollings</code> 是每串子字符串的长度（实际上字符串开头还加了上一次获取的最后一个字符，所以实际上字符串长度要比 <code class="highlighter-rouge">num_unrollings</code> 多一个）。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span>
<span class="n">num_unrollings</span><span class="o">=</span><span class="mi">10</span>

<span class="k">class</span> <span class="nc">BatchGenerator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_unrollings</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_text</span> <span class="o">=</span> <span class="n">text</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_text_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_num_unrollings</span> <span class="o">=</span> <span class="n">num_unrollings</span>
    <span class="n">segment</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_text_size</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_cursor</span> <span class="o">=</span> <span class="p">[</span> <span class="n">offset</span> <span class="o">*</span> <span class="n">segment</span> <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_last_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_next_batch</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">"""Generate a single batch from the current cursor position in the data."""</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_batch_size</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_batch_size</span><span class="p">):</span>
      <span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">char2id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_text</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_cursor</span><span class="p">[</span><span class="n">b</span><span class="p">]])]</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">_cursor</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_cursor</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">_text_size</span>
    <span class="k">return</span> <span class="n">batch</span>

  <span class="k">def</span> <span class="nf">next</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">"""Generate the next array of batches from the data. The array consists of
    the last batch of the previous array, followed by num_unrollings new ones.
    """</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_last_batch</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_num_unrollings</span><span class="p">):</span>
      <span class="n">batches</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_next_batch</span><span class="p">())</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_last_batch</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">batches</span>
</code></pre></div></div> <p>真不愧是优秀程序员写的代码，这个函数写的又让我学习了！</p> <p>它在初始化的时候先根据 <code class="highlighter-rouge">batch_size</code> 把段分好，然后设立一组游标 <code class="highlighter-rouge">_cursor</code> ，是一组哦，不是一个哦！然后定义好 <code class="highlighter-rouge">_last_batch</code> 看或许到哪了。</p> <p>然后获取需要的字符串的时候，是一批一批的获取各个字符。</p> <p>这样做，就可以针对整段字符串均匀的取样，从而避免某些地方学的太细，某些地方又没有学到。</p> <p>值得注意的是，在RNN准备数据的时候，所喂数据的结构是很容易搞错的。在前面博客中，也有很多同学对于他使用 <code class="highlighter-rouge">transpose</code> 的意义没法理解。这里需要详细记录一下。</p> <p><code class="highlighter-rouge">BatchGenerator.next()</code> 返回的数据格式，是一个list，list的长度是 <code class="highlighter-rouge">num_unrollings+1</code>，每一个元素，都是一个(<code class="highlighter-rouge">batch_size</code>,27)的array，27是 <code class="highlighter-rouge">vocabulary_size</code>，一个27维向量代表一个字符，是one-hot encoding的格式。</p> <p>所以，<strong>喂这一批数据进神经网络的时候，理论上是先进去一批的首字符，然后再进去同一批的第二个字符，然后再进去同一批的第三个字符…</strong></p> <p>也就是说，下图才是真正的RNN的结构，我们要做的，是按照顺序一个一个的按顺序把东西喂进去。这个图，我看到名字叫 <code class="highlighter-rouge">RNN-rolled</code>：</p> <p><img src="/images/2016-11-16-study-lstm/RNN-rolled.png" alt="RNN-rolled" /></p> <p>我们平时看到的向右一路展开的RNN其实向右方向（我用了虚线）是代表先后顺序（同时也带记忆数据流），跟上下方向意义是不一样的。有没有同学误解那么一排东西是可以同时喂进去的？这个图，我看到名字叫 <code class="highlighter-rouge">RNN-unrolled</code>。</p> <p><img src="/images/2016-11-16-study-lstm/RNN-unrolled.png" alt="RNN-unrolled" /></p> <h3 id="7-另外两个工具函数">7. 另外两个工具函数</h3> <p>再定义两个用来把训练数据转换成可展现字符串的函数。</p> <p><code class="highlighter-rouge">characters</code> 先从one-hot encoding变回数字，再用id2char变成字符。</p> <p><code class="highlighter-rouge">batches2string</code> 则将训练数据变成可以展现的字符串。高手这么一批一批的处理数据逻辑还这么绕，而不是按凡人逻辑一个一个的处理让我觉得有点窒息的感觉，自感智商捉急了。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">characters</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
  <span class="s">"""Turn a 1-hot encoding or a probability distribution over the possible
  characters back into its (most likely) character representation."""</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">id2char</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">batches2string</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
  <span class="s">"""Convert a sequence of batches back into their (most likely) string
  representation."""</span>
  <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="s">''</span><span class="p">]</span> <span class="o">*</span> <span class="n">batches</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">characters</span><span class="p">(</span><span class="n">b</span><span class="p">))]</span>
  <span class="k">return</span> <span class="n">s</span>

<span class="n">train_batches</span> <span class="o">=</span> <span class="n">BatchGenerator</span><span class="p">(</span><span class="n">train_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_unrollings</span><span class="p">)</span>
<span class="n">valid_batches</span> <span class="o">=</span> <span class="n">BatchGenerator</span><span class="p">(</span><span class="n">valid_text</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">train_batches</span><span class="p">.</span><span class="nb">next</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">train_batches</span><span class="p">.</span><span class="nb">next</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">valid_batches</span><span class="p">.</span><span class="nb">next</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="n">valid_batches</span><span class="p">.</span><span class="nb">next</span><span class="p">()))</span>
</code></pre></div></div> <h3 id="8-另外四个工具函数">8. 另外四个工具函数</h3> <p>四个函数，给训练中输出摘要时使用。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
  <span class="s">"""Log-probability of the true labels in a predicted batch."""</span>
  <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-10</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)))</span> <span class="o">/</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sample_distribution</span><span class="p">(</span><span class="n">distribution</span><span class="p">):</span>
  <span class="s">"""Sample one element from a distribution assumed to be an array of normalized
  probabilities.
  """</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">distribution</span><span class="p">)):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">distribution</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">i</span>
  <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">distribution</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">prediction</span><span class="p">):</span>
  <span class="s">"""Turn a (column) prediction into 1-hot encoded samples."""</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
  <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_distribution</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="k">return</span> <span class="n">p</span>

<span class="k">def</span> <span class="nf">random_distribution</span><span class="p">():</span>
  <span class="s">"""Generate a random column of probabilities."""</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">b</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[:,</span><span class="bp">None</span><span class="p">]</span>
</code></pre></div></div> <p><code class="highlighter-rouge">logprob</code>： 用来测量预测工作完成的如何。</p> <p>先回忆一下 <code class="highlighter-rouge">cross_entropy</code>：</p> \[Cross Entropy = - \sum_{i}^N({predictions \cdot \log(labels)})\] <p>那么，</p> \[logprob = { Cross Entropy \over N }\] <p>后面三个函数 <code class="highlighter-rouge">sample_distribution</code> <code class="highlighter-rouge">sample</code> <code class="highlighter-rouge">random_distribution</code> 是一起使用的。</p> <p><code class="highlighter-rouge">random_distribution</code> 就是生成一个平均分布的，加总和为 1 的 array。但是我不知道为何写的这么花哨，我试了半天，似乎 <code class="highlighter-rouge">b/np.sum(b, 1)[:,None]</code> 和 <code class="highlighter-rouge">b/np.sum(b)</code> 的意思是一样的。</p> <p><code class="highlighter-rouge">sample</code> 则是靠 <code class="highlighter-rouge">sample_distribution</code> 以传入的 <code class="highlighter-rouge">prediction</code> 的概率，随机取一个维设成 1 ，其他都设成 0 ，也就是按照 <code class="highlighter-rouge">prediction</code> 的概率获得一个随机字母。（为啥不直接取概率最大的那个字母呢？搞这么复杂真的好吗？）</p> <h3 id="9-定义tensorflow模型">9. 定义Tensorflow模型</h3> <p>分为几个部分：定义变量，定义LSTM Cell，定义输入接口，循环执行LSTM Cell，定义loss，定义优化，定义预测。</p> <p>num_nodes 是代表这个神经网络中LSTM Cell层的Cell个数。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_nodes</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">graph</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>
</code></pre></div></div> <h4 id="1-定义变量">1) 定义变量</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  <span class="c1"># Parameters:
</span>  <span class="c1"># Input gate: input, previous output, and bias.
</span>  <span class="n">ix</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">im</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">ib</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c1"># Forget gate: input, previous output, and bias.
</span>  <span class="n">fx</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">fm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">fb</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c1"># Memory cell: input, state and bias.                             
</span>  <span class="n">cx</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">cm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">cb</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c1"># Output gate: input, previous output, and bias.
</span>  <span class="n">ox</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">om</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">ob</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="c1"># Variables saving state across unrollings.
</span>  <span class="n">saved_output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]),</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">saved_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]),</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="c1"># Classifier weights and biases.
</span>  <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>
</code></pre></div></div> <p>LSTM Cell 首先有三个门，input output forget三门。</p> <p>Memory cell 暂时不知道是个什么。</p> <p>saved_output 是向上的产出，saved_state 是自己的状态记忆。</p> <p>w 和 b 是最后用来做一个 full connection 的标准神经网络层，把结果变为 vocabulary_size 个之一。</p> <h4 id="2-定义lstm-cell">2) 定义LSTM Cell</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Definition of the cell computation.
</span>  <span class="k">def</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="s">"""Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
    Note that in this formulation, we omit the various connections between the
    previous state and the gates."""</span>
    <span class="n">input_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span> <span class="o">+</span> <span class="n">ib</span><span class="p">)</span>
    <span class="n">forget_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">fm</span><span class="p">)</span> <span class="o">+</span> <span class="n">fb</span><span class="p">)</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span> <span class="o">+</span> <span class="n">cb</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">forget_gate</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="n">input_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
    <span class="n">output_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ox</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">om</span><span class="p">)</span> <span class="o">+</span> <span class="n">ob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="n">state</span>
</code></pre></div></div> <p>这里定义的 LSTM Cell 似乎并不是我们平时熟悉的那种，而是如下图（http://arxiv.org/pdf/1402.1128v1.pdf）：</p> <p><img src="/images/2016-11-16-study-lstm/lstm-for-chars.png" alt="lstm for chars" /></p> <p>初看这个图可能不是很能理解，于是我重新画了一下：</p> <p><img src="/images/2016-11-16-study-lstm/lstm-model.png" alt="lstm model" /></p> <p>我手画的图例解释：</p> <p>(1) \(\otimes\)代表两个数据源乘上参数后相加。\(\oplus\)代表两个数据源相加。</p> <p>(2) \(\otimes\)外面再加花边的，代表两个数据源相乘后再取 <code class="highlighter-rouge">sigmoid</code> 。</p> <p>(3) 圆圈里是 \(g\) 的，代表取 <code class="highlighter-rouge">tanh</code> 。</p> <p>(4) \(State_{-1}\) 下标-1代表这是上一次迭代时的结果。</p> <blockquote> <p>回想一下，<code class="highlighter-rouge">sigmoid</code> 函数产生一个(0,1)的数，<code class="highlighter-rouge">tanh</code> 函数产生一个(-1,1)的数。</p> </blockquote> <p>作为对比，我再引用一个我认为画的最完美的标准 LSTM Cell 图，来自 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah 的博客</a>：</p> <p><img src="/images/2016-11-16-study-lstm/LSTM3-chain.png" alt="LSTM3-chain" /></p> <p>Colah 图例解释：</p> <p>(1) 方形中带 \(\sigma\) ，代表两个数据源连接在一起后乘参数，再取 <code class="highlighter-rouge">sigmoid</code> 。（<strong>嗯，这里有不同</strong>：Colah 博客中标准的 LSTM Cell 中，这里的操作是先接在一起，再乘参数，而我们这里是先各自乘参数，再相加。）</p> <p>(2) 方形中带 \(tanh\) ，代表两个数据源连接在一起后乘参数，再取 <code class="highlighter-rouge">tanh</code> 。（<strong>这里也是</strong>）</p> <p>(3) 椭圆形中带 \(tanh\)， 代表直接取 <code class="highlighter-rouge">tanh</code> 。</p> <p>(4) \(\otimes\)代表两个数据源相乘。\(\oplus\)代表两个数据源相加。</p> <p>(5) 两条从过去\(-1\)到当前 Cell 再到未来\(+1\)的横向黑色线条箭头，上方代表 <code class="highlighter-rouge">state</code>，下方代表 <code class="highlighter-rouge">output</code>。</p> <p>所以像论文里指出的，这里实现的 LSTM Cell 含有更多参数，效果更好？这种比较目前超出我的认知范围，以后再细看。</p> <h4 id="3-定义输入接口">3) 定义输入接口</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Input data.
</span>  <span class="n">train_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unrollings</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span><span class="n">vocabulary_size</span><span class="p">]))</span>
  <span class="n">train_inputs</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[:</span><span class="n">num_unrollings</span><span class="p">]</span>
  <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># labels are inputs shifted by one time step.
</span></code></pre></div></div> <p>这里也是一个 batch 同时处理的。但为了容易理解，我先假设 <code class="highlighter-rouge">batch_size=1</code> ，然后假设我们要训练一个字符串 abcdefg。</p> <p>那么 <code class="highlighter-rouge">train_inputs</code> 是 abcdef，<code class="highlighter-rouge">train_labels</code> 是 bcdefg 。</p> <h4 id="4-循环执行lstm-cell">4) 循环执行LSTM Cell</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Unrolled LSTM loop.
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">saved_output</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">saved_state</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">train_inputs</span><span class="p">:</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="n">outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div> <p>根据前面定义变量的时候规定，初始 <code class="highlighter-rouge">saved_output</code> 和 <code class="highlighter-rouge">saved_state</code> 都是全零。</p> <p>依次输入 a b c d e f ，把每一次的输出放在一起形成一个 list 就是 <code class="highlighter-rouge">outputs</code>。</p> <h4 id="5-定义loss">5) 定义loss</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># State saving across unrollings.
</span>  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">saved_output</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
                                <span class="n">saved_state</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">state</span><span class="p">)]):</span>
    <span class="c1"># Classifier.
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">outputs</span><span class="p">),</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)))</span>
</code></pre></div></div> <p>因为不是顺序执行语言，一般模型如果不是相关的语句，其执行是没有先后顺序的，<code class="highlighter-rouge">control_dependencies</code> 的作用就是建立先后顺序，保证前面两句被执行后，才执行后面的内容。</p> <p>这里也就是先把 <code class="highlighter-rouge">saved_output</code> 和 <code class="highlighter-rouge">saved_state</code> 保存之后，再计算 <code class="highlighter-rouge">logits</code> 和 <code class="highlighter-rouge">loss</code>。否则因为下面计算时没有关联到 <code class="highlighter-rouge">saved_output</code> 和 <code class="highlighter-rouge">saved_state</code>，如果不用 <code class="highlighter-rouge">control_dependencies</code> 那上面两句保存就不会被优化语句触发。</p> <p><a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#concat"><code class="highlighter-rouge">tf.concat(0, values)</code></a> 是指在 0 维上把 values 连接起来。本来 outputs 是一个 list，每一个元素都是一个27维向量表示一个字母（还是假设 <code class="highlighter-rouge">batch_size=1</code>）。</p> <p>通过 <code class="highlighter-rouge">tf.concat</code> 把结果连接起来，成为一个向量，可以拿来乘以 w 加上 b 这样进入一个 full connection，从而得到 logits 。</p> <p>然后再通过 <code class="highlighter-rouge">softmax_cross_entropy_with_logits</code> 比较连接并 full connection 的 <code class="highlighter-rouge">outputs</code> 和 连接起来的 <code class="highlighter-rouge">train_labels</code> ，得到 <code class="highlighter-rouge">loss</code> 。</p> <h4 id="6-定义优化">6) 定义优化</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Optimizer.
</span>  <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">exponential_decay</span><span class="p">(</span>
    <span class="mf">10.0</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">staircase</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
  <span class="n">gradients</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">optimizer</span><span class="p">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
  <span class="n">gradients</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
</code></pre></div></div> <p><a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#exponential_decay"><code class="highlighter-rouge">tf.train.exponential_decay</code></a> 可以用来实现 <code class="highlighter-rouge">learning_rate</code> 的指数型衰减，越到后面 <code class="highlighter-rouge">learning_rate</code> 越小。（依赖后面修改 <code class="highlighter-rouge">global_step</code> 值来实现）</p> <p><code class="highlighter-rouge">optimizer</code> 定义成使用标准 Gradient Descent 。每一种 <code class="highlighter-rouge">optimizer</code> 都有几个标准接口，我们前面常用的是 <code class="highlighter-rouge">minimize</code> 接口，他自动的调整整个 <code class="highlighter-rouge">Graph</code> 中可调节的 <code class="highlighter-rouge">Variables</code> 尝试最小化 <code class="highlighter-rouge">loss</code>。其实 <code class="highlighter-rouge">minimize</code> 函数就是这两步并起来： <code class="highlighter-rouge">compute_gradients</code> 和 <code class="highlighter-rouge">apply_gradients</code>。先计算梯度值，然后再把那些参数减去梯度值。这里把两步分开了，为了在 apply 之前先处理一下梯度值，Tensorflow 给了详细解释，我们来看看[手册][manual-compute-gradients]。</p> <p><code class="highlighter-rouge">compute_gradients</code> 函数返回一个list，里面是一对一对的 <code class="highlighter-rouge">gradient</code> 和 <code class="highlighter-rouge">variable</code>，说明针对某个可调整的变量，他的梯度是多少。</p> <p><code class="highlighter-rouge">clip_by_global_norm</code> 避免梯度值过大产生 Exploding Gradients 梯度爆炸问题，视频里有这么一个图：</p> <p><img src="/images/2016-11-16-study-lstm/clip_gradient.png" alt="clip gradients" /></p> <p><code class="highlighter-rouge">clip_by_global_norm</code> 的具体计算是，先计算 <code class="highlighter-rouge">global_norm</code> ，也就是整个 <code class="highlighter-rouge">tensor</code> 的模（二范数）。看这个模是否大于文中的<code class="highlighter-rouge">1.25</code>，如果大于，则结果等于 <code class="highlighter-rouge">gradients * 1.25 / global_norm</code>，如果不大于，就不变。</p> <p>最后，<code class="highlighter-rouge">apply_gradients</code>。这里传入的 <code class="highlighter-rouge">global_step</code> 是会被修改的，每次加一，这样下次计算 <code class="highlighter-rouge">learning_rate</code> 的时候就会使用新的 <code class="highlighter-rouge">global_step</code> 值。</p> <h4 id="7-定义预测">7) 定义预测</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Predictions.
</span>  <span class="n">train_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

  <span class="c1"># Sampling and validation eval: batch 1, no unrolling.
</span>  <span class="n">sample_input</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">])</span>
  <span class="n">saved_sample_output</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="n">saved_sample_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">]))</span>
  <span class="n">reset_sample_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">group</span><span class="p">(</span>
    <span class="n">saved_sample_output</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">])),</span>
    <span class="n">saved_sample_state</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">])))</span>
  <span class="n">sample_output</span><span class="p">,</span> <span class="n">sample_state</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="p">(</span>
    <span class="n">sample_input</span><span class="p">,</span> <span class="n">saved_sample_output</span><span class="p">,</span> <span class="n">saved_sample_state</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">saved_sample_output</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">sample_output</span><span class="p">),</span>
                                <span class="n">saved_sample_state</span><span class="p">.</span><span class="n">assign</span><span class="p">(</span><span class="n">sample_state</span><span class="p">)]):</span>
    <span class="n">sample_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">sample_output</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</code></pre></div></div> <p><code class="highlighter-rouge">sample_input</code> 是一个1-hot编码过的字符。</p> <p>建立初始 state 和 output，经过同样的 LSTM Cell，得到下一个预测的字符 <code class="highlighter-rouge">sample_prediction</code>。</p> <h3 id="10-开始训练">10. 开始训练</h3> <h4 id="1-训练">1) 训练</h4> <p>注意到这里喂进去的字符串长度正好是 <code class="highlighter-rouge">num_unrollings + 1</code>，恰好对应前面 <code class="highlighter-rouge">BatchGenerator.next()</code> 获取的时候得到的字符串长度，也恰好对应了模型定义里 <code class="highlighter-rouge">train_inputs</code> 和 <code class="highlighter-rouge">train_labels</code> 错开1个字符。</p> <p><code class="highlighter-rouge">mean_loss</code> 用来加总各步的 <code class="highlighter-rouge">loss</code> 值，用来后面输出。（还是建议叫 <code class="highlighter-rouge">subtotal_loss</code>）</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">7001</span>
<span class="n">summary_frequency</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">initialize_all_variables</span><span class="p">().</span><span class="n">run</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Initialized'</span><span class="p">)</span>
  <span class="n">mean_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">train_batches</span><span class="p">.</span><span class="nb">next</span><span class="p">()</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unrollings</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">feed_dict</span><span class="p">[</span><span class="n">train_data</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">batches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span>
      <span class="p">[</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">train_prediction</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
    <span class="n">mean_loss</span> <span class="o">+=</span> <span class="n">l</span>
</code></pre></div></div> <h4 id="2-定期输出摘要">2) 定期输出摘要</h4> <p>他怎么不用 tensorflow 来计算呀，反而用 numpy 来计算，很奇怪。来仔细看看。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">summary_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">mean_loss</span> <span class="o">/</span> <span class="n">summary_frequency</span>
      <span class="c1"># The mean loss is an estimate of the loss over the last few batches.
</span>      <span class="k">print</span><span class="p">(</span>
        <span class="s">'Average loss at step %d: %f learning rate: %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">lr</span><span class="p">))</span>
      <span class="n">mean_loss</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">batches</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Minibatch perplexity: %.2f'</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">))))</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="p">(</span><span class="n">summary_frequency</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Generate some samples.
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'='</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
          <span class="n">feed</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">random_distribution</span><span class="p">())</span>
          <span class="n">sentence</span> <span class="o">=</span> <span class="n">characters</span><span class="p">(</span><span class="n">feed</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
          <span class="n">reset_sample_state</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>
          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">79</span><span class="p">):</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="n">sample_prediction</span><span class="p">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">sample_input</span><span class="p">:</span> <span class="n">feed</span><span class="p">})</span>
            <span class="n">feed</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
            <span class="n">sentence</span> <span class="o">+=</span> <span class="n">characters</span><span class="p">(</span><span class="n">feed</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
          <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'='</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
      <span class="c1"># Measure validation set perplexity.
</span>      <span class="n">reset_sample_state</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>
      <span class="n">valid_logprob</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid_size</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">valid_batches</span><span class="p">.</span><span class="nb">next</span><span class="p">()</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">sample_prediction</span><span class="p">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">sample_input</span><span class="p">:</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]})</span>
        <span class="n">valid_logprob</span> <span class="o">=</span> <span class="n">valid_logprob</span> <span class="o">+</span> <span class="n">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'Validation set perplexity: %.2f'</span> <span class="o">%</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">valid_logprob</span> <span class="o">/</span> <span class="n">valid_size</span><span class="p">)))</span>
</code></pre></div></div> <p>每当 <code class="highlighter-rouge">summary_frequency</code> 整数倍步的时候，输出平均 <code class="highlighter-rouge">loss</code> 值和 <code class="highlighter-rouge">learning_rate</code> ，看看是否有 clip 掉，如果没有 clip 掉，那么都是 10.0 。然后再计算这一部分 train set perplexity。</p> <p>每当 <code class="highlighter-rouge">summary_frequency * 10</code> 整数倍步的时候，尝试输出一些文字结果。</p> <p>这里尝试得到 5 句，每局 80 个字符的文字结果。</p> <p>首先以平均分布随机得到一个字符，并作为 <code class="highlighter-rouge">sentence</code> 的第一个字符。</p> <p>然后 <code class="highlighter-rouge">reset_sample_state</code> 一下，保证初始化的 <code class="highlighter-rouge">state</code> 和 <code class="highlighter-rouge">output</code> 都设成 0 。</p> <p>然后传入第一个字符作为输入，得到第一个预测字符的预测概率 <code class="highlighter-rouge">prediction</code>，通过 <code class="highlighter-rouge">sample</code> 将其蜕化成一个确定的字符 <code class="highlighter-rouge">feed</code>，然后接到 <code class="highlighter-rouge">sentence</code> 上，并下一次传给模型作为输入。</p> <p>这样就得到了一句80字符的句子。重复这个过程 5 次，得到 5 句。</p> <p>继而，又是每当 <code class="highlighter-rouge">summary_frequency</code> 整数倍步的时候，（写的不好啊，明明应当把相近的写在一起。）用 valid_text 来计算平均的 validation set perplexity。</p> <p>根据信息论，perplexity <a href="https://en.wikipedia.org/wiki/Perplexity">wikipedia定义</a> 和 cross_entropy 的关系如下：</p> \[perplexity = e^{cross\_entropy}\] <h2 id="结束">结束</h2> <p>谢谢阅读，敬请留言。</p> Any feedback? Please at me on <a class="social-link" href="http://twitter.com/liusida2007" > <i class="fab fa-twitter"></i> </a> </div><!-- Incomment incase you want to use Disqus <div id="disqus_thread"></div> --> </article> <script> var disqus_config = function () { this.page.url = "http://172.25.168.122:4000/2016/11/16/study-lstm/"; /* Replace PAGE_URL with your page's canonical URL variable */ this.page.identifier = "/2016/11/16/study-lstm"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */ }; (function () { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript></div> </div> <div class="row"> <div class="col-md-4"> <div class="card"> <div class="card-header">About Sida Liu</div> <div class="card-body"> <p class="author_bio">I am currently a M.S. graduate student in Morphology, Evolution & Cognition Laboratory at University of Vermont. I am interested in artificial intelligence, artificial life, and artificial environment.</p><!-- Place this tag where you want the button to render. --> <a class="github-button" href="https://github.com/liusida" data-size="large" data-show-count="true" aria-label="Follow @liusida on GitHub">Follow @liusida</a></div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Categories</div> <div class="card-body text-dark"> <div id="#Old"></div> <li class="tag-head"> <a href="/blog/categories/Old">Old</a> </li> <a name="Old"></a> <div id="#CUDA"></div> <li class="tag-head"> <a href="/blog/categories/CUDA">CUDA</a> </li> <a name="CUDA"></a> <div id="#Simulation"></div> <li class="tag-head"> <a href="/blog/categories/Simulation">Simulation</a> </li> <a name="Simulation"></a> <div id="#GPU"></div> <li class="tag-head"> <a href="/blog/categories/GPU">GPU</a> </li> <a name="GPU"></a> <div id="#Reading"></div> <li class="tag-head"> <a href="/blog/categories/Reading">Reading</a> </li> <a name="Reading"></a> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Useful Links</div> <div class="card-body text-dark"> <li> <a href="http://172.25.168.122:4000/cover">Cover</a> </li> <li> <a href="http://172.25.168.122:4000/blog">Blog</a> </li> <li> <a href="http://172.25.168.122:4000/contact">Contact Me</a> </li> </div> </div> </div> </div> </div><footer> <p> Powered by <strong>devlopr jekyll</strong>. Subscribe via <a href=" /feed.xml ">RSS</a> </p> </footer> <script> var options = { classname: 'my-class', id: 'my-id' }; var nanobar = new Nanobar( options ); nanobar.go( 30 ); nanobar.go( 76 ); nanobar.go(100); </script> <!-- <div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div> --> <!-- <script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script> --> <!-- <script src="/assets/js/mode-switcher.js"></script> --> </body> </html>
