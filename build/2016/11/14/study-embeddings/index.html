<!DOCTYPE html> <html lang=" en "><head> <meta charset="utf-8"> <title>学习Tensorflow的Embeddings例子</title> <meta http-equip="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="学习Tensorflow的Embeddings例子" /> <meta name="keywords" content="学习Tensorflow的Embeddings例子, Sida Liu, " /> <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"> <meta content="" property="fb:app_id"> <meta content="Sida Liu" property="og:site_name"> <meta content="学习Tensorflow的Embeddings例子" property="og:title"> <meta content="article" property="og:type"> <meta content="The world is so complex that we cannot stop learning." property="og:description"> <meta content="http://172.25.168.122:4000/2016/11/14/study-embeddings/" property="og:url"> <meta content="2016-11-14T00:00:00+08:00" property="article:published_time"> <meta content="http://172.25.168.122:4000/about/" property="article:author"> <meta name="twitter:card" content="summary"> <meta name="twitter:site" content="@"> <meta name="twitter:creator" content="@"> <meta name="twitter:title" content="学习Tensorflow的Embeddings例子"> <meta content="Sida Liu" property="og:site_name"> <meta name="twitter:url" content="http://172.25.168.122:4000/2016/11/14/study-embeddings/"> <meta name="twitter:description" content="The world is so complex that we cannot stop learning."> <link rel="stylesheet" href="/assets/css/main.css" /> <!-- <link rel="stylesheet" href="/assets/css/custom-style.css" /> --> <link rel="stylesheet" href="/assets/bower_components/lightgallery/dist/css/lightgallery.min.css"/> <!-- <link rel="stylesheet" href="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.css" /> --> <!-- <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css"> --> <!-- <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css"> --> <link rel="stylesheet" href="/assets/bower_components/bootstrap/dist/css/bootstrap.min.css" /> <link rel="stylesheet" href="/assets/bower_components/font-awesome/web-fonts-with-css/css/fontawesome-all.min.css" /> <link rel="stylesheet" href="/assets/bower_components/icono/dist/icono.min.css"/> <!-- Fonts--> <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet"> <!-- Favicon --> <link rel="icon" href="http://172.25.168.122:4000/assets/img/favicon.ico" type="image/gif" sizes="16x16"> <!-- Jquery --> <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha384-vk5WoKIaW/vJyUAd9n/wmopsmNhiy+L2Z+SBxGYnUkunIxVxAv/UtMOhba/xskxh" crossorigin="anonymous"></script> <!-- <script src="/assets/bower_components/jquery/dist/jquery.min.js"></script> --> <script src="/assets/bower_components/jquery.easing/jquery.easing.min.js"></script> <script src="/assets/bower_components/bootstrap/dist/js/bootstrap.bundle.min.js"></script> <script src="/assets/bower_components/jquery-mousewheel/jquery.mousewheel.min.js"></script> <script src="/assets/bower_components/lightgallery/dist/js/lightgallery-all.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/picturefill/3.0.2/picturefill.min.js"></script> <script src="/assets/bower_components/imagesloaded/imagesloaded.pkgd.min.js"></script> <script src="/assets/bower_components/nanobar/nanobar.min.js"></script> <script src="/assets/bower_components/typewrite/dist/typewrite.min.js"></script> <!-- <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script> --> <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script> <!-- Github Button --> <script async defer src="https://buttons.github.io/buttons.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script> </head><body> <nav class="navbar navbar-expand-lg fixed-top navbar-dark" id="topNav"> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation" > <span class="icono-hamburger" id="navbar-hamburger"> </span> </button> <a class="navbar-brand" href="/">Sida Liu</a> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav"> <li class="nav-item"> <a class="nav-link" href="http://172.25.168.122:4000/cover" >Cover</a > </li> <li class="nav-item"> <a class="nav-link" href="http://172.25.168.122:4000/blog" >Blog</a > </li> <li class="nav-item"> <a class="nav-link" href="http://172.25.168.122:4000/contact" >Contact Me</a > </li> </ul> </div> <ul class="nav justify-content-end"> <!-- <li class="nav-item"> <a class="nav-link" id="search-icon" href="http://172.25.168.122:4000/search#/" ><i class="fa fa-search" aria-hidden="true"></i ></a> </li> <li class="nav-item"> <input class="nav-link switch" id="theme-toggle" onclick="modeSwitcher() " type="checkbox" name="checkbox" /> </li> --> </ul> </nav> <div class="col-lg-12"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <div class="row" id="blog-post-container"> <div class="col-lg-8 offset-md-2"><article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-header"> <h1 class="post-title" itemprop="name headline">学习Tensorflow的Embeddings例子</h1> <h4 class="post-meta"></h4> <p class="post-summary"> Posted by : (<time datetime="2016-11-14 00:00:00 +0800" itemprop="datePublished" >Nov 14, 2016</time >) </p> <span class="disqus-comment-count" data-disqus-identifier="/2016/11/14/study-embeddings/" ></span> <div class="post-categories"> Category : </div> </div> <div class="card-body" itemprop="articleBody"> <!-- <img class="card-img-top" src="http://172.25.168.122:4000/assets/img/posts/" alt="" /> --> <p>Udacity 上有一个 Google 技术人员提供的基于 Tensorflow 的深度学习课程，今天学到 Embeddings ，有点难理解，所以写个笔记记录下，以备日后回忆。</p> <p>链接：</p> <p><a href="https://classroom.udacity.com/courses/ud730/lessons/6378983156/concepts/63742734590923">Udacity课程视频</a> 这个课程在 Udacity 上的难度级别已经是 <em>高</em> 了。估计再下去就更少视频学习内容了。:~(</p> <p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb">例子Github地址</a></p> <h2 id="理解课程">理解课程：</h2> <ul> <li>课程使用的实现无监督文本学习的根据：<strong>相似的词，会伴随相似的上下文</strong>。 （我记得有人说过，看一个人的朋友，就知道这个人大致是怎样，看来词也一样。）如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/catandkitty.png" alt="word cat and kitty" /></p> <ul> <li>Embeddings的目标，就是把词都放到一个向量空间里去，这样相近的词就聚集在一起。有了这个模型以后，就可以做很多应用，例如找近义词、某一类词聚类、甚至进行向量加减来寻找衍生词。如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/embeddings.png" alt="embeddings" /></p> <ul> <li>如何建立这个Embeddings呢？首先使用一个工具叫word2vec。word2vec算法描述是这样，载入句子，从句子中取出一个词，例如FOX，将他放入Embeddings向量空间（最初位置肯定是随机的），然后通过一次逻辑回归分类预测出他对应的词语，然后与文中实际的QUICK BROWN JUMPS OVER四个词比对，并修正他在Embeddings向量空间里的位置。反复上述步骤，便可得到Embeddings向量空间。（TODO:我这段还需要再理解下）如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/word2vec.png" alt="word2vec" /></p> <ul> <li>再来看一下word2vec的具体流程图：把词<code class="highlighter-rouge">cat</code>放进Embeddings向量空间，然后做一次线性计算，然后取softmax，得到一个一批0-1的数值，然后cross_entropy，产出预测词<code class="highlighter-rouge">purr</code>。跟目标比对，然后调整。这就是训练过程。如下图：</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/word2vecm.png" alt="word2vec" /></p> <ul> <li>因为Embeddings向量空间是高维的（需要几维可以自己定义，比如说128维），要想直观的看到他，可以使用t-SNE降维技术，这个技术据说比原始的PCA降维要先进，能保留更多信息。</li> </ul> <p><img src="/images/2016-11-14-study-embeddings/t-SNE.png" alt="t-sne" /></p> <h2 id="读例子程序">读例子程序：</h2> <p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb">点此查看notebook</a></p> <h3 id="1-引入库文件">1. 引入库文件</h3> <p>先是引入一些库文件。第一行是如果要在Jupyter Notebook里运行的话，加这一行<code class="highlighter-rouge">%matplotlib inline</code>表示输出的图直接嵌入到Notebook里。</p> <p><code class="highlighter-rouge">__future__</code> 和 <code class="highlighter-rouge">six</code> 都是保证python2 3兼容的做法。</p> <p>我们可以看到，<code class="highlighter-rouge">TSNE</code>库，tensorflow并没有，所以使用的是<code class="highlighter-rouge">sklearn</code>的实现。 <code class="highlighter-rouge">collections</code>是python自带的一个工具库，据说很好，新手可以看这篇<a href="http://www.zlovezl.cn/articles/collections-in-python/">中文介绍博文</a>。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># These are all the modules we'll be using later. Make sure you can import them
# before proceeding further.
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
</code></pre></div></div> <h3 id="2-下载压缩包">2. 下载压缩包</h3> <p>下载数据集。如果网速不快，可以用下载工具下载，地址是 http://mattmahoney.net/dc/text8.zip 。保存到运行目录即可。</p> <p>如果手工解压开查看具体内容，你会开到这个文件里没有标点，全部小写，词与词之间空格隔开：</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>anarchism originated as a term of abuse first used against early working class ...
</code></pre></div></div> <p>英语不太好，去了标题之后，我都看不太懂是啥意思。想起了我们的没有标点古文。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">url</span> <span class="o">=</span> <span class="s">'http://mattmahoney.net/dc/'</span>

<span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="s">"""Download a file if not present, and make sure it's the right size."""</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="p">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Found and verified %s'</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="p">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span>
      <span class="s">'Failed to verify '</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'. Can you get to it with a browser?'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">filename</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">'text8.zip'</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>
</code></pre></div></div> <h3 id="3-读入文本">3. 读入文本</h3> <p>然后是读入压缩包里第一个文件的所有内容，并以空格分割，形成一个很大的list。</p> <p>这里tf.compat.as_str只是确保一下是string，应该没什么额外用途。</p> <p>tf.compat包里面都是一些关于兼容性（compatibility)的小工具。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="s">"""Extract the first file enclosed in a zip file as a list of words"""</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="p">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])).</span><span class="n">split</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">data</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Data size %d'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
</code></pre></div></div> <h3 id="4-数据预处理">4. 数据预处理</h3> <p><code class="highlighter-rouge">collections.Counter</code>很厉害，可以很方便的数元素出现了几次。以后就不要自己造轮子了，用这个Counter可高效多了。</p> <p><code class="highlighter-rouge">UNK</code>应该是Unknow的缩写，就是这边只统计50000-1个常用词，剩下的词统称<code class="highlighter-rouge">UNK</code>。</p> <p><code class="highlighter-rouge">count</code>就是包括<code class="highlighter-rouge">UNK</code>在内的所有50000-1个常用词的词语和出现次数。</p> <p>按照<code class="highlighter-rouge">count</code>里的词先后顺序，给词进行编号，<code class="highlighter-rouge">UNK</code>是0，出现最多的<code class="highlighter-rouge">the</code>是1，出现第二多的<code class="highlighter-rouge">of</code>是2。</p> <p><code class="highlighter-rouge">dictionary</code>就是词到编号的对应关系，可以快速查找词的编号：<code class="highlighter-rouge">index = dictionary(word)</code>。</p> <p><code class="highlighter-rouge">reverse_dictionary</code>则是编号到词的对应关系，可以快速查找某个编号是什么词：<code class="highlighter-rouge">word = reverse_dictionary(index)</code>。</p> <p>最后<code class="highlighter-rouge">data</code>是把原文的词都转化成对应编码以后的串。</p> <p>保存<code class="highlighter-rouge">dictionary</code>和<code class="highlighter-rouge">reverse_dictionary</code>这一点十分值的学习，对于频繁的查询，这样的缓存能大大增加速度。如果用函数的方式，每次查都要轮询，就土了。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
  <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'UNK'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
  <span class="n">count</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="p">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">).</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
    <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># dictionary['UNK']
</span>      <span class="n">unk_count</span> <span class="o">=</span> <span class="n">unk_count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
  <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
  <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="p">.</span><span class="n">keys</span><span class="p">()))</span>
  <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span>

<span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Most common words (+UNK)'</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Sample data'</span><span class="p">,</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="k">del</span> <span class="n">words</span>  <span class="c1"># Hint to reduce memory.
</span></code></pre></div></div> <h3 id="5-生成训练数据集函数">5. 生成训练数据集函数</h3> <p>准备要生成可供训练的数据集了。</p> <p>这里使用全局变量<code class="highlighter-rouge">data_index</code>来记录当前取到哪了，每次取一个batch后会向后移动，如果超出结尾，则又从头开始。<code class="highlighter-rouge">data_index = (data_index + 1) % len(data)</code>（怎么把这东西放在全局变量里，是不是有点不够漂亮？）</p> <p><code class="highlighter-rouge">skip_window</code>是确定取一个词周边多远的词来训练，比如说<code class="highlighter-rouge">skip_window</code>是2，则取这个词的左右各两个词，来作为它的上下文词。后面正式使用的时候取值是1，也就是只看左右各一个词。</p> <p>这里的<code class="highlighter-rouge">num_skips</code>我有点疑问，按下面注释是说，<code class="highlighter-rouge">How many times to reuse an input to generate a label.</code>，但是我觉得如果确定了<code class="highlighter-rouge">skip_window</code>之后，完全可以用</p> <p><code class="highlighter-rouge">num_skips=2*skip_window</code>来确定需要reuse的次数呀，难道还会浪费数据源不成？</p> <p>这边用了一个双向队列<code class="highlighter-rouge">collections.deque</code>，第一次遇见，看代码与<code class="highlighter-rouge">list</code>没啥区别，从网上简介来看，双向队列主要在左侧插入弹出的时候效率高，但这里并没有左侧的插入弹出呀，所以是不是应该老实用<code class="highlighter-rouge">list</code>会比较好呢？</p> <p>然后要维护一个<code class="highlighter-rouge">targets_to_avoid</code>，如果不是左右<code class="highlighter-rouge">skip_window</code>个词都用起来的话，则通过随机函数来随机选取对应的词。还是上面那个问题，为啥不直接全用啊？不全用还得随机，想不明白。</p> <p>这一段代码感觉写的怪怪的，疑点比较多，如果有能知道其中淫巧的同学，可以留言指点一下。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">data_index</span>
  <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
  <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># [ skip_window target skip_window ]
</span>  <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
    <span class="nb">buffer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">num_skips</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">skip_window</span>  <span class="c1"># target label at the center of the buffer
</span>    <span class="n">targets_to_avoid</span> <span class="o">=</span> <span class="p">[</span> <span class="n">skip_window</span> <span class="p">]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_skips</span><span class="p">):</span>
      <span class="k">while</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets_to_avoid</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">targets_to_avoid</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
      <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
      <span class="n">labels</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
    <span class="nb">buffer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span>
</code></pre></div></div> <p>这里是测试一下<code class="highlighter-rouge">generate_batch</code>函数，并无实际用途。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'data:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">8</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]:</span>
  <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_skips</span><span class="o">=</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="o">=</span><span class="n">skip_window</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">with num_skips = %d and skip_window = %d:'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">))</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'    batch:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'    labels:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</code></pre></div></div> <h3 id="6-超参数">6. 超参数</h3> <p>不管怎样，<code class="highlighter-rouge">generate_batch</code>函数算是准备好了，先备用，后面真实训练的时候会用到。</p> <p>接下来来定义一些超参数吧，这些超参数你也可以根据你的需要修改，来看看是否训练出来的Embeddings更符合你的需要。</p> <p>从前面图上，有人或许会以为Embeddings向量空间只是三维的，其实不是，它是高维的。这里定义<code class="highlighter-rouge">embedding_size</code>是128维。</p> <p>然后定义用来供人直观检验validate的一些参数。</p> <p><code class="highlighter-rouge">num_sampled</code>则是Sample Softmax时候用到的一个超参数，确定选几个词来对比优化。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># Dimension of the embedding vector.
</span><span class="n">skip_window</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># How many words to consider left and right.
</span><span class="n">num_skips</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># How many times to reuse an input to generate a label.
# We pick a random validation set to sample nearest neighbors. here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent.
</span><span class="n">valid_size</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># Random set of words to evaluate similarity on.
</span><span class="n">valid_window</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># Only pick dev samples in the head of the distribution.
</span><span class="n">valid_examples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">valid_window</span><span class="p">),</span> <span class="n">valid_size</span><span class="p">))</span>
<span class="n">num_sampled</span> <span class="o">=</span> <span class="mi">64</span> <span class="c1"># Number of negative examples to sample.
</span></code></pre></div></div> <h3 id="7-定义tensorflow模型">7. 定义Tensorflow模型</h3> <p>终于要开始定义Tensorflow的模型了。先回顾一下视频截图：</p> <p><img src="/images/2016-11-14-study-embeddings/word2vecm.png" alt="word2vec" /></p> <p><code class="highlighter-rouge">train_dataset</code> 和 <code class="highlighter-rouge">train_labels</code> 两个<code class="highlighter-rouge">placeholder</code>用来训练时传入x和y。</p> <p><code class="highlighter-rouge">valid_dataset</code> 则是用来人工验证的小数据集，是<code class="highlighter-rouge">constant</code>，直接赋值前面生成的<code class="highlighter-rouge">valid_examples</code>。</p> <p><code class="highlighter-rouge">embeddings</code>是用来存储Embeddings向量空间的变量，初始化成-1到1之间的随机数，后面优化时调整。这里它是一个 <code class="highlighter-rouge">50000</code> * <code class="highlighter-rouge">128</code> 的二维变量。</p> <p><code class="highlighter-rouge">softmax_weights</code> 和 <code class="highlighter-rouge">softmax_biases</code> 是用来做线性逻辑分类的参数。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="k">with</span> <span class="n">graph</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>

  <span class="c1"># Input data.
</span>  <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
  <span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">valid_examples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="c1"># Variables.
</span>  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
  <span class="n">softmax_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span>
                         <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)))</span>
  <span class="n">softmax_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>
</code></pre></div></div> <h3 id="8-模型前半部分">8. 模型前半部分</h3> <p>通过<code class="highlighter-rouge">tf.nn.embedding_lookup</code>可以直接根据<code class="highlighter-rouge">embeddings</code>表(50000,128)，取出一个与输入词对应的128个值的<code class="highlighter-rouge">embed</code>，也就是128维向量。其实是一batch同时处理，但说一个好理解一些。</p> <p>通过<code class="highlighter-rouge">tf.nn.sampled_softmax_loss</code>可以用效率较高的Sample Softmax来得到优化所需要的偏差。这个方法视频里有带过，反正就是全部比对速度慢，这样速度快。这个方法顺带把 wX+b 这步也一起算了。可能是因为放在一起可以优化计算速度，记得还有那个很长名字的<code class="highlighter-rouge">softmax_cross_entropy_with_logits</code>同时搞定softmax和cross_entropy，也是为了优化计算速度。但，这样的代码读起来就不好看了！</p> <p>通过<code class="highlighter-rouge">tf.reduce_mean</code>把<code class="highlighter-rouge">loss</code>偏差压到一个数值，用于优化。</p> <p>然后就是优化，这里使用了<code class="highlighter-rouge">AdagradOptimizer</code>，当然也可以使用其他<code class="highlighter-rouge">SGD</code>、<code class="highlighter-rouge">Adam</code>等各种优化算法，Tensorflow都实现同样的接口，只需要换个函数名就可以。</p> <p>这里注释提到，因为<code class="highlighter-rouge">embeddings</code>是定义成<code class="highlighter-rouge">tf.Variable</code>的，所以在优化的时候同时也会调整<code class="highlighter-rouge">embeddings</code>里的参数。这是因为<code class="highlighter-rouge">minimize</code>函数会将与传入的<code class="highlighter-rouge">loss</code>连接的所有源头变量进行调整优化。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Model.
</span>  <span class="c1"># Look up embeddings for inputs.
</span>  <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">)</span>
  <span class="c1"># Compute the softmax loss, using a sample of the negative labels each time.
</span>  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">softmax_weights</span><span class="p">,</span> <span class="n">softmax_biases</span><span class="p">,</span> <span class="n">embed</span><span class="p">,</span>
                               <span class="n">train_labels</span><span class="p">,</span> <span class="n">num_sampled</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>

  <span class="c1"># Optimizer.
</span>  <span class="c1"># Note: The optimizer will optimize the softmax_weights AND the embeddings.
</span>  <span class="c1"># This is because the embeddings are defined as a variable quantity and the
</span>  <span class="c1"># optimizer's `minimize` method will by default modify all variable quantities
</span>  <span class="c1"># that contribute to the tensor it is passed.
</span>  <span class="c1"># See docs on `tf.train.Optimizer.minimize()` for more details.
</span>  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

</code></pre></div></div> <h3 id="9-模型关键部位">9. 模型关键部位</h3> <p><a name="normalization"></a></p> <p>这里写的有点难看懂，必须再刷一次视频。</p> <p><img src="/images/2016-11-14-study-embeddings/cosine.png" alt="cosine compare" /></p> <p>原来这是要计算一个<code class="highlighter-rouge">valid_dataset</code>中单词的相似度。</p> <p>首先，<code class="highlighter-rouge">norm</code>是模的意思，也就是二范数，就是这个向量和原点的距离，土话就是这个向量的长度。</p> <p>计算<code class="highlighter-rouge">norm</code>的方法，就是向量各维度的平方和：</p> \[norm = \|X\| = {\sqrt{\sum_{i=1}^N{X_i^2}}}\] <p>然后利用这个向量长度<code class="highlighter-rouge">norm</code>来给向量标准化：</p> \[X_{normalized} = {X \over \|X\|}\] <p>这样得到的就是长度为1的向量。也就是抛弃了向量的长度信息，就剩下方向信息。（感谢潘程同学提醒，这东西就叫“<a href="https://zh.wikipedia.org/wiki/%E5%8D%95%E4%BD%8D%E5%90%91%E9%87%8F">单位向量</a>”，可以表示为 \(\hat{X}\)！）</p> <p>接下来，</p> <p>造一些简单的数据，用<code class="highlighter-rouge">numpy</code>模拟一下这一段代码，确认一下这段代码的意义：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="p">[[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">2</span> <span class="mi">4</span> <span class="mi">6</span> <span class="mi">8</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span> <span class="mi">2</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">3</span> <span class="mi">3</span> <span class="mi">3</span> <span class="mi">3</span><span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="p">[[</span> <span class="mi">1</span>  <span class="mi">4</span>  <span class="mi">9</span> <span class="mi">16</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">4</span> <span class="mi">16</span> <span class="mi">36</span> <span class="mi">64</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">4</span>  <span class="mi">4</span>  <span class="mi">4</span>  <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">9</span>  <span class="mi">9</span>  <span class="mi">9</span>  <span class="mi">9</span><span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">[[</span> <span class="mi">30</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">16</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">36</span><span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
<span class="p">[[</span>  <span class="mf">5.47722558</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">10.95445115</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">4.</span>        <span class="p">]</span>
 <span class="p">[</span>  <span class="mf">6.</span>        <span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">/</span> <span class="n">norm</span>
<span class="p">[[</span> <span class="mf">0.18257419</span>  <span class="mf">0.36514837</span>  <span class="mf">0.54772256</span>  <span class="mf">0.73029674</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.18257419</span>  <span class="mf">0.36514837</span>  <span class="mf">0.54772256</span>  <span class="mf">0.73029674</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>       <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>         <span class="mf">0.5</span>       <span class="p">]]</span>
<span class="o">&gt;&gt;</span> <span class="n">sim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">normalized_embeddings</span><span class="p">,</span><span class="n">normalized_embeddings</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="p">[[</span> <span class="mf">1.</span>          <span class="mf">1.</span>          <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.</span>          <span class="mf">1.</span>          <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span>  <span class="mf">1.</span>          <span class="mf">1.</span>        <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.91287093</span>  <span class="mf">0.91287093</span>  <span class="mf">1.</span>          <span class="mf">1.</span>        <span class="p">]]</span>
</code></pre></div></div> <p>可以看到，向量(1,2,3,4)和向量(2,4,6,8)在经过标准化之后，变成了同样的值(0.18,0.36,0.54,0.73)。同样向量(2,2,2,2)和(3,3,3,3)也都变成了一样的(0.5,0.5,0.5,0.5)。</p> <p>也就是说，这个操作是针对每一个向量，各自做 Normalization 标准化。</p> <p>也就是说，这个标准化操作抛弃了向量的长度，只关注向量的方向。视频里说，比较两个embeddings向量的时候，用的是cosine距离。</p> <p>然后从标准化后的向量空间里，找出用于检验的词语对应的向量值，<code class="highlighter-rouge">valid_embeddings</code> 这个变量名有点迷惑人，我建议将其改名为 <code class="highlighter-rouge">valid_embed</code>，以对应前文中的 <code class="highlighter-rouge">embed</code> 变量，他们俩是有相似意义的。</p> <p>最后通过<a href="https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7">余弦相似性原理</a>，计算出<code class="highlighter-rouge">similarity</code>：</p> \[similarity = cos(\theta) = { A \cdot B \over \|A\| \cdot \|B\| } = A_{normalized} \cdot B_{normalized}\] <p>当 \(\theta=0\) 时，两者重合， \(similarity=cos(0)=1\)。</p> <p>用检索出来的结果矩阵（前面模拟里我用了全部矩阵） 乘以 <code class="highlighter-rouge">normalized_embeddings</code>的转置，得到的结果最大值是1也就是相同，值越大代表越相似。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Compute the similarity between minibatch examples and all embeddings.
</span>  <span class="c1"># We use the cosine distance:
</span>  <span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
  <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">/</span> <span class="n">norm</span>
  <span class="n">valid_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
    <span class="n">normalized_embeddings</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">)</span>
  <span class="n">similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">valid_embeddings</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">normalized_embeddings</span><span class="p">))</span>
</code></pre></div></div> <h3 id="10-迭代训练">10. 迭代训练</h3> <p>模型定义完，就来执行训练了。</p> <p>这里有个小bug，新版本的Tensorflow不再支持<code class="highlighter-rouge">global_variables_initializer</code>方法，而改为使用<code class="highlighter-rouge">initialize_all_variables</code>方法，需要手工修改一下教程代码。（年轻而高速发展的Tensorflow里面有很多类似的接口变动，所以如果一个程序跑不通，去github上看看Tensorflow的源代码很有必要。）</p> <p>定义迭代次数，100001次，每一次迭代，都从<code class="highlighter-rouge">generate_batch</code>里面获取一个batch的训练数据。制作成<code class="highlighter-rouge">feed_dict</code>，对应前面模型里定义的<code class="highlighter-rouge">placeholder</code>。</p> <p>跑一次优化，记录下loss并加起来，等到每2000次的时候，输出一次平均loss。（又一个细节：这里的<code class="highlighter-rouge">average_loss</code>是否应该取名叫<code class="highlighter-rouge">subtotal_loss</code>？）</p> <p>等到每10000次，获取一下<code class="highlighter-rouge">similarity</code>对应的值，然后把最相似的几个词输出到屏幕，用于人工检验。<code class="highlighter-rouge">argsort</code>是获取排序后的indices。然后通过<code class="highlighter-rouge">reverse_dictionary</code>快速定位到词是什么。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">100001</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">().</span><span class="n">run</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Initialized'</span><span class="p">)</span>
  <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">batch_data</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span>
      <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">)</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">train_dataset</span> <span class="p">:</span> <span class="n">batch_data</span><span class="p">,</span> <span class="n">train_labels</span> <span class="p">:</span> <span class="n">batch_labels</span><span class="p">}</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
    <span class="n">average_loss</span> <span class="o">+=</span> <span class="n">l</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">average_loss</span> <span class="o">=</span> <span class="n">average_loss</span> <span class="o">/</span> <span class="mi">2000</span>
      <span class="c1"># The average loss is an estimate of the loss over the last 2000 batches.
</span>      <span class="k">print</span><span class="p">(</span><span class="s">'Average loss at step %d: %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">average_loss</span><span class="p">))</span>
      <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># note that this is expensive (~20% slowdown if computed every 500 steps)
</span>    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">sim</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid_size</span><span class="p">):</span>
        <span class="n">valid_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">valid_examples</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># number of nearest neighbors
</span>        <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">sim</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]).</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">log</span> <span class="o">=</span> <span class="s">'Nearest to %s:'</span> <span class="o">%</span> <span class="n">valid_word</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
          <span class="n">close_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">nearest</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
          <span class="n">log</span> <span class="o">=</span> <span class="s">'%s %s,'</span> <span class="o">%</span> <span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">close_word</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>
</code></pre></div></div> <p>训练完成，获取最后得到的<code class="highlighter-rouge">embeddings</code>向量空间！</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">final_embeddings</span> <span class="o">=</span> <span class="n">normalized_embeddings</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre></div></div> <h3 id="11-降维可视化">11. 降维可视化</h3> <p>下面把<code class="highlighter-rouge">embeddings</code>向量空间通过t-SNE（t-distributed Stochastic Neighbor Embedding）降维到2D，然后打出来看看。</p> <p>其实根据t-SNE的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">文档</a>，如果向量空间维数过高，比如说我们把128维改成1024维，或者更高，那么这里建议先使用PCA降维方法，降到50维左右以后，再使用t-SNE来继续降到二维或者三维。因为直接用t-SNE给高维向量空间做降维的话，效率会比较低。</p> <p>画图部分这里用<code class="highlighter-rouge">pylab</code>，就不再赘述，如果喜欢用<code class="highlighter-rouge">matplotlib</code>，也是类似的。（pylab是对标matlab的库，matplotlib也包含在pylab里。）</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_points</span> <span class="o">=</span> <span class="mi">400</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">two_d_embeddings</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">final_embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_points</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">'More labels than embeddings'</span>
  <span class="n">pylab</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c1"># in inches
</span>  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
    <span class="n">pylab</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pylab</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s">'offset points'</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'bottom'</span><span class="p">)</span>
  <span class="n">pylab</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_points</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></div> <p>嗯，我们会看到这样的结果：</p> <p><img src="/images/2016-11-14-study-embeddings/t-sne-result.png" alt="t-sne result" /></p> <h2 id="一个小debug处理">一个小debug处理</h2> <p>这个例子里用到了sklearn的tSNE来降维作显示，但每次运行到这里Jupyter都提醒Kernel死掉了，Kernel重启。于是把源码复制到单独python文件里执行，看到如下错误：</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.
</code></pre></div></div> <p>这个问题似乎是Intel的MKL有问题，网上查到这篇<a href="https://www.continuum.io/blog/developer-blog/anaconda-25-release-now-mkl-optimizations">为Anaconda2.5提供MKL优化</a>，里面也提供了去掉MKL优化的方法：</p> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install nomkl numpy scipy scikit-learn numexpr
conda remove mkl mkl-service
</code></pre></div></div> <p>执行过之后，就去除了mkl库，import mkl就找不到了，同时例子程序里的问题也没有再出现。 如果还是想使用Intel MKL优化的话，或许可以参考Anaconda官方文档<a href="https://docs.continuum.io/mkl-optimizations/">MKL OPTIMIZATIONS</a>来重新安装过。不过，我先不装了。</p> <h2 id="张量-tensor-的-阶数-rank-向量空间-vector-space-和-向量-vector-的-维数-dimension">张量 Tensor 的 阶数 Rank ，向量空间 Vector Space 和 向量 Vector 的 维数 Dimension</h2> <p>前面我在写到Embeddings是128维的时候，脑子有点晕，原来是混淆了Tensor的维数和Vector的维数。</p> <p>为了搞清楚概念，我查了下资料，应该是这样的：</p> <p>我们平时说的128维向量，是指一个向量，值是有128个数组成，代表128个方向上的量，例子里的 <code class="highlighter-rouge">embed</code> 变量就是一个batch的128维向量。</p> <p>向量空间是一群向量的集合，我们这里 <code class="highlighter-rouge">embeddings</code> 变量就是一个50000个向量集合起来的向量空间，所以他的<code class="highlighter-rouge">shape</code>是<code class="highlighter-rouge">(50000,128)</code>。</p> <p>张量 Tensor 也有一个维数，为了和向量的维数区分，一般成其为阶数 Rank。比如说变量 <code class="highlighter-rouge">embeddings</code> 是一个128维的向量空间，同时他也是一个二阶张量，也就是一张二维表格，行是单词，列是这个单词的128个值。假设其他地方我们看到<code class="highlighter-rouge">128维张量</code>的话，那么它的<code class="highlighter-rouge">shape</code>应该是(xx,xx,xx,xx,….,xx)一共128个xx数，一般来说现在我们学习中还没有碰到这么高阶的张量，可能4阶张量已经很多了，在Convolutional Neural Network里用到的输入，它的<code class="highlighter-rouge">shape</code>是<code class="highlighter-rouge">(batch, image_width, image_height, image_channel)</code>，就是一个4阶张量（其实应该算一个batch的3阶张量的集合）。注意这里的<code class="highlighter-rouge">image_width</code>不是说输入的是图片的宽度值，而是输入的是宽度为<code class="highlighter-rouge">image_width</code>的图片本身。</p> <p>这就是阶和维的区别。</p> Any feedback? Please at me on <a class="social-link" href="http://twitter.com/liusida2007" > <i class="fab fa-twitter"></i> </a> </div><!-- Incomment incase you want to use Disqus <div id="disqus_thread"></div> --> </article> <script> var disqus_config = function () { this.page.url = "http://172.25.168.122:4000/2016/11/14/study-embeddings/"; /* Replace PAGE_URL with your page's canonical URL variable */ this.page.identifier = "/2016/11/14/study-embeddings"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */ }; (function () { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript></div> </div> <div class="row"> <div class="col-md-4"> <div class="card"> <div class="card-header">About Sida Liu</div> <div class="card-body"> <p class="author_bio">I am currently a M.S. graduate student in Morphology, Evolution & Cognition Laboratory at University of Vermont. I am interested in artificial intelligence, artificial life, and artificial environment.</p><!-- Place this tag where you want the button to render. --> <a class="github-button" href="https://github.com/liusida" data-size="large" data-show-count="true" aria-label="Follow @liusida on GitHub">Follow @liusida</a></div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Categories</div> <div class="card-body text-dark"> <div id="#Old"></div> <li class="tag-head"> <a href="/blog/categories/Old">Old</a> </li> <a name="Old"></a> <div id="#CUDA"></div> <li class="tag-head"> <a href="/blog/categories/CUDA">CUDA</a> </li> <a name="CUDA"></a> <div id="#Simulation"></div> <li class="tag-head"> <a href="/blog/categories/Simulation">Simulation</a> </li> <a name="Simulation"></a> <div id="#GPU"></div> <li class="tag-head"> <a href="/blog/categories/GPU">GPU</a> </li> <a name="GPU"></a> <div id="#Reading"></div> <li class="tag-head"> <a href="/blog/categories/Reading">Reading</a> </li> <a name="Reading"></a> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Useful Links</div> <div class="card-body text-dark"> <li> <a href="http://172.25.168.122:4000/cover">Cover</a> </li> <li> <a href="http://172.25.168.122:4000/blog">Blog</a> </li> <li> <a href="http://172.25.168.122:4000/contact">Contact Me</a> </li> </div> </div> </div> </div> </div><footer> <p> Powered by <strong>devlopr jekyll</strong>. Subscribe via <a href=" /feed.xml ">RSS</a> </p> </footer> <script> var options = { classname: 'my-class', id: 'my-id' }; var nanobar = new Nanobar( options ); nanobar.go( 30 ); nanobar.go( 76 ); nanobar.go(100); </script> <!-- <div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div> --> <!-- <script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script> --> <!-- <script src="/assets/js/mode-switcher.js"></script> --> </body> </html>
